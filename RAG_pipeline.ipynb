{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qdOLlUP1pq4q"},"outputs":[],"source":["!pip install langchain langchain-community openai tiktoken unstructured chromadb --quiet\n","!pip install beautifulsoup4 requests --quiet\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MLtZdJSMuG5s"},"outputs":[],"source":["!pip install langchain-google-genai faiss-cpu pypdf langchain  --quiet\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Ph-twlpu8LW"},"outputs":[],"source":["!pip install google-generativeai==0.8.5 google-ai-generativelanguage==0.6.15 langchain-google-genai faiss-cpu pypdf langchain --quiet\n"]},{"cell_type":"markdown","metadata":{"id":"VGlenoMSFA34"},"source":["# single page scrape for demo"]},{"cell_type":"markdown","metadata":{"id":"SkdmRavPtERh"},"source":["## Website scraping"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UhrXDJ37s8W7"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","\n","url = \"https://huyenchip.com/2024/07/25/genai-platform.html\"\n","html = requests.get(url).text\n","soup = BeautifulSoup(html, \"html.parser\")\n","text = soup.get_text()\n","print(text[:1000])  # just preview\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0myrXgaLtdLP"},"outputs":[],"source":["import google.generativeai as genai\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","\n","print(\"Setup successful ✅\")\n"]},{"cell_type":"markdown","metadata":{"id":"OaAgKJWiBb6c"},"source":["##  Wrap the scraped text as a LangChain Document"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mYvUenGEvQEP"},"outputs":[],"source":["from langchain.docstore.document import Document\n","\n","docs = [Document(page_content=text, metadata={\"source\": url})]\n"]},{"cell_type":"markdown","metadata":{"id":"drIfdpuMBimY"},"source":["## Split the document into chunks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X5Js16B_BTHK"},"outputs":[],"source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1000,  # characters\n","    chunk_overlap=200\n",")\n","\n","chunks = text_splitter.split_documents(docs)\n","print(f\"Number of chunks: {len(chunks)}\")\n","print(chunks[0].page_content[:200])\n"]},{"cell_type":"markdown","metadata":{"id":"GC-CctNpB4Ob"},"source":["## Create Embeddings + Store in FAISS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gdBgHl-_Bp06"},"outputs":[],"source":["from langchain_google_genai import GoogleGenerativeAIEmbeddings\n","from langchain_community.vectorstores import FAISS\n","\n","# 1️⃣ Create the embedding model\n","embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n","\n","# 2️⃣ Create FAISS vector store from chunks\n","vectorstore = FAISS.from_documents(chunks, embedding_model)\n","\n","# 3️⃣ Save the FAISS index locally (optional)\n","vectorstore.save_local(\"faiss_index\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9b8614d"},"outputs":[],"source":["!pip install --upgrade --quiet  langchain-community"]},{"cell_type":"markdown","metadata":{"id":"qq4IvCK1Cj6e"},"source":["## Load FAISS and Ask Questions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CiiNke7UCkji"},"outputs":[],"source":["# Load the vectorstore\n","vectorstore = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n","\n","# Search for similar chunks\n","query = \"What is a GenAI platform?\"\n","docs = vectorstore.similarity_search(query, k=3)\n","\n","for i, doc in enumerate(docs, start=1):\n","    print(f\"Result {i}:\")\n","    print(doc.page_content[:300])\n","    print(\"---\")\n"]},{"cell_type":"markdown","metadata":{"id":"Uy87gll9C80a"},"source":["## Use LLM for Final Answer (RAG)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o0lCu4-5CpIP"},"outputs":[],"source":["from langchain.chains import RetrievalQA\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","\n","llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n","\n","\n","# Create RetrievalQA chain\n","qa_chain = RetrievalQA.from_chain_type(\n","    llm=llm,\n","    retriever=vectorstore.as_retriever(),\n","    return_source_documents=True\n",")\n","\n","# Ask a question\n","response = qa_chain({\"query\": \"What is a GenAI platform?\"})\n","print(\"Answer:\", response[\"result\"])\n","print(\"\\nSources:\", [doc.metadata[\"source\"] for doc in response[\"source_documents\"]])\n"]},{"cell_type":"markdown","metadata":{"id":"faxxlhzmFRdW"},"source":["## Minimal working RAG pipeline"]},{"cell_type":"markdown","metadata":{"id":"Vl16-9NEFb5w"},"source":["## Multi-URL Data Loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U4P4ulJWFczN"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","\n","def scrape_url(url):\n","    html = requests.get(url).text\n","    soup = BeautifulSoup(html, \"html.parser\")\n","    return soup.get_text(separator=\"\\n\")\n","\n","urls = [\n","    \"https://huyenchip.com/2024/07/25/genai-platform.html\",\n","    \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n","    \"https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/\",\n","    \"https://quoraengineering.quora.com/Building-Embedding-Search-at-Quora\"\n","]\n","\n","documents = []\n","for url in urls:\n","    text = scrape_url(url)\n","    documents.append({\"url\": url, \"text\": text})\n"]},{"cell_type":"markdown","metadata":{"id":"gIXaCaoGIDxn"},"source":["## Chunking"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"crYMyCN-IAfE"},"outputs":[],"source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1000,\n","    chunk_overlap=200\n",")\n","\n","chunks = []\n","for doc in documents:\n","    for chunk in text_splitter.split_text(doc[\"text\"]):\n","        chunks.append({\"text\": chunk, \"url\": doc[\"url\"]})\n"]},{"cell_type":"markdown","metadata":{"id":"L26jiyuzINDE"},"source":["## Embeddings + Vector Store (FAISS)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dJvPDJv7IJr8"},"outputs":[],"source":["from langchain_community.vectorstores import FAISS\n","from langchain_google_genai import GoogleGenerativeAIEmbeddings\n","\n","embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n","\n","texts = [c[\"text\"] for c in chunks]\n","metadatas = [{\"url\": c[\"url\"]} for c in chunks]\n","\n","vectorstore = FAISS.from_texts(texts, embedding_model, metadatas=metadatas)\n"]},{"cell_type":"markdown","metadata":{"id":"iw8l3chlIXtM"},"source":["## Retrieval + LLM Answer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SIb5BBTKIP6_"},"outputs":[],"source":["from langchain_google_genai import ChatGoogleGenerativeAI\n","\n","llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n","\n","def answer_query(query):\n","    docs = vectorstore.similarity_search(query, k=4)\n","    context = \"\\n\\n\".join([d.page_content for d in docs])\n","    sources = [d.metadata[\"url\"] for d in docs]\n","\n","    prompt = f\"Answer the question based on the following context:\\n\\n{context}\\n\\nQuestion: {query}\\nAnswer with citations.\"\n","    response = llm.predict(prompt)\n","    return response, list(set(sources))\n","\n","question = \"What is a GenAI platform?\"\n","answer, sources = answer_query(question)\n","print(answer)\n","print(\"Sources:\", sources)\n"]},{"cell_type":"markdown","metadata":{"id":"PFWPuXCKJozv"},"source":["## Structure the Pipeline Properly"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DphszFQ2Iakv"},"outputs":[],"source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.vectorstores import FAISS\n","from langchain_google_genai import GoogleGenerativeAIEmbeddings\n","\n","# 1. Split text\n","splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","chunks = splitter.split_text(text)\n","\n","# 2. Create embeddings\n","embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n","\n","# 3. Store in FAISS\n","db = FAISS.from_texts(chunks, embeddings)\n"]},{"cell_type":"markdown","metadata":{"id":"WOpDbfduKIxt"},"source":["## Add the Retrieval Step\n","When a user asks something:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YQFV9F31KAOd"},"outputs":[],"source":["query = \"What is a GenAI platform?\"\n","docs = db.similarity_search(query, k=3)  # Get top 3 relevant chunks\n","context = \" \".join([d.page_content for d in docs])\n","\n","prompt = f\"Answer based on the following context:\\n{context}\\n\\nQuestion: {query}\"\n","response = llm.invoke(prompt)\n","print(response)\n"]},{"cell_type":"markdown","metadata":{"id":"8Y7DUHh4LB7Q"},"source":["## Wrap scraped text into Document objects"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CAZ2hFCdKLik"},"outputs":[],"source":["from langchain.schema import Document\n","\n","docs = [Document(page_content=text, metadata={\"source\": url})]\n"]},{"cell_type":"markdown","metadata":{"id":"aDwzHzhOLGtg"},"source":["## Chunk the text (so retrieval is more accurate)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ny_KwwQK9nL"},"outputs":[],"source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","chunks = splitter.split_documents(docs)\n"]},{"cell_type":"markdown","metadata":{"id":"Lv7Wfx4tLOi-"},"source":["## Embed chunks and store in FAISS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d6Up9Zz1LK4-"},"outputs":[],"source":["from langchain_google_genai import GoogleGenerativeAIEmbeddings\n","from langchain.vectorstores import FAISS\n","\n","embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n","vectorstore = FAISS.from_documents(chunks, embeddings)\n"]},{"cell_type":"markdown","metadata":{"id":"ndpiSPWBLUS4"},"source":["## Create retriever + chain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IUlb1QMYLQ-4"},"outputs":[],"source":["retriever = vectorstore.as_retriever()\n"]},{"cell_type":"markdown","metadata":{"id":"k2EO8ZJ6Lb_P"},"source":["## Ask questions with retrieval + Gemini"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-pI5fHOYLWoq"},"outputs":[],"source":["from langchain.chains import RetrievalQA\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","\n","llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n","\n","qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)\n","result = qa_chain.run(\"What is a GenAI platform?\")\n","print(result)\n"]},{"cell_type":"markdown","metadata":{"id":"GogSI1jLvNe1"},"source":["## Step 1 — Data Ingestion from URLs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"II_LMfGGox89"},"outputs":[],"source":["import os\n","import requests\n","from bs4 import BeautifulSoup\n","\n","# Create data folder\n","data_dir = \"/content/drive/MyDrive/RAG_demo/data\"\n","os.makedirs(data_dir, exist_ok=True)\n","\n","urls = [\n","    \"https://huyenchip.com/2024/07/25/genai-platform.html\",\n","    \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n","    \"https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/\",\n","    \"https://quoraengineering.quora.com/Building-Embedding-Search-at-Quora\"\n","]\n","\n","def download_webpage(url, save_dir):\n","    response = requests.get(url)\n","    if response.status_code == 200:\n","        soup = BeautifulSoup(response.text, \"html.parser\")\n","        text = soup.get_text(separator=\"\\n\", strip=True)  # Extract clean text\n","        # Create filename from URL slug\n","        filename = url.split(\"/\")[-1] or \"index\"\n","        if not filename.endswith(\".txt\"):\n","            filename += \".txt\"\n","        save_path = os.path.join(save_dir, filename)\n","        with open(save_path, \"w\", encoding=\"utf-8\") as f:\n","            f.write(text)\n","        print(f\"✅ Saved: {save_path}\")\n","    else:\n","        print(f\"❌ Failed: {url}\")\n","\n","for url in urls:\n","    download_webpage(url, data_dir)\n"]},{"cell_type":"markdown","metadata":{"id":"gebcSF1uvW1f"},"source":["## Render the page (reliable for JS-heavy sites) — use Playwright in Colab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jZXIBpKLsTst"},"outputs":[],"source":["!pip install playwright==1.49.0  # or latest\n","!playwright install chromium\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GWtbb6Tcs6xE"},"outputs":[],"source":["import asyncio\n","from playwright.async_api import async_playwright\n","from bs4 import BeautifulSoup\n","import os\n","\n","async def render_and_save_text(url, save_path):\n","    async with async_playwright() as p:\n","        browser = await p.chromium.launch(headless=True, args=['--no-sandbox','--disable-dev-shm-usage'])\n","        page = await browser.new_page(user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/116\")\n","        await page.goto(url, wait_until=\"networkidle\", timeout=30000)\n","        html = await page.content()\n","        await browser.close()\n","\n","    soup = BeautifulSoup(html, \"html.parser\")\n","    text = soup.get_text(separator=\"\\n\", strip=True)\n","    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n","    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n","        f.write(text)\n","    print(\"Saved:\", save_path)\n","\n","# Example:\n","save_path = \"/content/drive/MyDrive/RAG_demo/data/quora_engineering.txt\"\n","# Run the async function\n","await render_and_save_text(\"https://quoraengineering.quora.com/Building-Embedding-Search-at-Quora\", save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Z6GAQvQtLPG"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","\n","url = \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\"\n","html = requests.get(url).text\n","soup = BeautifulSoup(html, \"html.parser\")\n","\n","# Remove scripts/styles\n","for tag in soup([\"script\", \"style\"]):\n","    tag.decompose()\n","\n","text = soup.get_text(separator=\"\\n\", strip=True)\n","with open(\"/content/drive/MyDrive/RAG_demo/data/hallucination.txt\", \"w\", encoding=\"utf-8\") as f:\n","    f.write(text)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fW8vUvcKyAIp"},"outputs":[],"source":["from langchain_community.document_loaders import UnstructuredURLLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","# 1. Load the article from URL\n","url = \"https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/\"\n","loader = UnstructuredURLLoader(urls=[url])\n","docs = loader.load()\n","\n","# 2. Add metadata for tracking\n","for doc in docs:\n","    doc.metadata[\"source\"] = url\n","\n","# 3. Split into smaller chunks\n","splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","split_docs = splitter.split_documents(docs)\n","\n","# 4. Add to your existing vectorstore\n","vectorstore.add_documents(split_docs)\n","\n","print(f\"✅ Re-ingested {len(split_docs)} chunks from {url}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9CbEFQWi0FC4"},"outputs":[],"source":["from langchain_community.document_loaders import UnstructuredURLLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_community.vectorstores import FAISS\n","\n","# 1. Load the article\n","url = \"https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/\"\n","loader = UnstructuredURLLoader(urls=[url])\n","docs = loader.load()\n","\n","# 2. Add metadata\n","for doc in docs:\n","    doc.metadata[\"source\"] = url\n","\n","# 3. Split into chunks\n","splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","split_docs = splitter.split_documents(docs)\n","\n","# 4. Load existing FAISS vectorstore\n","embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n","vectorstore = FAISS.load_local(\"/content/drive/MyDrive/RAG_demo/faiss_index\", embeddings, allow_dangerous_deserialization=True)\n","\n","# 5. Add documents (embeddings happen here automatically)\n","vectorstore.add_documents(split_docs)\n","\n","# 6. Save updated FAISS\n","vectorstore.save_local(\"/content/drive/MyDrive/RAG_demo/faiss_index\")\n","\n","print(f\"✅ Added {len(split_docs)} chunks from {url} and updated FAISS index.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GW-3bJy-uNBN"},"outputs":[],"source":["# Verify removal\n","sources = {doc.metadata.get(\"source\", \"Unknown\") for doc in vectorstore.docstore._dict.values()}\n","print(\"\\nRemaining sources:\")\n","for s in sorted(sources):\n","    print(\"-\", s)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5er82KE3wcER"},"outputs":[],"source":["from langchain_community.vectorstores import FAISS  # or Chroma, depending on your setup\n","\n","\n","# Create embeddings\n","embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n","# --- Step 1: Load your existing vectorstore ---\n","vectorstore_path = \"/content/drive/MyDrive/RAG_demo/faiss_index\"  # change if needed\n","vectorstore = FAISS.load_local(vectorstore_path, embeddings, allow_dangerous_deserialization=True)\n","\n","# --- Step 2: Filter out docs with old source ---\n","docs_to_keep = []\n","metadatas_to_keep = []\n","\n","for doc, metadata in zip(vectorstore.docstore._dict.values(), vectorstore.docstore._dict.values()):\n","    if \"index.txt\" not in metadata.metadata.get(\"source\", \"\"):\n","        docs_to_keep.append(doc)\n","        metadatas_to_keep.append(metadata.metadata)\n","\n","# --- Step 3: Rebuild vectorstore without old docs ---\n","new_vectorstore = FAISS.from_texts(\n","    [doc.page_content for doc in docs_to_keep],\n","    embeddings,\n","    metadatas=metadatas_to_keep\n",")\n","\n","# --- Step 4: Save cleaned store ---\n","new_vectorstore.save_local(vectorstore_path)\n","\n","print(f\"Cleanup complete. Removed 'index.txt' entries. New total docs: {len(docs_to_keep)}\")\n"]},{"cell_type":"markdown","metadata":{"id":"zWAVadQQV3ZW"},"source":["## Step 2: Chunking & Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AP-4Pvp_t_tY"},"outputs":[],"source":["!pip install langchain faiss-cpu sentence-transformers langchain-community\n","\n","from langchain_community.document_loaders import TextLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.vectorstores import FAISS\n","from langchain_community.embeddings import HuggingFaceEmbeddings\n","import os\n","\n","# Path to your folder\n","folder_path = \"/content/drive/MyDrive/RAG_demo/data\"\n","faiss_index_path = os.path.join(folder_path, \"faiss_index\")\n","\n","# Step 1: Load all .txt files\n","documents = []\n","for file_name in os.listdir(folder_path):\n","    if file_name.endswith(\".txt\"):\n","        loader = TextLoader(os.path.join(folder_path, file_name))\n","        documents.extend(loader.load())\n","\n","print(f\"Loaded {len(documents)} documents from {folder_path}\")\n","\n","# Step 2: Split into chunks\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1000,  # characters\n","    chunk_overlap=200  # overlap for better context\n",")\n","docs = text_splitter.split_documents(documents)\n","print(f\"Split into {len(docs)} chunks\")\n","\n","# Step 3: Create embeddings\n","embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n","\n","# Step 4: Store in FAISS\n","db = FAISS.from_documents(docs, embeddings)\n","\n","# Save FAISS index\n","db.save_local(faiss_index_path)\n","print(\"FAISS index saved successfully.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z_NTXX7U8olV"},"outputs":[],"source":["!pip install langchain faiss-cpu sentence-transformers langchain-community --quiet"]},{"cell_type":"markdown","metadata":{"id":"1Hm7F06KY6i8"},"source":["## Load FAISS Index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d3248gHeWSfU"},"outputs":[],"source":["from langchain_community.vectorstores import FAISS\n","from langchain_community.embeddings import HuggingFaceEmbeddings\n","import os\n","\n","# Paths\n","folder_path = \"/content/drive/MyDrive/RAG_demo/data\"\n","faiss_index_path = os.path.join(folder_path, \"faiss_index\")\n","\n","# Load the embedding model\n","embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n","\n","# Load the index\n","vectorstore = FAISS.load_local(faiss_index_path, embeddings=embeddings, allow_dangerous_deserialization=True)"]},{"cell_type":"markdown","metadata":{"id":"A7y-awSYZNRT"},"source":["## Query the Index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VeAXeZkXY9s-"},"outputs":[],"source":["query = \"What is hallucination in AI?\"\n","docs = vectorstore.similarity_search(query, k=3)\n","\n","for i, doc in enumerate(docs, 1):\n","    print(f\"\\n--- Result {i} ---\\n\")\n","    print(doc.page_content)\n"]},{"cell_type":"markdown","metadata":{"id":"_U1swqb1EQ23"},"source":["## store the API Key"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T3w89ORmBhPW"},"outputs":[],"source":["import os\n","from getpass import getpass\n","\n","# Prompt you to enter API key securely\n","os.environ[\"GOOGLE_API_KEY\"] = getpass(\"Enter your Gemini API key: \")\n","\n","# Check (for debugging only; remove print later)\n","print(\"Key set successfully!\")\n"]},{"cell_type":"markdown","metadata":{"id":"73QU47E0ecA4"},"source":["## FAISS index + retrieval + LLM answering pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AM4PIe7HZbw8"},"outputs":[],"source":["# --- Load FAISS index and set up QA pipeline ---\n","\n","from langchain.vectorstores import FAISS\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from langchain.chains import RetrievalQA\n","from langchain.prompts import PromptTemplate\n","import os\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","\n","# 1. Load embeddings model\n","embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n","\n","# 2. Load FAISS index from path\n","FAISS_PATH = \"/content/drive/MyDrive/RAG_demo/faiss_index\"\n","faiss_index = FAISS.load_local(FAISS_PATH, embeddings, allow_dangerous_deserialization=True)\n","\n","\n","# Prompt you to enter API key securely\n","os.environ[\"GOOGLE_API_KEY\"] = getpass(\"Enter your Google API key: \")\n","\n","llm = ChatGoogleGenerativeAI(\n","    model=\"gemini-1.5-flash\",\n","    temperature=0,\n","    google_api_key=os.environ[\"GOOGLE_API_KEY\"]  # force usage of API key\n",")\n","\n","\n","# 3. Initialize LLM\n","llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n","\n","# 4. Create QA chain with custom prompt\n","template = \"\"\"\n","Use the provided documents to answer the question.\n","If the answer is not in the documents, say:\n","\"The answer is not available in the provided context.\"\n","\n","Context:\n","{context}\n","\n","Question: {question}\n","Answer:\n","\"\"\"\n","QA_PROMPT = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n","\n","qa = RetrievalQA.from_chain_type(\n","    llm=llm,\n","    retriever=faiss_index.as_retriever(),\n","    chain_type=\"stuff\",\n","    chain_type_kwargs={\"prompt\": QA_PROMPT}\n",")\n","\n","# 5. Wrap into a function\n","def query_docs(question):\n","    return qa.run(question)\n","\n","# Example usage:\n","print(query_docs(\"What does the document say about RAG pipelines?\"))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wPS4li68tUap"},"outputs":[],"source":["!pip install langchain-openai --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nnsbU_qjnfZo"},"outputs":[],"source":["# --- Step 1: Imports ---\n","from langchain_community.vectorstores import FAISS\n","from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n","from langchain.prompts import PromptTemplate\n","from langchain.chains import RetrievalQA\n","import os\n","from langchain.vectorstores import FAISS\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from langchain.chains import RetrievalQA\n","from langchain.prompts import PromptTemplate\n","import os\n","from typing import List\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from getpass import getpass\n","\n","# --- Step 2: Configuration ---\n","FAISS_PATH = \"/content/drive/MyDrive/RAG_demo/faiss_index\"\n","# Prompt you to enter API key securely\n","os.environ[\"GOOGLE_API_KEY\"] = getpass(\"Enter your Google API key: \")\n","\n","# --- Step 3: Load FAISS index ---\n","embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n","vectorstore = FAISS.load_local(FAISS_PATH, embedding_model, allow_dangerous_deserialization=True)\n","\n","# --- Step 4: Setup LLM ---\n","llm = ChatGoogleGenerativeAI(\n","    model=\"gemini-1.5-flash\",\n","    temperature=0,\n","    google_api_key=os.environ[\"GOOGLE_API_KEY\"]\n",")\n","\n","# --- Step 5: Create prompt template (optional for better control) ---\n","prompt_template = \"\"\"\n","Use the provided context to answer the question.\n","If the answer is not in the context, say you don't know.\n","Also, return the source documents.\n","\n","Context:\n","{context}\n","\n","Question:\n","{question}\n","\n","Answer:\n","\"\"\"\n","prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n","\n","# --- Step 6: Wrap into function ---\n","def query_with_citations(question: str):\n","    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n","    docs = retriever.get_relevant_documents(question)\n","\n","    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n","    formatted_prompt = prompt.format(context=context, question=question)\n","\n","    response = llm.invoke(formatted_prompt)\n","\n","    # Extract sources, remove duplicates, keep only file names\n","    sources = sorted({os.path.basename(doc.metadata.get(\"source\", \"Unknown\")) for doc in docs})\n","\n","    return {\n","        \"answer\": response.content.strip(),\n","        \"sources\": sources\n","    }\n","\n","# --- Step 7: Test query ---\n","if __name__ == \"__main__\":\n","    user_query = \"What is genai?\"\n","    result = query_with_citations(user_query)\n","    print(\"\\nAnswer:\", result[\"answer\"])\n","    print(\"Sources:\", \", \".join(result[\"sources\"]))\n"]},{"cell_type":"markdown","metadata":{"id":"kemD4C0Lfy1l"},"source":["## Add Re-ranking"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DTtGmlz5L-d4"},"outputs":[],"source":["# --- Step 1: Imports ---\n","from langchain_community.vectorstores import FAISS\n","from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n","from langchain.prompts import PromptTemplate\n","from langchain.chains import RetrievalQA\n","import os\n","from langchain.vectorstores import FAISS\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from langchain.chains import RetrievalQA\n","from langchain.prompts import PromptTemplate\n","import os\n","from typing import List\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from getpass import getpass\n","\n","# --- Step 2: Configuration ---\n","FAISS_PATH = \"/content/drive/MyDrive/RAG_demo/faiss_index\"\n","# Prompt you to enter API key securely\n","os.environ[\"GOOGLE_API_KEY\"] = getpass(\"Enter your Google API key: \")\n","\n","# --- Step 3: Load FAISS index ---\n","embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n","vectorstore = FAISS.load_local(FAISS_PATH, embedding_model, allow_dangerous_deserialization=True)\n","\n","# --- Step 4: Setup LLM ---\n","llm = ChatGoogleGenerativeAI(\n","    model=\"gemini-1.5-flash\",\n","    temperature=0,\n","    google_api_key=os.environ[\"GOOGLE_API_KEY\"]\n",")\n","\n","# --- Step 5: Create prompt template ---\n","prompt_template = \"\"\"\n","Use the provided context to answer the question.\n","If the answer is not in the context, say: I couldn't find anything in my knowledge base about that topic. I can only answer questions related to AI, RAG, and the documents you provided.\n","\n","Context:\n","{context}\n","\n","Question:\n","{question}\n","\n","Answer:\n","\"\"\"\n","prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n","\n","# --- Step 6: Wrap into function ---\n","def query_with_citations(question: str):\n","    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})  # fetch more\n","    docs = retriever.get_relevant_documents(question)\n","\n","    # Keep top 3 by vector similarity\n","    docs = docs[:3]\n","\n","    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n","    formatted_prompt = prompt.format(context=context, question=question)\n","\n","    response = llm.invoke(formatted_prompt)\n","    answer_text = response.content.strip()\n","\n","    # Only show sources if it's not the fallback\n","    fallback_msg = \"I couldn't find anything in my knowledge base about that topic\"\n","    if fallback_msg.lower() in answer_text.lower():\n","        sources = []  # No sources for out-of-scope answers\n","    else:\n","        sources = list(set(doc.metadata.get(\"source\", \"Unknown\") for doc in docs))\n","\n","    return {\n","        \"answer\": answer_text,\n","        \"sources\": sources\n","    }\n","\n","# --- Step 7: Test query ---\n","if __name__ == \"__main__\":\n","    user_query = \"what is genai?\"\n","    result = query_with_citations(user_query)\n","\n","    print(\"\\nAnswer:\", result[\"answer\"])\n","    if result[\"sources\"]:\n","        print(\"Sources:\", \", \".join(result[\"sources\"]))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AiHY3QEtUXMP"},"outputs":[],"source":["# --- Step 0: Install needed packages ---\n","!pip install faiss-cpu langchain-community langchain-google-genai sentence-transformers --quiet"]},{"cell_type":"markdown","metadata":{"id":"hgmvtY1x9ALz"},"source":["## Full RAG Pipeline with citations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gn7BfUoy3rim"},"outputs":[],"source":["# --- Step 1: Imports ---\n","from langchain_community.vectorstores import FAISS\n","from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n","from langchain.prompts import PromptTemplate\n","from langchain.chains import RetrievalQA\n","import os\n","from langchain.vectorstores import FAISS\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from langchain.chains import RetrievalQA\n","from langchain.prompts import PromptTemplate\n","import os\n","from typing import List\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from getpass import getpass\n","\n","# --- Step 2: Configuration ---\n","FAISS_PATH = \"/content/drive/MyDrive/RAG_demo/faiss_index\"\n","# Prompt you to enter API key securely\n","os.environ[\"GOOGLE_API_KEY\"] = getpass(\"Enter your Google API key: \")\n","\n","# --- Step 3: Load FAISS index ---\n","embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n","vectorstore = FAISS.load_local(FAISS_PATH, embedding_model, allow_dangerous_deserialization=True)\n","\n","# --- Step 4: Setup LLM ---\n","llm = ChatGoogleGenerativeAI(\n","    model=\"gemini-1.5-flash\",\n","    temperature=0,\n","    google_api_key=os.environ[\"GOOGLE_API_KEY\"]\n",")\n","\n","# --- Step 5: Create prompt template ---\n","prompt_template = \"\"\"\n","Use the provided context to answer the question.\n","If the answer is not in the context, say: I couldn't find anything in my knowledge base about that topic. I can only answer questions related to AI, RAG, and the documents you provided.\n","\n","Context:\n","{context}\n","\n","Question:\n","{question}\n","\n","Answer:\n","\"\"\"\n","prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n","\n","# --- Step 6: Wrap into function ---\n","def query_with_citations(question: str):\n","    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})  # fetch more\n","    docs = retriever.get_relevant_documents(question)\n","\n","    # Keep top 3 by vector similarity\n","    docs = docs[:3]\n","\n","    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n","    formatted_prompt = prompt.format(context=context, question=question)\n","\n","    response = llm.invoke(formatted_prompt)\n","    answer_text = response.content.strip()\n","\n","    # Mapping from file paths to desired URL format\n","    source_url_map = {\n","        \"/content/drive/MyDrive/RAG_demo/data/genai-platform.txt\": \"https://huyenchip.com/2024/07/25/genai-platform.html\",\n","        \"/content/drive/MyDrive/RAG_demo/data/hallucination.txt\": \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n","        \"/content/drive/MyDrive/RAG_demo/data/quora_engineering.txt\": \"https://quoraengineering.quora.com/Building-Embedding-Search-at-Quora\"\n","    }\n","\n","    def format_sources(sources):\n","        \"\"\"Convert FAISS/metadata sources to consistent URL format.\"\"\"\n","        return [source_url_map.get(src, src) for src in sources]\n","\n","    # Only show sources if it's not the fallback\n","    fallback_msg = \"I couldn't find anything in my knowledge base about that topic\"\n","    if fallback_msg.lower() in answer_text.lower():\n","        sources = []  # No sources for out-of-scope answers\n","    else:\n","        sources = list(set(doc.metadata.get(\"source\", \"Unknown\") for doc in docs))\n","        sources = format_sources(sources)  # <-- Apply URL mapping here\n","\n","    return {\n","        \"answer\": answer_text,\n","        \"sources\": sources\n","    }\n","\n","# --- Step 7: Test query ---\n","if __name__ == \"__main__\":\n","    user_query = \"What is embedding based retieval?\"\n","    result = query_with_citations(user_query)\n","\n","    print(\"\\nAnswer:\", result[\"answer\"])\n","    if result[\"sources\"]:\n","        print(\"Sources:\", \", \".join(result[\"sources\"]))\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iOthQPyB64sj"},"source":["# HYBRID RETRIEVAL (BM25 + FAISS) + reranker"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i_ecTWuP6auB"},"outputs":[],"source":["!pip install rank_bm25 sentence-transformers faiss-cpu --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hNQSjvVj1TVo"},"outputs":[],"source":["# ===== HYBRID RETRIEVAL (BM25 + FAISS) + optional reranker =====\n","from rank_bm25 import BM25Okapi\n","from sentence_transformers import CrossEncoder\n","import os, pickle\n","from typing import List\n","from collections import OrderedDict\n","\n","# ---to match your environment ---\n","FAISS_PATH = \"/content/drive/MyDrive/RAG_demo/faiss_index\"   # saved FAISS\n","USE_RERANKER = True           # set False if you don't want Cross-Encoder reranking\n","RERANKER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"  # small & fast\n","BM25_TOP_K = 10               # initial BM25 candidates\n","FAISS_TOP_K = 10              # initial FAISS candidates\n","FINAL_TOP_K = 3               # final docs to pass to LLM\n","# ----------------------------------------------------------------\n","\n","\n","# Prompt you to enter API key securely\n","os.environ[\"GOOGLE_API_KEY\"] = getpass(\"Enter your Google API key: \")\n","\n","# ---Load FAISS index ---\n","embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n","vectorstore = FAISS.load_local(FAISS_PATH, embedding_model, allow_dangerous_deserialization=True)\n","\n","# ---Setup LLM ---\n","llm = ChatGoogleGenerativeAI(\n","    model=\"gemini-1.5-flash\",\n","    temperature=0,\n","    google_api_key=os.environ[\"GOOGLE_API_KEY\"]\n",")\n","\n","# ---Create prompt template ---\n","prompt_template = \"\"\"\n","Use the provided context to answer the question.\n","If the answer is not in the context, say: I couldn't find anything in my knowledge base about that topic. I can only answer questions related to AI, RAG, and the documents you provided.\n","\n","Context:\n","{context}\n","\n","Question:\n","{question}\n","\n","Answer:\n","\"\"\"\n","prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n","\n","# 1) Build BM25 corpus from your FAISS docstore (texts and metadata)\n","def build_bm25_from_vectorstore(vectorstore):\n","    docs = []\n","    sources = []\n","    # vectorstore.docstore._dict values are Document objects for many LangChain stores\n","    for doc_id, doc in vectorstore.docstore._dict.items():\n","        text = getattr(doc, \"page_content\", None) or doc.page_content\n","        docs.append(text)\n","        sources.append(doc.metadata.get(\"source\", \"Unknown\"))\n","    # simple tokenization: whitespace split (you can improve using nltk/spacy if needed)\n","    tokenized = [d.split() for d in docs]\n","    bm25 = BM25Okapi(tokenized)\n","    return bm25, docs, tokenized, sources\n","\n","print(\"Building BM25 index from vectorstore (this may take a sec)...\")\n","bm25, bm25_docs, bm25_tokenized, bm25_sources = build_bm25_from_vectorstore(vectorstore)\n","print(f\"BM25 built over {len(bm25_docs)} chunks.\")\n","\n","# Optionally persist BM25 for faster reload\n","BM25_PICKLE = os.path.join(FAISS_PATH, \"bm25.pkl\")\n","with open(BM25_PICKLE, \"wb\") as f:\n","    pickle.dump({\"bm25_docs\": bm25_docs, \"bm25_sources\": bm25_sources}, f)\n","\n","# 2) Optional: load cross-encoder reranker\n","if USE_RERANKER:\n","    print(\"Loading cross-encoder reranker (may take memory/time)...\")\n","    reranker = CrossEncoder(RERANKER_MODEL)\n","else:\n","    reranker = None\n","\n","# 3) Helper: map doc text -> Document object(s) from vectorstore\n","# We'll build a quick map from text to the stored Document instance(s)\n","text_to_docs = {}\n","for doc in vectorstore.docstore._dict.values():\n","    text = doc.page_content\n","    text_to_docs.setdefault(text, []).append(doc)\n","\n","# 4) Hybrid search function\n","def hybrid_search(query: str, bm_k:int=BM25_TOP_K, faiss_k:int=FAISS_TOP_K, top_k:int=FINAL_TOP_K) -> List:\n","    # 4a) BM25 candidates (by index of bm25_docs)\n","    tokenized_q = query.split()\n","    bm25_scores = bm25.get_scores(tokenized_q)\n","    bm25_indices = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:bm_k]\n","    bm25_candidates = []\n","    for idx in bm25_indices:\n","        txt = bm25_docs[idx]\n","        src = bm25_sources[idx]\n","        bm25_candidates.append((txt, src, bm25_scores[idx]))\n","\n","    # 4b) FAISS candidates (semantic)\n","    faiss_retriever = vectorstore.as_retriever(search_kwargs={\"k\": faiss_k})\n","    faiss_docs = faiss_retriever.get_relevant_documents(query)  # returns Document objects\n","    faiss_candidates = [(d.page_content, d.metadata.get(\"source\",\"Unknown\"), None) for d in faiss_docs]\n","\n","    # 4c) Merge candidates (keep order by source of score: BM25 first then FAISS)\n","    merged = OrderedDict()  # preserve first appearance\n","    for txt, src, score in bm25_candidates + faiss_candidates:\n","        key = txt.strip()\n","        if key not in merged:\n","            merged[key] = {\"text\": txt, \"source\": src, \"bm25_score\": score}\n","\n","    candidates = list(merged.values())  # list of dicts\n","\n","    # 4d) (Optional) Rerank candidates with cross-encoder: produce scores for (query, text)\n","    if reranker and candidates:\n","        pairs = [(query, c[\"text\"]) for c in candidates]\n","        scores = reranker.predict(pairs)\n","        for c, s in zip(candidates, scores):\n","            c[\"rerank_score\"] = float(s)\n","        # Sort by rerank_score desc\n","        candidates = sorted(candidates, key=lambda x: x[\"rerank_score\"], reverse=True)\n","    else:\n","        # fallback: keep bm25_score non-None then no particular order, or try simple heuristic:\n","        # prefer BM25 hits first (where bm25_score not None), then FAISS\n","        candidates = sorted(candidates, key=lambda x: (0 if x[\"bm25_score\"] is not None else 1, -(x[\"bm25_score\"] or 0)))\n","\n","    # 4e) Return top_k Document objects (use text_to_docs map to return actual Document objs)\n","    final_docs = []\n","    for c in candidates[:top_k]:\n","        text = c[\"text\"]\n","        docs_list = text_to_docs.get(text, [])\n","        if docs_list:\n","            # there may be multiple Doc objects with same text; pick first\n","            final_docs.append(docs_list[0])\n","        else:\n","            # Shouldn't happen if bm25 built from same corpus; but fallback: create pseudo doc\n","            from langchain.docstore.document import Document\n","            final_docs.append(Document(page_content=text, metadata={\"source\": c.get(\"source\",\"Unknown\")}))\n","    return final_docs\n","\n","# 5) Integrate into your query_with_citations\n","import os\n","source_url_map = {\n","    \"/content/drive/MyDrive/RAG_demo/data/genai-platform.txt\": \"https://huyenchip.com/2024/07/25/genai-platform.html\",\n","    \"/content/drive/MyDrive/RAG_demo/data/hallucination.txt\": \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n","    \"/content/drive/MyDrive/RAG_demo/data/quora_engineering.txt\": \"https://quoraengineering.quora.com/Building-Embedding-Search-at-Quora\"\n","}\n","def format_sources(sources):\n","    return [source_url_map.get(s, s) for s in sources]\n","\n","# Make sure prompt, llm exist in your notebook\n","def query_with_citations_hybrid(question: str):\n","    # Get hybrid top docs\n","    docs = hybrid_search(question, bm_k=BM25_TOP_K, faiss_k=FAISS_TOP_K, top_k=FINAL_TOP_K)\n","\n","    # Create context and ask LLM\n","    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n","    formatted_prompt = prompt.format(context=context, question=question)\n","    response = llm.invoke(formatted_prompt)\n","    answer_text = response.content.strip()\n","\n","    # fallback detection (no sources shown for out-of-scope)\n","    fallback_msg = \"i couldn't find anything in my knowledge base about that topic\"\n","    if fallback_msg.lower() in answer_text.lower():\n","        sources = []\n","    else:\n","        sources = list(OrderedDict.fromkeys(doc.metadata.get(\"source\",\"Unknown\") for doc in docs))  # dedupe keep order\n","        sources = format_sources(sources)\n","    return {\"answer\": answer_text, \"sources\": sources}\n","\n","# 6) Quick test\n","print(\"Running quick hybrid test...\")\n","q = \"What is ColBERT and why late interaction matters?\"\n","res = query_with_citations_hybrid(q)\n","print(\"\\nAnswer:\", res[\"answer\"][:400])\n","print(\"Sources:\", res[\"sources\"])\n"]},{"cell_type":"markdown","metadata":{"id":"B1arqvdk56go"},"source":["# EVALUATION METRICS"]},{"cell_type":"markdown","metadata":{"id":"NVNRC86054ZE"},"source":["##  LLM-as-a-judge"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"saYe56Yi4KbT"},"outputs":[],"source":["# --- LLM Judge Setup ---\n","# Prompt you to enter API key securely\n","os.environ[\"GOOGLE_API_KEY\"] = getpass(\"Enter your Google API key: \")\n","\n","# Initialize a separate LLM for judging\n","judge_llm = ChatGoogleGenerativeAI(\n","    model=\"gemini-1.5-flash\",  # You can choose a different model if needed\n","    temperature=0,\n","    google_api_key=os.environ[\"GOOGLE_API_KEY\"]\n",")\n","\n","\n","# Example ground truth set\n","gold_data = [\n","    {\n","        \"question\": \"What is ColBERT and why late interaction matters?\",\n","        \"ground_truth\": \"ColBERT is a retrieval model developed at Stanford that uses BERT embeddings with a late interaction mechanism. Late interaction improves efficiency by separating query and document processing until the final scoring step, balancing accuracy with scalability.\",\n","        \"expected_citations\": [\n","            \"https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/\"\n","        ]\n","    }\n","    # Add more entries here...\n","]\n","\n","def llm_judge(question, system_answer, citations, ground_truth, expected_citations, judge_llm):\n","    judge_prompt = f\"\"\"\n","    You are evaluating an AI system's answer.\n","\n","    Question: {question}\n","    System Answer: {system_answer}\n","    Ground Truth Answer: {ground_truth}\n","    System Citations: {citations}\n","    Expected Citations: {expected_citations}\n","\n","    Evaluate on:\n","    1. Accuracy (0 to 1): factual correctness compared to ground truth.\n","    2. Coverage (0 to 1): completeness of answer.\n","    3. Citation Match (0 to 1): citation relevance.\n","\n","    Respond ONLY in JSON:\n","    {{ \"accuracy\": float, \"coverage\": float, \"citation_match\": float }}\n","    \"\"\"\n","\n","    response = judge_llm.invoke(judge_prompt) # Use judge_llm here\n","    return response.content\n","\n","# --- Loop through gold dataset with hybrid retriever + judge ---\n","for item in gold_data:\n","    question = item[\"question\"]\n","    ground_truth = item[\"ground_truth\"]\n","    expected_citations = item[\"expected_citations\"]\n","\n","    # Run your hybrid retriever (make sure query_with_citations_hybrid is defined)\n","    result = query_with_citations_hybrid(question)\n","    answer = result[\"answer\"]\n","    citations = result[\"sources\"]\n","\n","    # Judge evaluation\n","    scores = llm_judge(question, answer, citations, ground_truth, expected_citations, judge_llm)\n","\n","    # Display results\n","    print(f\"\\nQ: {question}\")\n","    print(f\"Answer: {answer}\")\n","    print(f\"Citations: {citations}\")\n","    print(f\"Judge Scores: {scores}\")"]},{"cell_type":"markdown","metadata":{"id":"vh2jO16770E6"},"source":["## LLM-as-a-Judge Evaluation\n","\n","To assess the quality of answers generated by the hybrid retriever (BM25 + FAISS + Cross-Encoder Reranker), I integrated an LLM-as-a-judge evaluation layer. In this approach, a large language model reviews each retrieved answer against the cited sources and produces quantitative scores:\n","\n","Accuracy – Measures factual correctness of the answer relative to the source (0 to 1).\n","\n","Coverage – Evaluates how completely the answer addresses relevant information from the source (0 to 1).\n","\n","Citation Match – Checks whether the cited references align with the actual content used (0 to 1).\n","\n","For example, for the query \"What is ColBERT and why late interaction matters?\", the system generated the following metrics:\n","\n","Metric\tScore\n","Accuracy\t0.95\n","Coverage\t0.90\n","Citation Match\t1.00\n","\n","These scores indicate the system produced an accurate, well-covered, and perfectly cited answer. Running this evaluation across multiple queries provides a quantitative measure of retrieval and generation quality, making the system more robust."]},{"cell_type":"markdown","metadata":{"id":"BhQmgHYyJG_v"},"source":["## For testing only the retriever part before LLM answering"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pw3nyUiR91ql"},"outputs":[],"source":["test_queries = [\n","    {\n","        \"question\": \"What is embedding based retieval?\",\n","        \"relevant_sources\": [\n","            \"https://huyenchip.com/2024/07/25/genai-platform.html\"\n","        ]\n","    },\n","    {\n","        \"question\": \"Explain ColBERT model.\",\n","        \"relevant_sources\": [\n","            \"https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/\"\n","        ]\n","    },\n","    {\n","        \"question\": \"What causes hallucination?\",\n","        \"relevant_sources\": [\n","            \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\"\n","        ]\n","    },\n","    {\n","        \"question\": \"why qdrant\",\n","        \"relevant_sources\": [\n","            \"https://quoraengineering.quora.com/Building-Embedding-Search-at-Quora\"\n","        ]\n","    }\n","]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"adKfwv5NZixy"},"outputs":[],"source":["def evaluate_retrieval(vectorstore, test_queries, k=5):\n","    # Mapping local file paths to canonical URLs\n","    source_mapping = {\n","        \"/content/drive/MyDrive/RAG_demo/data/genai-platform.txt\": \"https://huyenchip.com/2024/07/25/genai-platform.html\",\n","        \"/content/drive/MyDrive/RAG_demo/data/hallucination.txt\": \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n","        \"/content/drive/MyDrive/RAG_demo/data/quora_engineering.txt\": \"https://quoraengineering.quora.com/Building-Embedding-Search-at-Quora\"\n","    }\n","\n","    def normalize_source(source):\n","        return source_mapping.get(source, source)  # Replace if in mapping\n","\n","    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n","\n","    all_precisions = []\n","    all_recalls = []\n","\n","    for tq in test_queries:\n","        docs = retriever.get_relevant_documents(tq[\"question\"])\n","\n","        # Normalize retrieved sources\n","        retrieved_sources = list(set(normalize_source(doc.metadata.get(\"source\", \"\")) for doc in docs))\n","\n","        # Convert to binary relevance\n","        y_true = [1 if src in tq[\"relevant_sources\"] else 0 for src in retrieved_sources]\n","\n","        if len(y_true) > 0:\n","            precision = sum(y_true) / len(y_true)  # TP / (TP+FP)\n","            recall = sum(y_true) / len(tq[\"relevant_sources\"])  # TP / (TP+FN)\n","        else:\n","            precision, recall = 0, 0\n","\n","        all_precisions.append(precision)\n","        all_recalls.append(recall)\n","\n","    avg_precision = sum(all_precisions) / len(all_precisions)\n","    avg_recall = sum(all_recalls) / len(all_recalls)\n","\n","    return avg_precision, avg_recall\n","\n","\n","# Example usage:\n","avg_p, avg_r = evaluate_retrieval(vectorstore, test_queries, k=5)\n","print(f\"Average Precision@5: {avg_p:.2f}\")\n","print(f\"Average Recall@5: {avg_r:.2f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FK7W3pYDJf5e"},"outputs":[],"source":["def normalize_source(src):\n","    \"\"\"\n","    Normalizes source paths/URLs for matching.\n","    - Strips whitespace\n","    - Converts local file paths to their corresponding URLs if mapping exists\n","    - Lowercases for case-insensitive match\n","    \"\"\"\n","    mapping = {\n","        \"/content/drive/MyDrive/RAG_demo/data/genai-platform.txt\": \"https://huyenchip.com/2024/07/25/genai-platform.html\",\n","        \"/content/drive/MyDrive/RAG_demo/data/hallucination.txt\": \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n","        \"/content/drive/MyDrive/RAG_demo/data/quora_engineering.txt\": \"https://quoraengineering.quora.com/Building-Embedding-Search-at-Quora\"\n","    }\n","    src = src.strip()\n","    return mapping.get(src, src).lower()\n","\n","\n","def evaluate_retrieval_with_per_query(vectorstore, test_queries, k=5):\n","    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n","\n","    all_precisions = []\n","    all_recalls = []\n","\n","    print(\"\\n=== Per-query Retrieval Evaluation (Normalized) ===\")\n","    for tq in test_queries:\n","        docs = retriever.get_relevant_documents(tq[\"question\"])\n","\n","        retrieved_sources = list(set(normalize_source(doc.metadata.get(\"source\", \"\")) for doc in docs))\n","        relevant_sources = [normalize_source(src) for src in tq[\"relevant_sources\"]]\n","\n","        # Compute precision & recall\n","        if retrieved_sources:\n","            tp = sum(1 for src in retrieved_sources if src in relevant_sources)\n","            precision = tp / len(retrieved_sources)\n","            recall = tp / len(relevant_sources)\n","        else:\n","            precision, recall = 0, 0\n","\n","        all_precisions.append(precision)\n","        all_recalls.append(recall)\n","\n","        print(f\"Q: {tq['question']}\")\n","        print(f\"  Retrieved: {retrieved_sources}\")\n","        print(f\"  Relevant : {relevant_sources}\")\n","        print(f\"  Precision@{k}: {precision:.2f}, Recall@{k}: {recall:.2f}\\n\")\n","\n","    avg_precision = sum(all_precisions) / len(all_precisions)\n","    avg_recall = sum(all_recalls) / len(all_recalls)\n","\n","    print(\"=== Overall Averages ===\")\n","    print(f\"Average Precision@{k}: {avg_precision:.2f}\")\n","    print(f\"Average Recall@{k}: {avg_recall:.2f}\")\n","\n","    return avg_precision, avg_recall\n","\n","\n","# Example usage:\n","avg_p, avg_r = evaluate_retrieval_with_per_query(vectorstore, test_queries, k=5)\n"]},{"cell_type":"markdown","metadata":{"id":"fCD-W67fLzUp"},"source":["# Evaluation Summary\n","\n","To ensure the quality and reliability of the RAG-powered question-answering system, two complementary evaluation approaches were implemented:\n","\n","1. Quantitative Metrics (Precision & Recall)\n","Method: Evaluated using normalized source URLs against a ground-truth mapping for each query.\n","\n","Metrics:\n","\n","Average Precision@5: 0.71\n","\n","Average Recall@5: 1.00\n","\n","Interpretation: The system retrieved relevant documents with high completeness (100% recall) and strong precision, indicating that almost all relevant sources were captured within the top-5 results.\n","\n","2. Qualitative Judgment (LLM-as-a-Judge)\n","\n","Method: Integrated a Large Language Model to automatically assess answers based on:\n","\n","Accuracy – factual correctness of the generated answer.\n","\n","Coverage – completeness of the answer with respect to the question.\n","\n","Citation Match – alignment of the cited sources with the retrieved documents.\n","\n","Example Result:\n","\n","Accuracy: 0.95\n","\n","Coverage: 0.90\n","\n","Citation Match: 1.00\n","\n","Interpretation: The hybrid retrieval pipeline (BM25 + FAISS + reranker) produced highly accurate, complete, and source-grounded answers.\n","\n","Overall Conclusion:\n","\n","The system achieves strong performance in both retrieval relevance (quantitative) and answer quality (qualitative), making it suitable for real-world deployment where trustworthy and well-sourced responses are required."]},{"cell_type":"markdown","metadata":{"id":"mA7EaOdxFU2l"},"source":["# RAG API for single query with Dynamic Indexing\n","\n","Hybrid Retrieval-Augmented Generation API with BM25 + FAISS and Dynamic Web Indexing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lltj6lpnEcdp"},"outputs":[],"source":["# Install packages\n","!pip install fastapi uvicorn pyngrok nest-asyncio --quiet\n","!pip install rank-bm25 sentence-transformers langchain-community langchain-google-genai faiss-cpu --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kNuKu6y3lsIe"},"outputs":[],"source":["# Setup imports and basic configuration\n","import os\n","import json\n","import time\n","import shutil\n","from datetime import datetime, timedelta\n","from typing import List, Optional, Dict, Any\n","from urllib.parse import urlparse, urljoin\n","import requests\n","from bs4 import BeautifulSoup\n","import nest_asyncio\n","from pyngrok import ngrok\n","import uvicorn\n","from fastapi import FastAPI, HTTPException, Depends, status, Body\n","from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n","from fastapi.middleware.cors import CORSMiddleware\n","from pydantic import BaseModel, Field, validator\n","import logging\n","from getpass import getpass\n","from collections import OrderedDict\n","import hashlib\n","import re\n","\n","# Import the RAG components\n","from rank_bm25 import BM25Okapi\n","from sentence_transformers import CrossEncoder\n","from langchain_community.embeddings import HuggingFaceEmbeddings\n","from langchain_community.vectorstores import FAISS\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from langchain.prompts import PromptTemplate\n","from langchain.docstore.document import Document\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","# Enable asyncio in Colab\n","nest_asyncio.apply()\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","print(\"✅ All imports successful!\")\n","\n","# Set up API keys\n","# Set Google API key\n","if \"GOOGLE_API_KEY\" not in os.environ:\n","    os.environ[\"GOOGLE_API_KEY\"] = getpass(\"Enter your Google API key: \")\n","\n","# Set ngrok token\n","ngrok_token = getpass(\"Enter your ngrok token (get free at https://dashboard.ngrok.com/get-started/your-authtoken): \")\n","ngrok.set_auth_token(ngrok_token)\n","\n","print(\"✅ API keys configured!\")\n","\n","# Define the FastAPI app\n","app = FastAPI(\n","    title=\"RAG API with Dynamic Indexing\",\n","    description=\"Hybrid Retrieval-Augmented Generation API with BM25 + FAISS and Dynamic Web Indexing\",\n","    version=\"2.0.0\"\n",")\n","\n","app.add_middleware(\n","    CORSMiddleware,\n","    allow_origins=[\"*\"],\n","    allow_credentials=True,\n","    allow_methods=[\"*\"],\n","    allow_headers=[\"*\"],\n",")\n","\n","security = HTTPBearer()\n","\n","# Configuration\n","class Config:\n","    # Static index (existing)\n","    STATIC_FAISS_PATH = \"/content/drive/MyDrive/RAG_demo/faiss_index\"\n","\n","    # Dynamic index (new)\n","    DYNAMIC_BASE_PATH = \"/content/drive/MyDrive/RAG_demo/dynamic_index\"\n","    DYNAMIC_FAISS_PATH = \"/content/drive/MyDrive/RAG_demo/dynamic_index/faiss_index\"\n","    METADATA_PATH = \"/content/drive/MyDrive/RAG_demo/dynamic_index/metadata\"\n","    BACKUP_PATH = \"/content/drive/MyDrive/RAG_demo/dynamic_index/backups\"\n","\n","    # Indexing settings\n","    USE_RERANKER = True\n","    RERANKER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n","    BM25_TOP_K = 10\n","    FAISS_TOP_K = 10\n","    FINAL_TOP_K = 3\n","\n","    # Web scraping settings\n","    REQUEST_TIMEOUT = 30\n","    MAX_RETRIES = 3\n","    RETRY_DELAY = 2\n","    CHUNK_SIZE = 1000\n","    CHUNK_OVERLAP = 100\n","    RE_INDEX_DAYS = 7\n","    MAX_VERSIONS = 3\n","\n","    # Authentication\n","    VALID_API_KEYS = {\n","        \"demo-api-key-123\": {\"user\": \"demo_user\", \"permissions\": [\"read\", \"query\", \"index\"]},\n","    }\n","\n","config = Config()\n","\n","# Pydantic models\n","class IndexRequest(BaseModel):\n","    url: List[str] = Field(..., min_items=1, max_items=10, description=\"URLs to index\")\n","\n","    @validator('url')\n","    def validate_urls(cls, v):\n","        for url in v:\n","            parsed = urlparse(url)\n","            if not parsed.scheme or not parsed.netloc:\n","                raise ValueError(f\"Invalid URL format: {url}\")\n","        return v\n","\n","class IndexResponse(BaseModel):\n","    status: str\n","    indexed_url: List[str]\n","    failed_url: Optional[List[Dict[str, str]]] = None\n","    metadata: Dict[str, Any] = Field(default_factory=dict)\n","    timestamp: datetime = Field(default_factory=datetime.now)\n","\n","class QueryRequest(BaseModel):\n","    question: str = Field(..., min_length=1, max_length=1000, description=\"The question to ask\")\n","    top_k: Optional[int] = Field(default=3, ge=1, le=10, description=\"Number of results to return\")\n","    use_reranker: Optional[bool] = Field(default=True, description=\"Whether to use cross-encoder reranking\")\n","    use_dynamic_index: Optional[bool] = Field(default=True, description=\"Whether to search dynamic index\")\n","\n","class QueryResponse(BaseModel):\n","    answer: str\n","    sources: List[str]\n","    metadata: dict = Field(default_factory=dict)\n","    timestamp: datetime = Field(default_factory=datetime.now)\n","\n","class HealthResponse(BaseModel):\n","    status: str\n","    timestamp: datetime = Field(default_factory=datetime.now)\n","    components: dict\n","\n","# Global variables\n","embedding_model = None\n","static_vectorstore = None\n","dynamic_vectorstore = None\n","llm = None\n","bm25_static = None\n","bm25_dynamic = None\n","bm25_docs_static = None\n","bm25_docs_dynamic = None\n","bm25_sources_static = None\n","bm25_sources_dynamic = None\n","reranker = None\n","text_to_docs_static = None\n","text_to_docs_dynamic = None\n","prompt = None\n","text_splitter = None\n","\n","print(\"✅ FastAPI app configured!\")\n","\n","# Authentication\n","async def verify_api_key(credentials: HTTPAuthorizationCredentials = Depends(security)):\n","    \"\"\"Verify API key from Authorization header\"\"\"\n","    api_key = credentials.credentials\n","    if api_key not in config.VALID_API_KEYS:\n","        logger.warning(f\"Invalid API key attempted: {api_key[:10]}...\")\n","        raise HTTPException(\n","            status_code=status.HTTP_401_UNAUTHORIZED,\n","            detail=\"Invalid API key\",\n","            headers={\"WWW-Authenticate\": \"Bearer\"},\n","        )\n","    return config.VALID_API_KEYS[api_key]\n","\n","# Utility functions\n","def ensure_directories():\n","    \"\"\"Ensure all required directories exist\"\"\"\n","    directories = [\n","        config.DYNAMIC_BASE_PATH,\n","        config.DYNAMIC_FAISS_PATH,\n","        config.METADATA_PATH,\n","        config.BACKUP_PATH\n","    ]\n","    for directory in directories:\n","        os.makedirs(directory, exist_ok=True)\n","    logger.info(\"✅ Directory structure created\")\n","\n","def get_url_hash(url: str) -> str:\n","    \"\"\"Generate a hash for URL to use as unique identifier\"\"\"\n","    return hashlib.md5(url.encode()).hexdigest()\n","\n","def load_metadata(file_path: str) -> Dict:\n","    \"\"\"Load metadata from JSON file\"\"\"\n","    if os.path.exists(file_path):\n","        try:\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                return json.load(f)\n","        except Exception as e:\n","            logger.error(f\"Error loading metadata from {file_path}: {str(e)}\")\n","    return {}\n","\n","def save_metadata(data: Dict, file_path: str):\n","    \"\"\"Save metadata to JSON file\"\"\"\n","    try:\n","        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n","        with open(file_path, 'w', encoding='utf-8') as f:\n","            json.dump(data, f, indent=2, default=str)\n","    except Exception as e:\n","        logger.error(f\"Error saving metadata to {file_path}: {str(e)}\")\n","        raise\n","\n","def should_reindex_url(url: str, metadata: Dict) -> bool:\n","    \"\"\"Check if URL should be re-indexed\"\"\"\n","    url_hash = get_url_hash(url)\n","    if url_hash not in metadata:\n","        return True\n","\n","    last_indexed = datetime.fromisoformat(metadata[url_hash]['timestamp'])\n","    age_days = (datetime.now() - last_indexed).days\n","\n","    return age_days >= config.RE_INDEX_DAYS\n","\n","# Web scraping functions\n","class WebScraper:\n","    def __init__(self):\n","        self.session = requests.Session()\n","        self.session.headers.update({\n","            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n","        })\n","\n","    def extract_content(self, url: str) -> Dict[str, Any]:\n","        \"\"\"Extract content from URL with retry logic\"\"\"\n","        for attempt in range(config.MAX_RETRIES):\n","            try:\n","                response = self.session.get(url, timeout=config.REQUEST_TIMEOUT)\n","                response.raise_for_status()\n","\n","                soup = BeautifulSoup(response.content, 'html.parser')\n","\n","                # Remove unwanted elements\n","                for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'advertisement']):\n","                    element.decompose()\n","\n","                # Extract title\n","                title = soup.find('title')\n","                title_text = title.get_text().strip() if title else url\n","\n","                # Extract main content\n","                content_selectors = [\n","                    'article', 'main', '[role=\"main\"]',\n","                    '.content', '.post', '.article',\n","                    'div.container', 'div.wrapper'\n","                ]\n","\n","                content_text = \"\"\n","                for selector in content_selectors:\n","                    content = soup.select_one(selector)\n","                    if content:\n","                        content_text = content.get_text()\n","                        break\n","\n","                if not content_text:\n","                    # Fallback to body\n","                    body = soup.find('body')\n","                    content_text = body.get_text() if body else \"\"\n","\n","                # Clean text\n","                content_text = re.sub(r'\\s+', ' ', content_text).strip()\n","\n","                if not content_text:\n","                    raise ValueError(\"No content extracted\")\n","\n","                return {\n","                    'title': title_text,\n","                    'content': content_text,\n","                    'url': url,\n","                    'success': True,\n","                    'error': None\n","                }\n","\n","            except Exception as e:\n","                logger.warning(f\"Attempt {attempt + 1} failed for {url}: {str(e)}\")\n","                if attempt < config.MAX_RETRIES - 1:\n","                    time.sleep(config.RETRY_DELAY * (2 ** attempt))\n","                else:\n","                    return {\n","                        'title': None,\n","                        'content': None,\n","                        'url': url,\n","                        'success': False,\n","                        'error': str(e)\n","                    }\n","\n","# FAISS management functions\n","def create_version_backup():\n","    \"\"\"Create a backup of current version\"\"\"\n","    if not os.path.exists(config.DYNAMIC_FAISS_PATH):\n","        return\n","\n","    versions_file = os.path.join(config.METADATA_PATH, 'versions.json')\n","    versions = load_metadata(versions_file)\n","\n","    current_version = versions.get('current_version', 0)\n","    backup_dir = os.path.join(config.BACKUP_PATH, f'v{current_version}_backup')\n","\n","    if os.path.exists(config.DYNAMIC_FAISS_PATH):\n","        if os.path.exists(backup_dir):\n","            shutil.rmtree(backup_dir)\n","        shutil.copytree(config.DYNAMIC_FAISS_PATH, backup_dir)\n","        logger.info(f\"✅ Created backup: v{current_version}\")\n","\n","def cleanup_old_backups():\n","    \"\"\"Remove old backup versions\"\"\"\n","    versions_file = os.path.join(config.METADATA_PATH, 'versions.json')\n","    versions = load_metadata(versions_file)\n","    current_version = versions.get('current_version', 0)\n","\n","    for i in range(max(0, current_version - config.MAX_VERSIONS)):\n","        old_backup = os.path.join(config.BACKUP_PATH, f'v{i}_backup')\n","        if os.path.exists(old_backup):\n","            shutil.rmtree(old_backup)\n","            logger.info(f\"🗑️ Removed old backup: v{i}\")\n","\n","def update_version():\n","    \"\"\"Update version metadata\"\"\"\n","    versions_file = os.path.join(config.METADATA_PATH, 'versions.json')\n","    versions = load_metadata(versions_file)\n","\n","    new_version = versions.get('current_version', 0) + 1\n","    versions.update({\n","        'current_version': new_version,\n","        'last_updated': datetime.now().isoformat(),\n","        'total_updates': versions.get('total_updates', 0) + 1\n","    })\n","\n","    save_metadata(versions, versions_file)\n","    return new_version\n","\n","def build_bm25_from_vectorstore(vectorstore):\n","    \"\"\"Build BM25 index from FAISS vectorstore\"\"\"\n","    try:\n","        docs = []\n","        sources = []\n","        for doc_id, doc in vectorstore.docstore._dict.items():\n","            text = getattr(doc, \"page_content\", None) or doc.page_content\n","            docs.append(text)\n","            sources.append(doc.metadata.get(\"source\", \"Unknown\"))\n","\n","        tokenized = [d.split() for d in docs]\n","        bm25 = BM25Okapi(tokenized)\n","\n","        # Build text to docs mapping\n","        text_to_docs = {}\n","        for doc in vectorstore.docstore._dict.values():\n","            text = doc.page_content\n","            text_to_docs.setdefault(text, []).append(doc)\n","\n","        logger.info(f\"BM25 built over {len(docs)} chunks\")\n","        return bm25, docs, sources, text_to_docs\n","    except Exception as e:\n","        logger.error(f\"Error building BM25: {str(e)}\")\n","        raise\n","\n","# Source URL mapping\n","source_url_map = {\n","    \"/content/drive/MyDrive/RAG_demo/data/genai-platform.txt\": \"https://huyenchip.com/2024/07/25/genai-platform.html\",\n","    \"/content/drive/MyDrive/RAG_demo/data/hallucination.txt\": \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n","    \"/content/drive/MyDrive/RAG_demo/data/quora_engineering.txt\": \"https://quoraengineering.quora.com/Building-Embedding-Search-at-Quora\"\n","}\n","\n","def format_sources(sources):\n","    \"\"\"Convert file paths to URLs\"\"\"\n","    formatted = []\n","    for source in sources:\n","        if source.startswith('http'):\n","            formatted.append(source)\n","        else:\n","            formatted.append(source_url_map.get(source, source))\n","    return formatted\n","\n","def hybrid_search(query: str, use_dynamic: bool = True, bm_k: int = 10, faiss_k: int = 10, top_k: int = 3, use_reranker: bool = True):\n","    \"\"\"Perform hybrid search on static and/or dynamic indices\"\"\"\n","    try:\n","        all_candidates = []\n","\n","        # Search static index\n","        if static_vectorstore:\n","            candidates_static = _search_single_index(\n","                query, static_vectorstore, bm25_static, bm25_docs_static,\n","                bm25_sources_static, text_to_docs_static, bm_k, faiss_k\n","            )\n","            all_candidates.extend(candidates_static)\n","\n","        # Search dynamic index\n","        if use_dynamic and dynamic_vectorstore:\n","            candidates_dynamic = _search_single_index(\n","                query, dynamic_vectorstore, bm25_dynamic, bm25_docs_dynamic,\n","                bm25_sources_dynamic, text_to_docs_dynamic, bm_k, faiss_k\n","            )\n","            all_candidates.extend(candidates_dynamic)\n","\n","        if not all_candidates:\n","            return []\n","\n","        # Merge and deduplicate candidates\n","        merged = OrderedDict()\n","        for candidate in all_candidates:\n","            key = candidate[\"text\"].strip()\n","            if key not in merged or (candidate.get(\"bm25_score\") or 0) > (merged[key].get(\"bm25_score\") or 0):\n","                merged[key] = candidate\n","\n","        candidates = list(merged.values())\n","\n","        # Optional reranking\n","        if use_reranker and reranker and candidates:\n","            pairs = [(query, c[\"text\"]) for c in candidates]\n","            scores = reranker.predict(pairs)\n","            for c, s in zip(candidates, scores):\n","                c[\"rerank_score\"] = float(s)\n","            candidates = sorted(candidates, key=lambda x: x[\"rerank_score\"], reverse=True)\n","        else:\n","            candidates = sorted(candidates, key=lambda x: (0 if x[\"bm25_score\"] is not None else 1, -(x[\"bm25_score\"] or 0)))\n","\n","        # Return top_k Document objects\n","        final_docs = []\n","        for c in candidates[:top_k]:\n","            text = c[\"text\"]\n","            # Try to find in both mappings\n","            docs_list = text_to_docs_static.get(text, []) if text_to_docs_static else []\n","            if not docs_list and text_to_docs_dynamic:\n","                docs_list = text_to_docs_dynamic.get(text, [])\n","\n","            if docs_list:\n","                final_docs.append(docs_list[0])\n","            else:\n","                final_docs.append(Document(page_content=text, metadata={\"source\": c.get(\"source\", \"Unknown\")}))\n","\n","        return final_docs\n","\n","    except Exception as e:\n","        logger.error(f\"Error in hybrid search: {str(e)}\")\n","        raise\n","\n","def _search_single_index(query: str, vectorstore, bm25, bm25_docs, bm25_sources, text_to_docs, bm_k: int, faiss_k: int):\n","    \"\"\"Search a single index (static or dynamic)\"\"\"\n","    candidates = []\n","\n","    # BM25 candidates\n","    if bm25 and bm25_docs:\n","        tokenized_q = query.split()\n","        bm25_scores = bm25.get_scores(tokenized_q)\n","        bm25_indices = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:bm_k]\n","        for idx in bm25_indices:\n","            txt = bm25_docs[idx]\n","            src = bm25_sources[idx]\n","            candidates.append({\"text\": txt, \"source\": src, \"bm25_score\": bm25_scores[idx]})\n","\n","    # FAISS candidates\n","    if vectorstore:\n","        faiss_retriever = vectorstore.as_retriever(search_kwargs={\"k\": faiss_k})\n","        faiss_docs = faiss_retriever.get_relevant_documents(query)\n","        for d in faiss_docs:\n","            candidates.append({\"text\": d.page_content, \"source\": d.metadata.get(\"source\", \"Unknown\"), \"bm25_score\": None})\n","\n","    return candidates\n","\n","print(\"✅ Helper functions defined!\")\n","\n","# Initialize models\n","def initialize_models():\n","    \"\"\"Initialize all models and components\"\"\"\n","    global embedding_model, static_vectorstore, dynamic_vectorstore, llm\n","    global bm25_static, bm25_dynamic, bm25_docs_static, bm25_docs_dynamic\n","    global bm25_sources_static, bm25_sources_dynamic, reranker\n","    global text_to_docs_static, text_to_docs_dynamic, prompt, text_splitter\n","\n","    try:\n","        print(\"🔄 Loading models...\")\n","        ensure_directories()\n","\n","        # Load embedding model\n","        embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n","        print(\"✅ Embedding model loaded\")\n","\n","        # Initialize text splitter\n","        text_splitter = RecursiveCharacterTextSplitter(\n","            chunk_size=config.CHUNK_SIZE,\n","            chunk_overlap=config.CHUNK_OVERLAP,\n","            length_function=len,\n","            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n","        )\n","        print(\"✅ Text splitter initialized\")\n","\n","        # Load static FAISS vectorstore\n","        try:\n","            static_vectorstore = FAISS.load_local(\n","                config.STATIC_FAISS_PATH,\n","                embedding_model,\n","                allow_dangerous_deserialization=True\n","            )\n","            bm25_static, bm25_docs_static, bm25_sources_static, text_to_docs_static = build_bm25_from_vectorstore(static_vectorstore)\n","            print(\"✅ Static FAISS vectorstore loaded\")\n","        except Exception as e:\n","            print(f\"⚠️ Static FAISS not found: {str(e)}\")\n","\n","        # Load dynamic FAISS vectorstore (if exists)\n","        try:\n","            dynamic_vectorstore = FAISS.load_local(\n","                config.DYNAMIC_FAISS_PATH,\n","                embedding_model,\n","                allow_dangerous_deserialization=True\n","            )\n","            bm25_dynamic, bm25_docs_dynamic, bm25_sources_dynamic, text_to_docs_dynamic = build_bm25_from_vectorstore(dynamic_vectorstore)\n","            print(\"✅ Dynamic FAISS vectorstore loaded\")\n","        except Exception as e:\n","            print(f\"ℹ️ Dynamic FAISS not found (will create on first index): {str(e)}\")\n","\n","        # Initialize LLM\n","        llm = ChatGoogleGenerativeAI(\n","            model=\"gemini-1.5-flash\",\n","            temperature=0,\n","            google_api_key=os.environ[\"GOOGLE_API_KEY\"]\n","        )\n","        print(\"✅ LLM initialized\")\n","\n","        # Create prompt template\n","        prompt_template = \"\"\"\n","        Use the provided context to answer the question.\n","        If the answer is not in the context, say: I couldn't find anything in my knowledge base about that topic. I can only answer questions related to AI, RAG, and the documents you provided.\n","\n","        Context:\n","        {context}\n","\n","        Question:\n","        {question}\n","\n","        Answer:\n","        \"\"\"\n","        prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n","        print(\"✅ Prompt template created\")\n","\n","        # Load reranker\n","        if config.USE_RERANKER:\n","            reranker = CrossEncoder(config.RERANKER_MODEL)\n","            print(\"✅ Reranker loaded\")\n","\n","        print(\"🎉 All models initialized successfully!\")\n","\n","    except Exception as e:\n","        print(f\"❌ Error initializing models: {str(e)}\")\n","        raise\n","\n","# Run the initialization\n","initialize_models()\n","\n","# API Endpoints\n","@app.get(\"/health\", response_model=HealthResponse)\n","async def health_check():\n","    \"\"\"Health check endpoint\"\"\"\n","    components = {\n","        \"static_vectorstore\": static_vectorstore is not None,\n","        \"dynamic_vectorstore\": dynamic_vectorstore is not None,\n","        \"llm\": llm is not None,\n","        \"bm25_static\": bm25_static is not None,\n","        \"bm25_dynamic\": bm25_dynamic is not None,\n","        \"reranker\": reranker is not None if config.USE_RERANKER else \"disabled\"\n","    }\n","    status = \"healthy\" if llm is not None else \"unhealthy\"\n","    return HealthResponse(status=status, components=components)\n","\n","@app.post(\"/api/v1/index\", response_model=IndexResponse)\n","async def index_urls(\n","    request: IndexRequest,\n","    user_info: dict = Depends(verify_api_key)\n","):\n","    \"\"\"Index URLs into the dynamic vector database\"\"\"\n","    global dynamic_vectorstore, bm25_dynamic, bm25_docs_dynamic, bm25_sources_dynamic, text_to_docs_dynamic\n","\n","    # Check permissions\n","    if \"index\" not in user_info.get(\"permissions\", []):\n","        raise HTTPException(\n","            status_code=status.HTTP_403_FORBIDDEN,\n","            detail=\"Insufficient permissions for indexing\"\n","        )\n","\n","    logger.info(f\"Indexing request from {user_info['user']}: {len(request.url)} URLs\")\n","\n","    scraper = WebScraper()\n","    indexed_urls = []\n","    failed_urls = []\n","\n","    # Load existing metadata\n","    urls_metadata_file = os.path.join(config.METADATA_PATH, 'indexed_urls.json')\n","    urls_metadata = load_metadata(urls_metadata_file)\n","\n","    try:\n","        # Create backup before major changes\n","        create_version_backup()\n","\n","        new_documents = []\n","\n","        for url in request.url:\n","            try:\n","                # Check if we should reindex\n","                if not should_reindex_url(url, urls_metadata):\n","                    logger.info(f\"Skipping {url} - recently indexed\")\n","                    continue\n","\n","                logger.info(f\"Processing URL: {url}\")\n","\n","                # Extract content\n","                result = scraper.extract_content(url)\n","\n","                if not result['success']:\n","                    failed_urls.append({\n","                        \"url\": url,\n","                        \"error\": result['error'],\n","                        \"error_type\": \"EXTRACTION_FAILED\"\n","                    })\n","                    continue\n","\n","                # Split content into chunks\n","                chunks = text_splitter.split_text(result['content'])\n","\n","                # Create documents\n","                for i, chunk in enumerate(chunks):\n","                    doc = Document(\n","                        page_content=chunk,\n","                        metadata={\n","                            \"source\": url,\n","                            \"title\": result['title'],\n","                            \"chunk_id\": i,\n","                            \"total_chunks\": len(chunks),\n","                            \"indexed_at\": datetime.now().isoformat(),\n","                            \"url_hash\": get_url_hash(url)\n","                        }\n","                    )\n","                    new_documents.append(doc)\n","\n","                # Update URL metadata\n","                url_hash = get_url_hash(url)\n","                urls_metadata[url_hash] = {\n","                    \"url\": url,\n","                    \"title\": result['title'],\n","                    \"timestamp\": datetime.now().isoformat(),\n","                    \"chunk_count\": len(chunks),\n","                    \"status\": \"indexed\"\n","                }\n","\n","                indexed_urls.append(url)\n","                logger.info(f\"✅ Successfully processed {url} - {len(chunks)} chunks\")\n","\n","            except Exception as e:\n","                logger.error(f\"Error processing {url}: {str(e)}\")\n","                failed_urls.append({\n","                    \"url\": url,\n","                    \"error\": str(e),\n","                    \"error_type\": \"PROCESSING_ERROR\"\n","                })\n","\n","        # Update vector database if we have new documents\n","        if new_documents:\n","            try:\n","                if dynamic_vectorstore is None:\n","                    # Create new FAISS index\n","                    dynamic_vectorstore = FAISS.from_documents(new_documents, embedding_model)\n","                    logger.info(\"✅ Created new dynamic FAISS index\")\n","                else:\n","                    # Add to existing index\n","                    dynamic_vectorstore.add_documents(new_documents)\n","                    logger.info(f\"✅ Added {len(new_documents)} documents to existing index\")\n","\n","                # Save updated index\n","                dynamic_vectorstore.save_local(config.DYNAMIC_FAISS_PATH)\n","\n","                # Rebuild BM25 and mappings\n","                bm25_dynamic, bm25_docs_dynamic, bm25_sources_dynamic, text_to_docs_dynamic = build_bm25_from_vectorstore(dynamic_vectorstore)\n","\n","                # Update version\n","                new_version = update_version()\n","\n","                # Cleanup old backups\n","                cleanup_old_backups()\n","\n","                logger.info(f\"✅ Dynamic index updated to version {new_version}\")\n","\n","            except Exception as e:\n","                logger.error(f\"Error updating vector database: {str(e)}\")\n","                raise HTTPException(\n","                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n","                    detail=f\"Failed to update vector database: {str(e)}\"\n","                )\n","\n","        # Save URL metadata\n","        save_metadata(urls_metadata, urls_metadata_file)\n","\n","        # Prepare response\n","        response_status = \"success\" if indexed_urls else \"failed\"\n","        if indexed_urls and failed_urls:\n","            response_status = \"partial_success\"\n","\n","        metadata = {\n","            \"total_requested\": len(request.url),\n","            \"successfully_indexed\": len(indexed_urls),\n","            \"failed\": len(failed_urls),\n","            \"new_documents_added\": len(new_documents),\n","            \"user\": user_info[\"user\"]\n","        }\n","\n","        return IndexResponse(\n","            status=response_status,\n","            indexed_url=indexed_urls,\n","            failed_url=failed_urls if failed_urls else None,\n","            metadata=metadata\n","        )\n","\n","    except Exception as e:\n","        logger.error(f\"Critical error in indexing: {str(e)}\")\n","        raise HTTPException(\n","            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n","            detail=f\"Indexing failed: {str(e)}\"\n","        )\n","\n","@app.post(\"/query\", response_model=QueryResponse)\n","async def query_rag(\n","    request: QueryRequest,\n","    user_info: dict = Depends(verify_api_key)\n","):\n","    \"\"\"Main RAG query endpoint\"\"\"\n","    try:\n","        logger.info(f\"Query from user {user_info['user']}: {request.question[:50]}...\")\n","\n","        # Perform hybrid search\n","        docs = hybrid_search(\n","            request.question,\n","            use_dynamic=request.use_dynamic_index,\n","            bm_k=config.BM25_TOP_K,\n","            faiss_k=config.FAISS_TOP_K,\n","            top_k=request.top_k,\n","            use_reranker=request.use_reranker and config.USE_RERANKER\n","        )\n","\n","        if not docs:\n","            raise HTTPException(\n","                status_code=status.HTTP_404_NOT_FOUND,\n","                detail=\"No relevant documents found\"\n","            )\n","\n","        # Create context and query LLM\n","        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n","        formatted_prompt = prompt.format(context=context, question=request.question)\n","        response = llm.invoke(formatted_prompt)\n","        answer_text = response.content.strip()\n","\n","        # Check for out-of-scope responses\n","        fallback_msg = \"i couldn't find anything in my knowledge base about that topic\"\n","        if fallback_msg.lower() in answer_text.lower():\n","            sources = []\n","            logger.info(f\"Out-of-scope query: {request.question}\")\n","        else:\n","            sources = list(OrderedDict.fromkeys(doc.metadata.get(\"source\", \"Unknown\") for doc in docs))\n","            sources = format_sources(sources)\n","\n","        metadata = {\n","            \"query_length\": len(request.question),\n","            \"num_docs_retrieved\": len(docs),\n","            \"reranker_used\": request.use_reranker and config.USE_RERANKER,\n","            \"dynamic_index_used\": request.use_dynamic_index,\n","            \"static_index_available\": static_vectorstore is not None,\n","            \"dynamic_index_available\": dynamic_vectorstore is not None,\n","            \"user\": user_info[\"user\"]\n","        }\n","\n","        return QueryResponse(\n","            answer=answer_text,\n","            sources=sources,\n","            metadata=metadata\n","        )\n","\n","    except HTTPException:\n","        raise\n","    except Exception as e:\n","        logger.error(f\"Error processing query: {str(e)}\")\n","        raise HTTPException(\n","            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n","            detail=f\"Internal server error: {str(e)}\"\n","        )\n","\n","@app.get(\"/api/v1/sources\")\n","async def list_sources(user_info: dict = Depends(verify_api_key)):\n","    \"\"\"List available sources in both static and dynamic knowledge bases\"\"\"\n","    static_sources = set()\n","    dynamic_sources = set()\n","\n","    # Get static sources\n","    if static_vectorstore:\n","        for doc in static_vectorstore.docstore._dict.values():\n","            source = doc.metadata.get(\"source\", \"Unknown\")\n","            static_sources.add(source)\n","\n","    # Get dynamic sources\n","    if dynamic_vectorstore:\n","        for doc in dynamic_vectorstore.docstore._dict.values():\n","            source = doc.metadata.get(\"source\", \"Unknown\")\n","            dynamic_sources.add(source)\n","\n","    # Load indexed URLs metadata\n","    urls_metadata_file = os.path.join(config.METADATA_PATH, 'indexed_urls.json')\n","    urls_metadata = load_metadata(urls_metadata_file)\n","\n","    # Load versions metadata\n","    versions_file = os.path.join(config.METADATA_PATH, 'versions.json')\n","    versions = load_metadata(versions_file)\n","\n","    return {\n","        \"static_sources\": {\n","            \"sources\": format_sources(list(static_sources)),\n","            \"total_documents\": len(static_vectorstore.docstore._dict) if static_vectorstore else 0\n","        },\n","        \"dynamic_sources\": {\n","            \"sources\": list(dynamic_sources),\n","            \"total_documents\": len(dynamic_vectorstore.docstore._dict) if dynamic_vectorstore else 0,\n","            \"indexed_urls_count\": len(urls_metadata),\n","            \"current_version\": versions.get(\"current_version\", 0),\n","            \"last_updated\": versions.get(\"last_updated\")\n","        },\n","        \"metadata\": {\n","            \"total_urls_indexed\": len(urls_metadata),\n","            \"total_documents\": (len(static_vectorstore.docstore._dict) if static_vectorstore else 0) +\n","                             (len(dynamic_vectorstore.docstore._dict) if dynamic_vectorstore else 0)\n","        },\n","        \"timestamp\": datetime.now()\n","    }\n","\n","@app.get(\"/api/v1/index/status\")\n","async def get_index_status(user_info: dict = Depends(verify_api_key)):\n","    \"\"\"Get detailed status of the dynamic indexing system\"\"\"\n","\n","    # Load metadata files\n","    urls_metadata_file = os.path.join(config.METADATA_PATH, 'indexed_urls.json')\n","    urls_metadata = load_metadata(urls_metadata_file)\n","\n","    versions_file = os.path.join(config.METADATA_PATH, 'versions.json')\n","    versions = load_metadata(versions_file)\n","\n","    # Calculate statistics\n","    recent_urls = []\n","    old_urls = []\n","    now = datetime.now()\n","\n","    for url_hash, url_data in urls_metadata.items():\n","        indexed_time = datetime.fromisoformat(url_data['timestamp'])\n","        age_days = (now - indexed_time).days\n","\n","        url_info = {\n","            \"url\": url_data['url'],\n","            \"title\": url_data.get('title', ''),\n","            \"indexed_at\": url_data['timestamp'],\n","            \"age_days\": age_days,\n","            \"chunk_count\": url_data.get('chunk_count', 0)\n","        }\n","\n","        if age_days < config.RE_INDEX_DAYS:\n","            recent_urls.append(url_info)\n","        else:\n","            old_urls.append(url_info)\n","\n","    # Get backup information\n","    backup_versions = []\n","    if os.path.exists(config.BACKUP_PATH):\n","        for item in os.listdir(config.BACKUP_PATH):\n","            if item.endswith('_backup'):\n","                version_num = item.replace('_backup', '').replace('v', '')\n","                backup_path = os.path.join(config.BACKUP_PATH, item)\n","                backup_size = sum(os.path.getsize(os.path.join(backup_path, f))\n","                                for f in os.listdir(backup_path) if os.path.isfile(os.path.join(backup_path, f)))\n","                backup_versions.append({\n","                    \"version\": version_num,\n","                    \"size_bytes\": backup_size,\n","                    \"created\": datetime.fromtimestamp(os.path.getctime(backup_path)).isoformat()\n","                })\n","\n","    return {\n","        \"system_status\": {\n","            \"dynamic_index_exists\": dynamic_vectorstore is not None,\n","            \"current_version\": versions.get(\"current_version\", 0),\n","            \"total_updates\": versions.get(\"total_updates\", 0),\n","            \"last_updated\": versions.get(\"last_updated\")\n","        },\n","        \"url_statistics\": {\n","            \"total_indexed\": len(urls_metadata),\n","            \"recent_urls\": len(recent_urls),\n","            \"outdated_urls\": len(old_urls),\n","            \"re_index_threshold_days\": config.RE_INDEX_DAYS\n","        },\n","        \"recent_urls\": recent_urls[:10],  # Show last 10 recent URLs\n","        \"outdated_urls\": old_urls[:5],    # Show first 5 outdated URLs\n","        \"backup_info\": {\n","            \"available_backups\": backup_versions,\n","            \"max_versions_kept\": config.MAX_VERSIONS\n","        },\n","        \"configuration\": {\n","            \"chunk_size\": config.CHUNK_SIZE,\n","            \"chunk_overlap\": config.CHUNK_OVERLAP,\n","            \"max_retries\": config.MAX_RETRIES,\n","            \"request_timeout\": config.REQUEST_TIMEOUT\n","        },\n","        \"timestamp\": datetime.now()\n","    }\n","\n","@app.delete(\"/api/v1/index/{url_hash}\")\n","async def remove_indexed_url(\n","    url_hash: str,\n","    user_info: dict = Depends(verify_api_key)\n","):\n","    \"\"\"Remove a specific indexed URL from the dynamic database\"\"\"\n","    global dynamic_vectorstore, bm25_dynamic, bm25_docs_dynamic, bm25_sources_dynamic, text_to_docs_dynamic\n","\n","    # Check permissions\n","    if \"index\" not in user_info.get(\"permissions\", []):\n","        raise HTTPException(\n","            status_code=status.HTTP_403_FORBIDDEN,\n","            detail=\"Insufficient permissions for index modification\"\n","        )\n","\n","    # Load URL metadata\n","    urls_metadata_file = os.path.join(config.METADATA_PATH, 'indexed_urls.json')\n","    urls_metadata = load_metadata(urls_metadata_file)\n","\n","    if url_hash not in urls_metadata:\n","        raise HTTPException(\n","            status_code=status.HTTP_404_NOT_FOUND,\n","            detail=\"URL hash not found in index\"\n","        )\n","\n","    try:\n","        # Create backup before modification\n","        create_version_backup()\n","\n","        # Find and remove documents with this URL hash\n","        if dynamic_vectorstore:\n","            # This is a complex operation - FAISS doesn't support direct deletion\n","            # We need to rebuild the index without the target documents\n","            remaining_docs = []\n","            removed_count = 0\n","\n","            for doc_id, doc in dynamic_vectorstore.docstore._dict.items():\n","                if doc.metadata.get(\"url_hash\") != url_hash:\n","                    remaining_docs.append(doc)\n","                else:\n","                    removed_count += 1\n","\n","            if remaining_docs:\n","                # Rebuild the vectorstore with remaining documents\n","                dynamic_vectorstore = FAISS.from_documents(remaining_docs, embedding_model)\n","                dynamic_vectorstore.save_local(config.DYNAMIC_FAISS_PATH)\n","\n","                # Rebuild BM25 and mappings\n","                bm25_dynamic, bm25_docs_dynamic, bm25_sources_dynamic, text_to_docs_dynamic = build_bm25_from_vectorstore(dynamic_vectorstore)\n","            else:\n","                # No documents left, remove the vectorstore\n","                dynamic_vectorstore = None\n","                bm25_dynamic = None\n","                bm25_docs_dynamic = None\n","                bm25_sources_dynamic = None\n","                text_to_docs_dynamic = None\n","\n","                # Remove the directory\n","                if os.path.exists(config.DYNAMIC_FAISS_PATH):\n","                    shutil.rmtree(config.DYNAMIC_FAISS_PATH)\n","                    os.makedirs(config.DYNAMIC_FAISS_PATH)\n","\n","        # Remove from URL metadata\n","        url_info = urls_metadata.pop(url_hash)\n","        save_metadata(urls_metadata, urls_metadata_file)\n","\n","        # Update version\n","        new_version = update_version()\n","        cleanup_old_backups()\n","\n","        logger.info(f\"✅ Removed URL {url_info['url']} from index\")\n","\n","        return {\n","            \"status\": \"success\",\n","            \"removed_url\": url_info['url'],\n","            \"documents_removed\": removed_count,\n","            \"new_version\": new_version,\n","            \"timestamp\": datetime.now()\n","        }\n","\n","    except Exception as e:\n","        logger.error(f\"Error removing URL: {str(e)}\")\n","        raise HTTPException(\n","            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n","            detail=f\"Failed to remove URL: {str(e)}\"\n","        )\n","\n","@app.post(\"/api/v1/index/reindex\")\n","async def reindex_all_urls(user_info: dict = Depends(verify_api_key)):\n","    \"\"\"Force reindex all URLs in the metadata\"\"\"\n","    # Check permissions\n","    if \"index\" not in user_info.get(\"permissions\", []):\n","        raise HTTPException(\n","            status_code=status.HTTP_403_FORBIDDEN,\n","            detail=\"Insufficient permissions for reindexing\"\n","        )\n","\n","    # Load URL metadata\n","    urls_metadata_file = os.path.join(config.METADATA_PATH, 'indexed_urls.json')\n","    urls_metadata = load_metadata(urls_metadata_file)\n","\n","    if not urls_metadata:\n","        raise HTTPException(\n","            status_code=status.HTTP_404_NOT_FOUND,\n","            detail=\"No URLs found to reindex\"\n","        )\n","\n","    # Extract URLs and create reindex request\n","    urls_to_reindex = [data['url'] for data in urls_metadata.values()]\n","\n","    # Clear existing metadata to force reindexing\n","    save_metadata({}, urls_metadata_file)\n","\n","    # Create index request\n","    reindex_request = IndexRequest(url=urls_to_reindex)\n","\n","    # Call the index endpoint\n","    return await index_urls(reindex_request, user_info)\n","\n","print(\"✅ API endpoints defined!\")\n","\n","# Start the server\n","def start_server():\n","    \"\"\"Start the FastAPI server with ngrok\"\"\"\n","    # Create ngrok tunnel\n","    public_url = ngrok.connect(8000)\n","\n","    print(f\"🌐 Public URL: {public_url}\")\n","    print(f\"📚 API Documentation: {public_url}/docs\")\n","    print(f\"🔑 API Key: demo-api-key-123\")\n","    print(\"\\n📋 Available Endpoints:\")\n","    print(f\"  • POST {public_url}/api/v1/index - Index new URLs\")\n","    print(f\"  • POST {public_url}/query - Query the RAG system\")\n","    print(f\"  • GET  {public_url}/api/v1/sources - List all sources\")\n","    print(f\"  • GET  {public_url}/api/v1/index/status - Get index status\")\n","    print(f\"  • DELETE {public_url}/api/v1/index/{{url_hash}} - Remove indexed URL\")\n","    print(f\"  • POST {public_url}/api/v1/index/reindex - Reindex all URLs\")\n","    print(f\"  • GET  {public_url}/health - Health check\")\n","    print(\"=\" * 50)\n","\n","    # Example curl commands\n","    print(\"💡 Example Usage:\")\n","    print(f\"\"\"\n","# Index a URL:\n","curl -X POST \"{public_url}/api/v1/index\" \\\\\n","  -H \"Authorization: Bearer demo-api-key-123\" \\\\\n","  -H \"Content-Type: application/json\" \\\\\n","  -d '{{\"url\": [\"https://example.com\"]}}'\n","\n","# Query the system:\n","curl -X POST \"{public_url}/query\" \\\\\n","  -H \"Authorization: Bearer demo-api-key-123\" \\\\\n","  -H \"Content-Type: application/json\" \\\\\n","  -d '{{\"question\": \"What is RAG?\", \"use_dynamic_index\": true}}'\n","\n","# Check index status:\n","curl -X GET \"{public_url}/api/v1/index/status\" \\\\\n","  -H \"Authorization: Bearer demo-api-key-123\"\n","    \"\"\")\n","    print(\"=\" * 50)\n","\n","    # Start the server\n","    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n","\n","# Run the server\n","start_server()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F-KBotpclweD"},"outputs":[],"source":["# ========================================\n","# STEP 1: SETUP AND IMPORTS\n","# ========================================\n","\n","!pip install -q fastapi uvicorn pyngrok nest-asyncio --quiet\n","!pip install -q rank-bm25 sentence-transformers langchain-community langchain-google-genai --quiet\n","!pip install -q faiss-cpu beautifulsoup4 requests pydantic --quiet\n","!pip install -q langchain scikit-learn pandas numpy  --quiet\n"]},{"cell_type":"markdown","metadata":{"id":"A-fmZsWvh9KH"},"source":["# Conversational RAG API with Dynamic Indexing and Sources"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qIZPNVupD2AG"},"outputs":[],"source":["# Setup imports and basic configuration\n","import os\n","import json\n","import time\n","import shutil\n","from datetime import datetime, timedelta\n","from typing import List, Optional, Dict, Any, Union\n","from urllib.parse import urlparse, urljoin\n","import requests\n","from bs4 import BeautifulSoup\n","import nest_asyncio\n","from pyngrok import ngrok\n","import uvicorn\n","from fastapi import FastAPI, HTTPException, Depends, status, Body\n","from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n","from fastapi.middleware.cors import CORSMiddleware\n","from pydantic import BaseModel, Field, validator\n","import logging\n","from getpass import getpass\n","from collections import OrderedDict\n","import hashlib\n","import re\n","import uuid\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","import asyncio\n","from threading import Thread\n","\n","# Import the RAG components\n","from rank_bm25 import BM25Okapi\n","from sentence_transformers import CrossEncoder\n","from langchain_community.embeddings import HuggingFaceEmbeddings\n","from langchain_community.vectorstores import FAISS\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from langchain.prompts import PromptTemplate\n","from langchain.docstore.document import Document\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","# Enable asyncio in Colab\n","nest_asyncio.apply()\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","print(\"✅ All imports successful!\")\n","\n","# ========================================\n","# STEP 2: CONFIGURATION AND API KEYS\n","# ========================================\n","\n","# Set up API keys\n","if \"GOOGLE_API_KEY\" not in os.environ:\n","    os.environ[\"GOOGLE_API_KEY\"] = getpass(\"Enter your Google API key: \")\n","\n","# Set ngrok token\n","ngrok_token = getpass(\"Enter your ngrok token (get free at https://dashboard.ngrok.com/get-started/your-authtoken): \")\n","ngrok.set_auth_token(ngrok_token)\n","\n","# Configuration\n","class Config:\n","    # Static index (existing)\n","    STATIC_FAISS_PATH = \"/content/drive/MyDrive/RAG_demo/faiss_index\"\n","\n","    # Dynamic index (new)\n","    DYNAMIC_BASE_PATH = \"/content/drive/MyDrive/RAG_demo/dynamic_index\"\n","    DYNAMIC_FAISS_PATH = \"/content/drive/MyDrive/RAG_demo/dynamic_index/faiss_index\"\n","    METADATA_PATH = \"/content/drive/MyDrive/RAG_demo/dynamic_index/metadata\"\n","    BACKUP_PATH = \"/content/drive/MyDrive/RAG_demo/dynamic_index/backups\"\n","\n","    # Conversation settings\n","    CONVERSATIONS_PATH = \"/content/drive/MyDrive/RAG_demo/conversations\"\n","    MAX_CONVERSATION_LENGTH = 10  # Maximum number of message pairs\n","    CONVERSATION_TIMEOUT = 1800   # 30 minutes in seconds\n","    MAX_CONTEXT_LENGTH = 3        # Last N message pairs to include in context\n","\n","    # Indexing settings\n","    USE_RERANKER = True\n","    RERANKER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n","    BM25_TOP_K = 10\n","    FAISS_TOP_K = 10\n","    FINAL_TOP_K = 3\n","\n","    # Web scraping settings\n","    REQUEST_TIMEOUT = 30\n","    MAX_RETRIES = 3\n","    RETRY_DELAY = 2\n","    CHUNK_SIZE = 1000\n","    CHUNK_OVERLAP = 100\n","    RE_INDEX_DAYS = 7\n","    MAX_VERSIONS = 3\n","\n","    # Evaluation settings\n","    EVALUATION_DATASET_PATH = \"/content/drive/MyDrive/RAG_demo/evaluation\"\n","\n","    # Authentication\n","    VALID_API_KEYS = {\n","        \"demo-api-key-123\": {\"user\": \"demo_user\", \"permissions\": [\"read\", \"query\", \"index\", \"chat\"]},\n","        \"eval-key-456\": {\"user\": \"evaluator\", \"permissions\": [\"read\", \"query\", \"chat\", \"eval\"]},\n","    }\n","\n","config = Config()\n","print(\"✅ Configuration loaded!\")\n","\n","# ========================================\n","# STEP 3: PYDANTIC MODELS\n","# ========================================\n","\n","# Chat-specific models\n","class ChatMessage(BaseModel):\n","    role: str = Field(..., pattern=\"^(user|assistant)$\")\n","    content: str = Field(..., min_length=1, max_length=2000)\n","    timestamp: Optional[datetime] = Field(default_factory=datetime.now)\n","\n","class ChatRequest(BaseModel):\n","    messages: List[ChatMessage] = Field(..., min_items=1, max_items=20)\n","    session_id: Optional[str] = Field(default=None)\n","    use_dynamic_index: Optional[bool] = Field(default=True)\n","    use_reranker: Optional[bool] = Field(default=True)\n","    top_k: Optional[int] = Field(default=3, ge=1, le=5)\n","\n","class ChatResponse(BaseModel):\n","    session_id: str\n","    response: Dict[str, Any]\n","    conversation_length: int\n","    timestamp: datetime = Field(default_factory=datetime.now)\n","\n","# Original models (updated)\n","class IndexRequest(BaseModel):\n","    url: List[str] = Field(..., min_items=1, max_items=5, description=\"URLs to index\")\n","\n","    @validator('url')\n","    def validate_urls(cls, v):\n","        for url in v:\n","            parsed = urlparse(url)\n","            if not parsed.scheme or not parsed.netloc:\n","                raise ValueError(f\"Invalid URL format: {url}\")\n","        return v\n","\n","class IndexResponse(BaseModel):\n","    status: str\n","    indexed_url: List[str]\n","    failed_url: Optional[List[Dict[str, str]]] = None\n","    metadata: Dict[str, Any] = Field(default_factory=dict)\n","    timestamp: datetime = Field(default_factory=datetime.now)\n","\n","class HealthResponse(BaseModel):\n","    status: str\n","    timestamp: datetime = Field(default_factory=datetime.now)\n","    components: dict\n","    conversations_active: int\n","\n","# NEW: Sources models\n","class SourceInfo(BaseModel):\n","    source_url: str\n","    title: Optional[str] = None\n","    indexed_at: Optional[datetime] = None\n","    document_count: int = 0\n","    source_type: str = Field(..., description=\"static or dynamic\")\n","    last_updated: Optional[datetime] = None\n","\n","class SourcesResponse(BaseModel):\n","    total_sources: int\n","    static_sources: List[SourceInfo]\n","    dynamic_sources: List[SourceInfo]\n","    timestamp: datetime = Field(default_factory=datetime.now)\n","\n","# Evaluation models\n","class EvaluationRequest(BaseModel):\n","    test_cases: List[Dict[str, Any]]\n","    session_id: Optional[str] = None\n","\n","class EvaluationResponse(BaseModel):\n","    overall_score: float\n","    detailed_results: List[Dict[str, Any]]\n","    metrics: Dict[str, float]\n","    timestamp: datetime = Field(default_factory=datetime.now)\n","\n","print(\"✅ Pydantic models defined!\")\n","\n","# ========================================\n","# STEP 4: GLOBAL VARIABLES AND UTILITIES\n","# ========================================\n","\n","# Global variables\n","embedding_model = None\n","static_vectorstore = None\n","dynamic_vectorstore = None\n","llm = None\n","bm25_static = None\n","bm25_dynamic = None\n","bm25_docs_static = None\n","bm25_docs_dynamic = None\n","bm25_sources_static = None\n","bm25_sources_dynamic = None\n","reranker = None\n","text_to_docs_static = None\n","text_to_docs_dynamic = None\n","chat_prompt = None\n","text_splitter = None\n","\n","# In-memory conversation storage\n","active_conversations = {}\n","\n","# Source URL mapping for static content\n","source_url_map = {\n","    \"/content/drive/MyDrive/RAG_demo/data/genai-platform.txt\": \"https://huyenchip.com/2024/07/25/genai-platform.html\",\n","    \"/content/drive/MyDrive/RAG_demo/data/hallucination.txt\": \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n","    \"/content/drive/MyDrive/RAG_demo/data/quora_engineering.txt\": \"https://quoraengineering.quora.com/Building-Embedding-Search-at-Quora\"\n","}\n","\n","# Utility functions\n","def ensure_directories():\n","    \"\"\"Ensure all required directories exist\"\"\"\n","    directories = [\n","        config.DYNAMIC_BASE_PATH,\n","        config.DYNAMIC_FAISS_PATH,\n","        config.METADATA_PATH,\n","        config.BACKUP_PATH,\n","        config.CONVERSATIONS_PATH,\n","        config.EVALUATION_DATASET_PATH\n","    ]\n","    for directory in directories:\n","        os.makedirs(directory, exist_ok=True)\n","    logger.info(\"✅ Directory structure created\")\n","\n","def get_url_hash(url: str) -> str:\n","    \"\"\"Generate a hash for URL to use as unique identifier\"\"\"\n","    return hashlib.md5(url.encode()).hexdigest()\n","\n","def load_metadata(file_path: str) -> Dict:\n","    \"\"\"Load metadata from JSON file\"\"\"\n","    if os.path.exists(file_path):\n","        try:\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                return json.load(f)\n","        except Exception as e:\n","            logger.error(f\"Error loading metadata from {file_path}: {str(e)}\")\n","    return {}\n","\n","def save_metadata(data: Dict, file_path: str):\n","    \"\"\"Save metadata to JSON file\"\"\"\n","    try:\n","        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n","        with open(file_path, 'w', encoding='utf-8') as f:\n","            json.dump(data, f, indent=2, default=str)\n","    except Exception as e:\n","        logger.error(f\"Error saving metadata to {file_path}: {str(e)}\")\n","        raise\n","\n","def format_sources(sources):\n","    \"\"\"Convert file paths to URLs\"\"\"\n","    formatted = []\n","    for source in sources:\n","        if source.startswith('http'):\n","            formatted.append(source)\n","        else:\n","            formatted.append(source_url_map.get(source, source))\n","    return formatted\n","\n","def generate_session_id() -> str:\n","    \"\"\"Generate a unique session ID\"\"\"\n","    return str(uuid.uuid4())\n","\n","def clean_old_conversations():\n","    \"\"\"Remove old conversations from memory\"\"\"\n","    current_time = datetime.now()\n","    expired_sessions = []\n","\n","    for session_id, conv_data in active_conversations.items():\n","        last_active = conv_data.get('last_active', current_time)\n","        if isinstance(last_active, str):\n","            last_active = datetime.fromisoformat(last_active)\n","\n","        if (current_time - last_active).seconds > config.CONVERSATION_TIMEOUT:\n","            expired_sessions.append(session_id)\n","\n","    for session_id in expired_sessions:\n","        del active_conversations[session_id]\n","        logger.info(f\"Cleaned expired conversation: {session_id}\")\n","\n","print(\"✅ Utility functions defined!\")\n","\n","# ========================================\n","# STEP 5: CONVERSATION MANAGEMENT\n","# ========================================\n","\n","class ConversationManager:\n","    def __init__(self):\n","        self.conversations = active_conversations\n","\n","    def get_or_create_session(self, session_id: Optional[str] = None) -> str:\n","        \"\"\"Get existing session or create new one\"\"\"\n","        if session_id and session_id in self.conversations:\n","            # Update last active time\n","            self.conversations[session_id]['last_active'] = datetime.now()\n","            return session_id\n","\n","        # Create new session\n","        new_session_id = generate_session_id()\n","        self.conversations[new_session_id] = {\n","            'messages': [],\n","            'created_at': datetime.now(),\n","            'last_active': datetime.now(),\n","            'metadata': {}\n","        }\n","        logger.info(f\"Created new conversation session: {new_session_id}\")\n","        return new_session_id\n","\n","    def add_message(self, session_id: str, message: ChatMessage):\n","        \"\"\"Add message to conversation\"\"\"\n","        if session_id not in self.conversations:\n","            raise ValueError(f\"Session {session_id} not found\")\n","\n","        self.conversations[session_id]['messages'].append({\n","            'role': message.role,\n","            'content': message.content,\n","            'timestamp': message.timestamp.isoformat() if message.timestamp else datetime.now().isoformat()\n","        })\n","        self.conversations[session_id]['last_active'] = datetime.now()\n","\n","        # Trim conversation if too long\n","        messages = self.conversations[session_id]['messages']\n","        if len(messages) > config.MAX_CONVERSATION_LENGTH * 2:  # *2 for user+assistant pairs\n","            self.conversations[session_id]['messages'] = messages[-config.MAX_CONVERSATION_LENGTH * 2:]\n","\n","    def get_conversation_context(self, session_id: str, max_pairs: int = None) -> str:\n","        \"\"\"Get conversation context for LLM\"\"\"\n","        if session_id not in self.conversations:\n","            return \"\"\n","\n","        messages = self.conversations[session_id]['messages']\n","        if not messages:\n","            return \"\"\n","\n","        # Get last N pairs (user + assistant messages)\n","        max_pairs = max_pairs or config.MAX_CONTEXT_LENGTH\n","        recent_messages = messages[-(max_pairs * 2):]\n","\n","        context_parts = []\n","        for msg in recent_messages:\n","            role = \"Human\" if msg['role'] == 'user' else \"Assistant\"\n","            context_parts.append(f\"{role}: {msg['content']}\")\n","\n","        return \"\\n\".join(context_parts)\n","\n","    def get_conversation_length(self, session_id: str) -> int:\n","        \"\"\"Get number of message exchanges\"\"\"\n","        if session_id not in self.conversations:\n","            return 0\n","        return len(self.conversations[session_id]['messages'])\n","\n","    def save_conversation(self, session_id: str):\n","        \"\"\"Save conversation to disk\"\"\"\n","        if session_id not in self.conversations:\n","            return\n","\n","        conv_file = os.path.join(config.CONVERSATIONS_PATH, f\"{session_id}.json\")\n","        save_metadata(self.conversations[session_id], conv_file)\n","\n","conversation_manager = ConversationManager()\n","print(\"✅ Conversation manager initialized!\")\n","\n","# ========================================\n","# STEP 6: WEB SCRAPING AND INDEXING\n","# ========================================\n","\n","class WebScraper:\n","    def __init__(self):\n","        self.session = requests.Session()\n","        self.session.headers.update({\n","            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n","        })\n","\n","    def extract_content(self, url: str) -> Dict[str, Any]:\n","        \"\"\"Extract content from URL with retry logic\"\"\"\n","        for attempt in range(config.MAX_RETRIES):\n","            try:\n","                response = self.session.get(url, timeout=config.REQUEST_TIMEOUT)\n","                response.raise_for_status()\n","\n","                soup = BeautifulSoup(response.content, 'html.parser')\n","\n","                # Remove unwanted elements\n","                for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'advertisement']):\n","                    element.decompose()\n","\n","                # Extract title\n","                title = soup.find('title')\n","                title_text = title.get_text().strip() if title else url\n","\n","                # Extract main content\n","                content_selectors = [\n","                    'article', 'main', '[role=\"main\"]',\n","                    '.content', '.post', '.article',\n","                    'div.container', 'div.wrapper'\n","                ]\n","\n","                content_text = \"\"\n","                for selector in content_selectors:\n","                    content = soup.select_one(selector)\n","                    if content:\n","                        content_text = content.get_text()\n","                        break\n","\n","                if not content_text:\n","                    # Fallback to body\n","                    body = soup.find('body')\n","                    content_text = body.get_text() if body else \"\"\n","\n","                # Clean text\n","                content_text = re.sub(r'\\s+', ' ', content_text).strip()\n","\n","                if not content_text:\n","                    raise ValueError(\"No content extracted\")\n","\n","                return {\n","                    'title': title_text,\n","                    'content': content_text,\n","                    'url': url,\n","                    'success': True,\n","                    'error': None\n","                }\n","\n","            except Exception as e:\n","                logger.warning(f\"Attempt {attempt + 1} failed for {url}: {str(e)}\")\n","                if attempt < config.MAX_RETRIES - 1:\n","                    time.sleep(config.RETRY_DELAY * (2 ** attempt))\n","                else:\n","                    return {\n","                        'title': None,\n","                        'content': None,\n","                        'url': url,\n","                        'success': False,\n","                        'error': str(e)\n","                    }\n","\n","def build_bm25_from_vectorstore(vectorstore):\n","    \"\"\"Build BM25 index from FAISS vectorstore\"\"\"\n","    try:\n","        docs = []\n","        sources = []\n","        for doc_id, doc in vectorstore.docstore._dict.items():\n","            text = getattr(doc, \"page_content\", None) or doc.page_content\n","            docs.append(text)\n","            sources.append(doc.metadata.get(\"source\", \"Unknown\"))\n","\n","        tokenized = [d.split() for d in docs]\n","        bm25 = BM25Okapi(tokenized)\n","\n","        # Build text to docs mapping\n","        text_to_docs = {}\n","        for doc in vectorstore.docstore._dict.values():\n","            text = doc.page_content\n","            text_to_docs.setdefault(text, []).append(doc)\n","\n","        logger.info(f\"BM25 built over {len(docs)} chunks\")\n","        return bm25, docs, sources, text_to_docs\n","    except Exception as e:\n","        logger.error(f\"Error building BM25: {str(e)}\")\n","        raise\n","\n","print(\"✅ Web scraping and indexing functions defined!\")\n","\n","# ========================================\n","# STEP 7: HYBRID SEARCH SYSTEM\n","# ========================================\n","\n","def hybrid_search(query: str, use_dynamic: bool = True, bm_k: int = 10, faiss_k: int = 10, top_k: int = 3, use_reranker: bool = True, conversation_context: str = \"\"):\n","    \"\"\"Perform hybrid search on static and/or dynamic indices with conversation context\"\"\"\n","    try:\n","        # Enhanced query with context\n","        enhanced_query = query\n","        if conversation_context:\n","            # Add context to help with pronouns and references\n","            enhanced_query = f\"Context: {conversation_context}\\n\\nCurrent question: {query}\"\n","\n","        all_candidates = []\n","\n","        # Search static index\n","        if static_vectorstore:\n","            candidates_static = _search_single_index(\n","                enhanced_query, static_vectorstore, bm25_static, bm25_docs_static,\n","                bm25_sources_static, text_to_docs_static, bm_k, faiss_k\n","            )\n","            all_candidates.extend(candidates_static)\n","\n","        # Search dynamic index\n","        if use_dynamic and dynamic_vectorstore:\n","            candidates_dynamic = _search_single_index(\n","                enhanced_query, dynamic_vectorstore, bm25_dynamic, bm25_docs_dynamic,\n","                bm25_sources_dynamic, text_to_docs_dynamic, bm_k, faiss_k\n","            )\n","            all_candidates.extend(candidates_dynamic)\n","\n","        if not all_candidates:\n","            return []\n","\n","        # Merge and deduplicate candidates\n","        merged = OrderedDict()\n","        for candidate in all_candidates:\n","            key = candidate[\"text\"].strip()[:100]  # Use first 100 chars as key\n","            if key not in merged or (candidate.get(\"bm25_score\") or 0) > (merged[key].get(\"bm25_score\") or 0):\n","                merged[key] = candidate\n","\n","        candidates = list(merged.values())\n","\n","        # Optional reranking - use original query for reranking\n","        if use_reranker and reranker and candidates:\n","            pairs = [(query, c[\"text\"]) for c in candidates]  # Use original query for reranking\n","            scores = reranker.predict(pairs)\n","            for c, s in zip(candidates, scores):\n","                c[\"rerank_score\"] = float(s)\n","            candidates = sorted(candidates, key=lambda x: x[\"rerank_score\"], reverse=True)\n","        else:\n","            candidates = sorted(candidates, key=lambda x: (0 if x[\"bm25_score\"] is not None else 1, -(x[\"bm25_score\"] or 0)))\n","\n","        # Return top_k Document objects\n","        final_docs = []\n","        for c in candidates[:top_k]:\n","            text = c[\"text\"]\n","            # Try to find in both mappings\n","            docs_list = text_to_docs_static.get(text, []) if text_to_docs_static else []\n","            if not docs_list and text_to_docs_dynamic:\n","                docs_list = text_to_docs_dynamic.get(text, [])\n","\n","            if docs_list:\n","                final_docs.append(docs_list[0])\n","            else:\n","                final_docs.append(Document(page_content=text, metadata={\"source\": c.get(\"source\", \"Unknown\")}))\n","\n","        return final_docs\n","\n","    except Exception as e:\n","        logger.error(f\"Error in hybrid search: {str(e)}\")\n","        raise\n","\n","def _search_single_index(query: str, vectorstore, bm25, bm25_docs, bm25_sources, text_to_docs, bm_k: int, faiss_k: int):\n","    \"\"\"Search a single index (static or dynamic)\"\"\"\n","    candidates = []\n","\n","    # BM25 candidates\n","    if bm25 and bm25_docs:\n","        tokenized_q = query.split()\n","        bm25_scores = bm25.get_scores(tokenized_q)\n","        bm25_indices = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:bm_k]\n","        for idx in bm25_indices:\n","            txt = bm25_docs[idx]\n","            src = bm25_sources[idx]\n","            candidates.append({\"text\": txt, \"source\": src, \"bm25_score\": bm25_scores[idx]})\n","\n","    # FAISS candidates\n","    if vectorstore:\n","        faiss_retriever = vectorstore.as_retriever(search_kwargs={\"k\": faiss_k})\n","        faiss_docs = faiss_retriever.get_relevant_documents(query)\n","        for d in faiss_docs:\n","            candidates.append({\"text\": d.page_content, \"source\": d.metadata.get(\"source\", \"Unknown\"), \"bm25_score\": None})\n","\n","    return candidates\n","\n","print(\"✅ Hybrid search system defined!\")\n","\n","# ========================================\n","# STEP 8: EVALUATION SYSTEM\n","# ========================================\n","\n","class RAGEvaluator:\n","    def __init__(self):\n","        self.test_cases = []\n","        self.results = []\n","\n","    def create_test_dataset(self):\n","        \"\"\"Create evaluation test cases\"\"\"\n","        test_cases = [\n","            # Context retention tests\n","            {\n","                \"conversation\": [\n","                    {\"role\": \"user\", \"content\": \"What is attention mechanism in transformers?\"},\n","                    {\"role\": \"user\", \"content\": \"How does it help with long sequences?\"}\n","                ],\n","                \"expected_context\": \"attention mechanism\",\n","                \"test_type\": \"context_retention\"\n","            },\n","\n","            # Citation accuracy tests\n","            {\n","                \"conversation\": [\n","                    {\"role\": \"user\", \"content\": \"Tell me about RAG systems\"}\n","                ],\n","                \"expected_citations\": True,\n","                \"test_type\": \"citation_accuracy\"\n","            },\n","\n","            # Topic switching tests\n","            {\n","                \"conversation\": [\n","                    {\"role\": \"user\", \"content\": \"Explain neural networks\"},\n","                    {\"role\": \"user\", \"content\": \"Now tell me about cooking recipes\"}\n","                ],\n","                \"expected_behavior\": \"topic_switch\",\n","                \"test_type\": \"topic_switching\"\n","            },\n","\n","            # Multi-source tests\n","            {\n","                \"conversation\": [\n","                    {\"role\": \"user\", \"content\": \"What are the latest trends in AI?\"}\n","                ],\n","                \"expected_sources\": \"mixed\",\n","                \"test_type\": \"multi_source\"\n","            }\n","        ]\n","\n","        # Save test cases\n","        test_file = os.path.join(config.EVALUATION_DATASET_PATH, \"test_cases.json\")\n","        save_metadata(test_cases, test_file)\n","        return test_cases\n","\n","    def evaluate_response_relevance(self, question: str, answer: str, context_docs: List[Document]) -> float:\n","        \"\"\"Evaluate how relevant the answer is to the question\"\"\"\n","        try:\n","            # Simple keyword overlap score\n","            question_words = set(question.lower().split())\n","            answer_words = set(answer.lower().split())\n","\n","            overlap = len(question_words.intersection(answer_words))\n","            relevance_score = overlap / len(question_words) if question_words else 0.0\n","\n","            # Bonus for using context\n","            context_text = \" \".join([doc.page_content for doc in context_docs])\n","            context_words = set(context_text.lower().split())\n","            context_usage = len(answer_words.intersection(context_words)) / len(context_words) if context_words else 0.0\n","\n","            final_score = min(1.0, (relevance_score + context_usage) / 2)\n","            return final_score\n","\n","        except Exception as e:\n","            logger.error(f\"Error evaluating relevance: {str(e)}\")\n","            return 0.0\n","\n","    def evaluate_citation_accuracy(self, answer: str, sources: List[str]) -> float:\n","        \"\"\"Evaluate citation accuracy\"\"\"\n","        if not sources:\n","            return 0.0\n","\n","        # Check if answer contains factual claims (simple heuristic)\n","        factual_indicators = [\"according to\", \"research shows\", \"studies indicate\", \"data reveals\", \"analysis found\"]\n","        has_factual_claims = any(indicator in answer.lower() for indicator in factual_indicators)\n","\n","        if has_factual_claims and sources:\n","            return 1.0\n","        elif not has_factual_claims:\n","            return 0.8  # No factual claims, so no citations needed\n","        else:\n","            return 0.0  # Factual claims but no citations\n","\n","    def evaluate_context_retention(self, conversation_history: List[Dict], current_answer: str) -> float:\n","        \"\"\"Evaluate how well context from previous messages is retained\"\"\"\n","        if len(conversation_history) <= 1:\n","            return 1.0  # No previous context to retain\n","\n","        # Look for references to previous topics\n","        previous_content = \" \".join([msg[\"content\"] for msg in conversation_history[:-1]])\n","        previous_words = set(previous_content.lower().split())\n","        answer_words = set(current_answer.lower().split())\n","\n","        # Check for pronouns and references\n","        references = [\"this\", \"that\", \"it\", \"they\", \"these\", \"those\"]\n","        has_references = any(ref in current_answer.lower() for ref in references)\n","\n","        # Calculate context retention score\n","        word_overlap = len(previous_words.intersection(answer_words)) / len(previous_words) if previous_words else 0.0\n","        reference_bonus = 0.3 if has_references else 0.0\n","\n","        context_score = min(1.0, word_overlap + reference_bonus)\n","        return context_score\n","\n","    async def run_evaluation(self, test_cases: List[Dict]) -> Dict[str, Any]:\n","        \"\"\"Run comprehensive evaluation\"\"\"\n","        results = []\n","        total_scores = {\"relevance\": [], \"citation\": [], \"context\": []}\n","\n","        for i, test_case in enumerate(test_cases):\n","            try:\n","                logger.info(f\"Running test case {i+1}/{len(test_cases)}\")\n","\n","                # Simulate conversation\n","                session_id = generate_session_id()\n","                conversation_history = []\n","\n","                for message in test_case[\"conversation\"]:\n","                    # Add user message\n","                    user_msg = ChatMessage(role=\"user\", content=message[\"content\"])\n","                    conversation_manager.add_message(session_id, user_msg)\n","                    conversation_history.append({\"role\": \"user\", \"content\": message[\"content\"]})\n","\n","                    # Get bot response\n","                    context = conversation_manager.get_conversation_context(session_id)\n","                    docs = hybrid_search(\n","                        message[\"content\"],\n","                        use_dynamic=True,\n","                        conversation_context=context\n","                    )\n","\n","                    if docs:\n","                        doc_context = \"\\n\\n\".join([doc.page_content for doc in docs])\n","                        formatted_prompt = chat_prompt.format(\n","                            conversation_context=context,\n","                            context=doc_context,\n","                            question=message[\"content\"]\n","                        )\n","                        response = llm.invoke(formatted_prompt)\n","                        answer = response.content.strip()\n","                        sources = list(set([doc.metadata.get(\"source\", \"Unknown\") for doc in docs]))\n","                        sources = format_sources(sources)\n","                    else:\n","                        answer = \"I couldn't find specific information about that in my knowledge base.\"\n","                        sources = []\n","\n","                    # Add assistant message\n","                    assistant_message = ChatMessage(role=\"assistant\", content=answer)\n","                    conversation_manager.add_message(session_id, assistant_message)\n","                    conversation_history.append({\"role\": \"assistant\", \"content\": answer})\n","\n","                # Evaluate the last response\n","                last_question = test_case[\"conversation\"][-1][\"content\"]\n","                last_answer = conversation_history[-1][\"content\"]\n","\n","                # Calculate scores\n","                relevance_score = self.evaluate_response_relevance(last_question, last_answer, docs if docs else [])\n","                citation_score = self.evaluate_citation_accuracy(last_answer, sources if sources else [])\n","                context_score = self.evaluate_context_retention(conversation_history[:-1], last_answer)\n","\n","                result = {\n","                    \"test_case_id\": i,\n","                    \"test_type\": test_case.get(\"test_type\", \"general\"),\n","                    \"question\": last_question,\n","                    \"answer\": last_answer,\n","                    \"sources\": sources if sources else [],\n","                    \"scores\": {\n","                        \"relevance\": relevance_score,\n","                        \"citation\": citation_score,\n","                        \"context_retention\": context_score\n","                    },\n","                    \"overall_score\": (relevance_score + citation_score + context_score) / 3\n","                }\n","\n","                results.append(result)\n","                total_scores[\"relevance\"].append(relevance_score)\n","                total_scores[\"citation\"].append(citation_score)\n","                total_scores[\"context\"].append(context_score)\n","\n","            except Exception as e:\n","                logger.error(f\"Error in test case {i}: {str(e)}\")\n","                continue\n","\n","        # Calculate overall metrics\n","        metrics = {\n","            \"average_relevance\": np.mean(total_scores[\"relevance\"]) if total_scores[\"relevance\"] else 0.0,\n","            \"average_citation\": np.mean(total_scores[\"citation\"]) if total_scores[\"citation\"] else 0.0,\n","            \"average_context\": np.mean(total_scores[\"context\"]) if total_scores[\"context\"] else 0.0,\n","            \"overall_average\": np.mean([np.mean(scores) for scores in total_scores.values()]) if any(total_scores.values()) else 0.0,\n","            \"test_cases_completed\": len(results),\n","            \"test_cases_failed\": len(test_cases) - len(results)\n","        }\n","\n","        return {\n","            \"overall_score\": metrics[\"overall_average\"],\n","            \"detailed_results\": results,\n","            \"metrics\": metrics\n","        }\n","\n","evaluator = RAGEvaluator()\n","print(\"✅ Evaluation system initialized!\")\n","\n","# ========================================\n","# STEP 9: MODEL INITIALIZATION\n","# ========================================\n","\n","def initialize_models():\n","    \"\"\"Initialize all models and components\"\"\"\n","    global embedding_model, static_vectorstore, dynamic_vectorstore, llm\n","    global bm25_static, bm25_dynamic, bm25_docs_static, bm25_docs_dynamic\n","    global bm25_sources_static, bm25_sources_dynamic, reranker\n","    global text_to_docs_static, text_to_docs_dynamic, chat_prompt, text_splitter\n","\n","    try:\n","        print(\"🔄 Loading models...\")\n","        ensure_directories()\n","        clean_old_conversations()\n","\n","        # Load embedding model\n","        embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n","        print(\"✅ Embedding model loaded\")\n","\n","        # Initialize text splitter\n","        text_splitter = RecursiveCharacterTextSplitter(\n","            chunk_size=config.CHUNK_SIZE,\n","            chunk_overlap=config.CHUNK_OVERLAP,\n","            length_function=len,\n","            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n","        )\n","        print(\"✅ Text splitter initialized\")\n","\n","        # Load static FAISS vectorstore\n","        try:\n","            static_vectorstore = FAISS.load_local(\n","                config.STATIC_FAISS_PATH,\n","                embedding_model,\n","                allow_dangerous_deserialization=True\n","            )\n","            bm25_static, bm25_docs_static, bm25_sources_static, text_to_docs_static = build_bm25_from_vectorstore(static_vectorstore)\n","            print(\"✅ Static FAISS vectorstore loaded\")\n","        except Exception as e:\n","            print(f\"⚠️ Static FAISS not found: {str(e)}\")\n","\n","        # Load dynamic FAISS vectorstore (if exists)\n","        try:\n","            dynamic_vectorstore = FAISS.load_local(\n","                config.DYNAMIC_FAISS_PATH,\n","                embedding_model,\n","                allow_dangerous_deserialization=True\n","            )\n","            bm25_dynamic, bm25_docs_dynamic, bm25_sources_dynamic, text_to_docs_dynamic = build_bm25_from_vectorstore(dynamic_vectorstore)\n","            print(\"✅ Dynamic FAISS vectorstore loaded\")\n","        except Exception as e:\n","            print(f\"ℹ️ Dynamic FAISS not found (will create on first index): {str(e)}\")\n","\n","        # Initialize LLM\n","        llm = ChatGoogleGenerativeAI(\n","            model=\"gemini-1.5-flash\",\n","            temperature=0,\n","            google_api_key=os.environ[\"GOOGLE_API_KEY\"]\n","        )\n","        print(\"✅ LLM initialized\")\n","\n","        # Create chat prompt template\n","        chat_prompt_template = \"\"\"\n","You are a helpful AI assistant with access to multiple knowledge sources. You can maintain context across conversations and provide accurate citations.\n","\n","Previous conversation context:\n","{conversation_context}\n","\n","Current context from knowledge base:\n","{context}\n","\n","Current question:\n","{question}\n","\n","Instructions:\n","1. Use the conversation context to understand references like \"it\", \"this\", \"that\", etc.\n","2. Provide accurate answers based on the knowledge base context\n","3. If you reference specific information, it should be from the provided context\n","4. If the answer is not in the knowledge base, say: \"I couldn't find specific information about that in my knowledge base.\"\n","5. Be conversational and natural in your responses\n","6. Handle follow-up questions by connecting them to previous context when appropriate\n","\n","Answer:\n","        \"\"\"\n","        chat_prompt = PromptTemplate(\n","            input_variables=[\"conversation_context\", \"context\", \"question\"],\n","            template=chat_prompt_template\n","        )\n","        print(\"✅ Chat prompt template created\")\n","\n","        # Load reranker\n","        if config.USE_RERANKER:\n","            reranker = CrossEncoder(config.RERANKER_MODEL)\n","            print(\"✅ Reranker loaded\")\n","\n","        print(\"🎉 All models initialized successfully!\")\n","\n","    except Exception as e:\n","        print(f\"❌ Error initializing models: {str(e)}\")\n","        raise\n","\n","# ========================================\n","# STEP 10: FASTAPI APPLICATION\n","# ========================================\n","\n","app = FastAPI(\n","    title=\"Conversational RAG API with Dynamic Indexing and Sources\",\n","    description=\"Hybrid Retrieval-Augmented Generation API with Conversation Support, BM25 + FAISS, Dynamic Web Indexing, and Source Management\",\n","    version=\"3.1.0\"\n",")\n","\n","app.add_middleware(\n","    CORSMiddleware,\n","    allow_origins=[\"*\"],\n","    allow_credentials=True,\n","    allow_methods=[\"*\"],\n","    allow_headers=[\"*\"],\n",")\n","\n","security = HTTPBearer()\n","\n","# Authentication\n","async def verify_api_key(credentials: HTTPAuthorizationCredentials = Depends(security)):\n","    \"\"\"Verify API key from Authorization header\"\"\"\n","    api_key = credentials.credentials\n","    if api_key not in config.VALID_API_KEYS:\n","        logger.warning(f\"Invalid API key attempted: {api_key[:10]}...\")\n","        raise HTTPException(\n","            status_code=status.HTTP_401_UNAUTHORIZED,\n","            detail=\"Invalid API key\",\n","            headers={\"WWW-Authenticate\": \"Bearer\"},\n","        )\n","    return config.VALID_API_KEYS[api_key]\n","\n","# ========================================\n","# STEP 11: API ENDPOINTS\n","# ========================================\n","\n","@app.get(\"/health\", response_model=HealthResponse)\n","async def health_check():\n","    \"\"\"Health check endpoint\"\"\"\n","    clean_old_conversations()  # Cleanup old conversations on health check\n","\n","    components = {\n","        \"static_vectorstore\": static_vectorstore is not None,\n","        \"dynamic_vectorstore\": dynamic_vectorstore is not None,\n","        \"llm\": llm is not None,\n","        \"bm25_static\": bm25_static is not None,\n","        \"bm25_dynamic\": bm25_dynamic is not None,\n","        \"reranker\": reranker is not None if config.USE_RERANKER else \"disabled\",\n","        \"conversation_manager\": True\n","    }\n","    status = \"healthy\" if llm is not None else \"unhealthy\"\n","    return HealthResponse(\n","        status=status,\n","        components=components,\n","        conversations_active=len(active_conversations)\n","    )\n","\n","@app.post(\"/api/v1/chat\", response_model=ChatResponse)\n","async def chat_with_rag(\n","    request: ChatRequest,\n","    user_info: dict = Depends(verify_api_key)\n","):\n","    \"\"\"Main conversational RAG endpoint\"\"\"\n","    try:\n","        # Check permissions\n","        if \"chat\" not in user_info.get(\"permissions\", []):\n","            raise HTTPException(\n","                status_code=status.HTTP_403_FORBIDDEN,\n","                detail=\"Insufficient permissions for chat\"\n","            )\n","\n","        logger.info(f\"Chat request from user {user_info['user']}\")\n","\n","        # Get or create session\n","        session_id = conversation_manager.get_or_create_session(request.session_id)\n","\n","        # Add conversation history to session (except the last message which is the current question)\n","        for message in request.messages[:-1]:\n","            conversation_manager.add_message(session_id, message)\n","\n","        # Get current question\n","        current_message = request.messages[-1]\n","        if current_message.role != \"user\":\n","            raise HTTPException(\n","                status_code=status.HTTP_400_BAD_REQUEST,\n","                detail=\"Last message must be from user\"\n","            )\n","\n","        # Add current question to conversation\n","        conversation_manager.add_message(session_id, current_message)\n","\n","        # Get conversation context for retrieval and generation\n","        conversation_context = conversation_manager.get_conversation_context(session_id)\n","\n","        # Perform hybrid search with conversation context\n","        docs = hybrid_search(\n","            current_message.content,\n","            use_dynamic=request.use_dynamic_index,\n","            bm_k=config.BM25_TOP_K,\n","            faiss_k=config.FAISS_TOP_K,\n","            top_k=request.top_k,\n","            use_reranker=request.use_reranker and config.USE_RERANKER,\n","            conversation_context=conversation_context\n","        )\n","\n","        if not docs:\n","            answer = \"I couldn't find specific information about that in my knowledge base.\"\n","            sources = []\n","        else:\n","            # Create context and query LLM\n","            doc_context = \"\\n\\n\".join([doc.page_content for doc in docs])\n","            formatted_prompt = chat_prompt.format(\n","                conversation_context=conversation_context,\n","                context=doc_context,\n","                question=current_message.content\n","            )\n","\n","            response = llm.invoke(formatted_prompt)\n","            answer = response.content.strip()\n","\n","            # Get unique sources\n","            sources = list(OrderedDict.fromkeys(doc.metadata.get(\"source\", \"Unknown\") for doc in docs))\n","            sources = format_sources(sources)\n","\n","        # Add assistant response to conversation\n","        assistant_message = ChatMessage(role=\"assistant\", content=answer)\n","        conversation_manager.add_message(session_id, assistant_message)\n","\n","        # Save conversation periodically\n","        if conversation_manager.get_conversation_length(session_id) % 4 == 0:  # Every 4 messages\n","            conversation_manager.save_conversation(session_id)\n","\n","        # UPDATED RESPONSE STRUCTURE WITH ENHANCED SOURCES\n","        response_data = {\n","            \"answer\": {\n","                \"content\": answer,\n","                \"role\": \"assistant\"\n","            },\n","            \"sources\": sources,  # Make sources more prominent\n","            \"citations\": sources,  # Keep backward compatibility\n","            \"retrieved_documents\": [\n","                {\n","                    \"content\": doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content,\n","                    \"source\": doc.metadata.get(\"source\", \"Unknown\"),\n","                    \"title\": doc.metadata.get(\"title\", \"\"),\n","                    \"chunk_id\": doc.metadata.get(\"chunk_id\", 0)\n","                } for doc in docs\n","            ] if docs else [],\n","            \"metadata\": {\n","                \"num_docs_retrieved\": len(docs),\n","                \"num_sources\": len(sources),\n","                \"reranker_used\": request.use_reranker and config.USE_RERANKER,\n","                \"dynamic_index_used\": request.use_dynamic_index,\n","                \"conversation_length\": conversation_manager.get_conversation_length(session_id),\n","                \"user\": user_info[\"user\"]\n","            }\n","        }\n","\n","        return ChatResponse(\n","            session_id=session_id,\n","            response=response_data,\n","            conversation_length=conversation_manager.get_conversation_length(session_id)\n","        )\n","\n","    except HTTPException:\n","        raise\n","    except Exception as e:\n","        logger.error(f\"Error processing chat: {str(e)}\")\n","        raise HTTPException(\n","            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n","            detail=f\"Internal server error: {str(e)}\"\n","        )\n","\n","# NEW: Sources management endpoints\n","@app.get(\"/api/v1/sources\", response_model=SourcesResponse)\n","async def get_sources(user_info: dict = Depends(verify_api_key)):\n","    \"\"\"Get all available sources in the system\"\"\"\n","    try:\n","        # Check permissions\n","        if \"read\" not in user_info.get(\"permissions\", []):\n","            raise HTTPException(\n","                status_code=status.HTTP_403_FORBIDDEN,\n","                detail=\"Insufficient permissions to view sources\"\n","            )\n","\n","        static_sources = []\n","        dynamic_sources = []\n","\n","        # Get static sources\n","        if static_vectorstore:\n","            static_source_map = {}\n","            for doc_id, doc in static_vectorstore.docstore._dict.items():\n","                source = doc.metadata.get(\"source\", \"Unknown\")\n","                title = doc.metadata.get(\"title\", \"\")\n","\n","                if source not in static_source_map:\n","                    static_source_map[source] = {\n","                        \"count\": 0,\n","                        \"title\": title\n","                    }\n","                static_source_map[source][\"count\"] += 1\n","\n","            for source, info in static_source_map.items():\n","                # Convert file paths to URLs using source_url_map\n","                display_source = source_url_map.get(source, source)\n","                static_sources.append(SourceInfo(\n","                    source_url=display_source,\n","                    title=info[\"title\"],\n","                    document_count=info[\"count\"],\n","                    source_type=\"static\",\n","                    indexed_at=None,  # Static sources don't have index timestamp\n","                    last_updated=None\n","                ))\n","\n","        # Get dynamic sources\n","        if dynamic_vectorstore:\n","            dynamic_source_map = {}\n","            for doc_id, doc in dynamic_vectorstore.docstore._dict.items():\n","                source = doc.metadata.get(\"source\", \"Unknown\")\n","                title = doc.metadata.get(\"title\", \"\")\n","                indexed_at = doc.metadata.get(\"indexed_at\")\n","\n","                if source not in dynamic_source_map:\n","                    dynamic_source_map[source] = {\n","                        \"count\": 0,\n","                        \"title\": title,\n","                        \"indexed_at\": indexed_at\n","                    }\n","                dynamic_source_map[source][\"count\"] += 1\n","\n","                # Keep the most recent indexed_at\n","                if indexed_at and (not dynamic_source_map[source][\"indexed_at\"] or indexed_at > dynamic_source_map[source][\"indexed_at\"]):\n","                    dynamic_source_map[source][\"indexed_at\"] = indexed_at\n","\n","            for source, info in dynamic_source_map.items():\n","                indexed_datetime = None\n","                if info[\"indexed_at\"]:\n","                    try:\n","                        indexed_datetime = datetime.fromisoformat(info[\"indexed_at\"]) if isinstance(info[\"indexed_at\"], str) else info[\"indexed_at\"]\n","                    except:\n","                        indexed_datetime = None\n","\n","                dynamic_sources.append(SourceInfo(\n","                    source_url=source,\n","                    title=info[\"title\"],\n","                    document_count=info[\"count\"],\n","                    source_type=\"dynamic\",\n","                    indexed_at=indexed_datetime,\n","                    last_updated=indexed_datetime\n","                ))\n","\n","        # Sort sources by document count (descending)\n","        static_sources.sort(key=lambda x: x.document_count, reverse=True)\n","        dynamic_sources.sort(key=lambda x: x.document_count, reverse=True)\n","\n","        return SourcesResponse(\n","            total_sources=len(static_sources) + len(dynamic_sources),\n","            static_sources=static_sources,\n","            dynamic_sources=dynamic_sources\n","        )\n","\n","    except Exception as e:\n","        logger.error(f\"Error retrieving sources: {str(e)}\")\n","        raise HTTPException(\n","            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n","            detail=f\"Failed to retrieve sources: {str(e)}\"\n","        )\n","\n","@app.get(\"/api/v1/sources/{source_hash}\")\n","async def get_source_details(\n","    source_hash: str,\n","    user_info: dict = Depends(verify_api_key)\n","):\n","    \"\"\"Get detailed information about a specific source\"\"\"\n","    # Check permissions\n","    if \"read\" not in user_info.get(\"permissions\", []):\n","        raise HTTPException(\n","            status_code=status.HTTP_403_FORBIDDEN,\n","            detail=\"Insufficient permissions to view source details\"\n","        )\n","\n","    # Find source by hash or URL\n","    source_found = False\n","    source_details = {\n","        \"source_url\": None,\n","        \"title\": None,\n","        \"chunks\": [],\n","        \"source_type\": None,\n","        \"indexed_at\": None\n","    }\n","\n","    # Search in both vectorstores\n","    for vectorstore, source_type in [(static_vectorstore, \"static\"), (dynamic_vectorstore, \"dynamic\")]:\n","        if vectorstore:\n","            for doc_id, doc in vectorstore.docstore._dict.items():\n","                source = doc.metadata.get(\"source\", \"\")\n","                url_hash = get_url_hash(source)\n","\n","                if url_hash == source_hash or source.endswith(source_hash):\n","                    source_found = True\n","                    source_details.update({\n","                        \"source_url\": source_url_map.get(source, source),\n","                        \"title\": doc.metadata.get(\"title\", \"\"),\n","                        \"source_type\": source_type,\n","                        \"indexed_at\": doc.metadata.get(\"indexed_at\")\n","                    })\n","\n","                    source_details[\"chunks\"].append({\n","                        \"chunk_id\": doc.metadata.get(\"chunk_id\", 0),\n","                        \"content_preview\": doc.page_content[:300] + \"...\" if len(doc.page_content) > 300 else doc.page_content,\n","                        \"content_length\": len(doc.page_content)\n","                    })\n","\n","    if not source_found:\n","        raise HTTPException(\n","            status_code=status.HTTP_404_NOT_FOUND,\n","            detail=\"Source not found\"\n","        )\n","\n","    return source_details\n","\n","@app.post(\"/api/v1/index\", response_model=IndexResponse)\n","async def index_urls(\n","    request: IndexRequest,\n","    user_info: dict = Depends(verify_api_key)\n","):\n","    \"\"\"Index URLs into the dynamic vector database\"\"\"\n","    global dynamic_vectorstore, bm25_dynamic, bm25_docs_dynamic, bm25_sources_dynamic, text_to_docs_dynamic\n","\n","    # Check permissions\n","    if \"index\" not in user_info.get(\"permissions\", []):\n","        raise HTTPException(\n","            status_code=status.HTTP_403_FORBIDDEN,\n","            detail=\"Insufficient permissions for indexing\"\n","        )\n","\n","    logger.info(f\"Indexing request from {user_info['user']}: {len(request.url)} URLs\")\n","\n","    scraper = WebScraper()\n","    indexed_urls = []\n","    failed_urls = []\n","\n","    try:\n","        new_documents = []\n","\n","        for url in request.url:\n","            try:\n","                logger.info(f\"Processing URL: {url}\")\n","\n","                # Extract content\n","                result = scraper.extract_content(url)\n","\n","                if not result['success']:\n","                    failed_urls.append({\n","                        \"url\": url,\n","                        \"error\": result['error'],\n","                        \"error_type\": \"EXTRACTION_FAILED\"\n","                    })\n","                    continue\n","\n","                # Split content into chunks\n","                chunks = text_splitter.split_text(result['content'])\n","\n","                # Create documents\n","                for i, chunk in enumerate(chunks):\n","                    doc = Document(\n","                        page_content=chunk,\n","                        metadata={\n","                            \"source\": url,\n","                            \"title\": result['title'],\n","                            \"chunk_id\": i,\n","                            \"total_chunks\": len(chunks),\n","                            \"indexed_at\": datetime.now().isoformat(),\n","                            \"url_hash\": get_url_hash(url)\n","                        }\n","                    )\n","                    new_documents.append(doc)\n","\n","                indexed_urls.append(url)\n","                logger.info(f\"✅ Successfully processed {url} - {len(chunks)} chunks\")\n","\n","            except Exception as e:\n","                logger.error(f\"Error processing {url}: {str(e)}\")\n","                failed_urls.append({\n","                    \"url\": url,\n","                    \"error\": str(e),\n","                    \"error_type\": \"PROCESSING_ERROR\"\n","                })\n","\n","        # Update vector database if we have new documents\n","        if new_documents:\n","            try:\n","                if dynamic_vectorstore is None:\n","                    # Create new FAISS index\n","                    dynamic_vectorstore = FAISS.from_documents(new_documents, embedding_model)\n","                    logger.info(\"✅ Created new dynamic FAISS index\")\n","                else:\n","                    # Add to existing index\n","                    dynamic_vectorstore.add_documents(new_documents)\n","                    logger.info(f\"✅ Added {len(new_documents)} documents to existing index\")\n","\n","                # Save updated index\n","                dynamic_vectorstore.save_local(config.DYNAMIC_FAISS_PATH)\n","\n","                # Rebuild BM25 and mappings\n","                bm25_dynamic, bm25_docs_dynamic, bm25_sources_dynamic, text_to_docs_dynamic = build_bm25_from_vectorstore(dynamic_vectorstore)\n","\n","                logger.info(f\"✅ Dynamic index updated\")\n","\n","            except Exception as e:\n","                logger.error(f\"Error updating vector database: {str(e)}\")\n","                raise HTTPException(\n","                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n","                    detail=f\"Failed to update vector database: {str(e)}\"\n","                )\n","\n","        # Prepare response\n","        response_status = \"success\" if indexed_urls else \"failed\"\n","        if indexed_urls and failed_urls:\n","            response_status = \"partial_success\"\n","\n","        metadata = {\n","            \"total_requested\": len(request.url),\n","            \"successfully_indexed\": len(indexed_urls),\n","            \"failed\": len(failed_urls),\n","            \"new_documents_added\": len(new_documents),\n","            \"user\": user_info[\"user\"]\n","        }\n","\n","        return IndexResponse(\n","            status=response_status,\n","            indexed_url=indexed_urls,\n","            failed_url=failed_urls if failed_urls else None,\n","            metadata=metadata\n","        )\n","\n","    except Exception as e:\n","        logger.error(f\"Critical error in indexing: {str(e)}\")\n","        raise HTTPException(\n","            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n","            detail=f\"Indexing failed: {str(e)}\"\n","        )\n","\n","@app.post(\"/api/v1/evaluate\", response_model=EvaluationResponse)\n","async def evaluate_system(\n","    request: EvaluationRequest,\n","    user_info: dict = Depends(verify_api_key)\n","):\n","    \"\"\"Run automated evaluation of the RAG system\"\"\"\n","    # Check permissions\n","    if \"eval\" not in user_info.get(\"permissions\", []):\n","        raise HTTPException(\n","            status_code=status.HTTP_403_FORBIDDEN,\n","            detail=\"Insufficient permissions for evaluation\"\n","        )\n","\n","    logger.info(f\"Evaluation request from user {user_info['user']}\")\n","\n","    try:\n","        # Use provided test cases or create default ones\n","        test_cases = request.test_cases if request.test_cases else evaluator.create_test_dataset()\n","\n","        # Run evaluation\n","        evaluation_results = await evaluator.run_evaluation(test_cases)\n","\n","        return EvaluationResponse(\n","            overall_score=evaluation_results[\"overall_score\"],\n","            detailed_results=evaluation_results[\"detailed_results\"],\n","            metrics=evaluation_results[\"metrics\"]\n","        )\n","\n","    except Exception as e:\n","        logger.error(f\"Error running evaluation: {str(e)}\")\n","        raise HTTPException(\n","            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n","            detail=f\"Evaluation failed: {str(e)}\"\n","        )\n","\n","@app.get(\"/api/v1/conversations/{session_id}\")\n","async def get_conversation(\n","    session_id: str,\n","    user_info: dict = Depends(verify_api_key)\n","):\n","    \"\"\"Get conversation history\"\"\"\n","    if session_id not in active_conversations:\n","        raise HTTPException(\n","            status_code=status.HTTP_404_NOT_FOUND,\n","            detail=\"Conversation not found\"\n","        )\n","\n","    conversation_data = active_conversations[session_id]\n","    return {\n","        \"session_id\": session_id,\n","        \"messages\": conversation_data[\"messages\"],\n","        \"created_at\": conversation_data[\"created_at\"],\n","        \"last_active\": conversation_data[\"last_active\"],\n","        \"message_count\": len(conversation_data[\"messages\"])\n","    }\n","\n","@app.delete(\"/api/v1/conversations/{session_id}\")\n","async def delete_conversation(\n","    session_id: str,\n","    user_info: dict = Depends(verify_api_key)\n","):\n","    \"\"\"Delete a conversation\"\"\"\n","    if session_id not in active_conversations:\n","        raise HTTPException(\n","            status_code=status.HTTP_404_NOT_FOUND,\n","            detail=\"Conversation not found\"\n","        )\n","\n","    del active_conversations[session_id]\n","    return {\"status\": \"deleted\", \"session_id\": session_id}\n","\n","@app.get(\"/api/v1/conversations\")\n","async def list_conversations(user_info: dict = Depends(verify_api_key)):\n","    \"\"\"List active conversations\"\"\"\n","    clean_old_conversations()\n","\n","    conversations_summary = []\n","    for session_id, conv_data in active_conversations.items():\n","        conversations_summary.append({\n","            \"session_id\": session_id,\n","            \"message_count\": len(conv_data[\"messages\"]),\n","            \"created_at\": conv_data[\"created_at\"],\n","            \"last_active\": conv_data[\"last_active\"],\n","            \"last_message_preview\": conv_data[\"messages\"][-1][\"content\"][:100] if conv_data[\"messages\"] else \"\"\n","        })\n","\n","    return {\n","        \"active_conversations\": len(conversations_summary),\n","        \"conversations\": conversations_summary\n","    }\n","\n","print(\"✅ API endpoints defined!\")\n","\n","# ========================================\n","# STEP 12: DEMO AND TESTING FUNCTIONS\n","# ========================================\n","\n","def create_demo_test_cases():\n","    \"\"\"Create demonstration test cases for your project\"\"\"\n","    demo_cases = [\n","        {\n","            \"name\": \"Context Retention Test\",\n","            \"conversation\": [\n","                {\"role\": \"user\", \"content\": \"What is a transformer in machine learning?\"},\n","                {\"role\": \"user\", \"content\": \"How does its attention mechanism work?\"},\n","                {\"role\": \"user\", \"content\": \"What are the advantages of this approach?\"}\n","            ],\n","            \"expected_behavior\": \"Should understand that 'its' refers to transformer and 'this approach' refers to attention mechanism\"\n","        },\n","        {\n","            \"name\": \"Multi-Source Retrieval Test\",\n","            \"conversation\": [\n","                {\"role\": \"user\", \"content\": \"Tell me about recent developments in AI and their impact on software engineering\"}\n","            ],\n","            \"expected_behavior\": \"Should pull from both static and dynamic sources, provide citations\"\n","        },\n","        {\n","            \"name\": \"Topic Switching Test\",\n","            \"conversation\": [\n","                {\"role\": \"user\", \"content\": \"Explain neural networks\"},\n","                {\"role\": \"user\", \"content\": \"Actually, let's talk about database indexing instead\"}\n","            ],\n","            \"expected_behavior\": \"Should cleanly switch topics without carrying over irrelevant context\"\n","        },\n","        {\n","            \"name\": \"Citation Accuracy Test\",\n","            \"conversation\": [\n","                {\"role\": \"user\", \"content\": \"What does research say about hallucinations in language models?\"}\n","            ],\n","            \"expected_behavior\": \"Should provide specific citations for research claims\"\n","        }\n","    ]\n","\n","    # Save demo test cases\n","    demo_file = os.path.join(config.EVALUATION_DATASET_PATH, \"demo_test_cases.json\")\n","    save_metadata(demo_cases, demo_file)\n","\n","    print(\"✅ Demo test cases created!\")\n","    print(\"📁 Location:\", demo_file)\n","    return demo_cases\n","\n","# ========================================\n","# STEP 13: SERVER STARTUP\n","# ========================================\n","\n","def start_server():\n","    \"\"\"Start the FastAPI server with ngrok\"\"\"\n","    # Create ngrok tunnel\n","    public_url = ngrok.connect(8000)\n","\n","    print(\"=\" * 60)\n","    print(\"🚀 CONVERSATIONAL RAG SYSTEM WITH SOURCES API LAUNCHED!\")\n","    print(\"=\" * 60)\n","    print(f\"🌐 Public URL: {public_url}\")\n","    print(f\"📚 API Documentation: {public_url}/docs\")\n","    print(f\"🔑 API Keys:\")\n","    print(f\"   • demo-api-key-123 (full access)\")\n","    print(f\"   • eval-key-456 (evaluation access)\")\n","    print(\"\")\n","    print(\"🎯 MAIN ENDPOINTS:\")\n","    print(f\"  • POST {public_url}/api/v1/chat - Chat with the system\")\n","    print(f\"  • POST {public_url}/api/v1/index - Index new URLs\")\n","    print(f\"  • GET  {public_url}/api/v1/sources - List all sources\")\n","    print(f\"  • GET  {public_url}/api/v1/sources/{{hash}} - Get source details\")\n","    print(f\"  • POST {public_url}/api/v1/evaluate - Run automated evaluation\")\n","    print(f\"  • GET  {public_url}/api/v1/conversations - List conversations\")\n","    print(f\"  • GET  {public_url}/health - Health check\")\n","    print(\"\")\n","    print(\"💡 EXAMPLE USAGE:\")\n","    print(f\"\"\"\n","# Start a conversation:\n","curl -X POST \"{public_url}/api/v1/chat\" \\\\\n","  -H \"Authorization: Bearer demo-api-key-123\" \\\\\n","  -H \"Content-Type: application/json\" \\\\\n","  -d '{{\n","    \"messages\": [\n","      {{\"role\": \"user\", \"content\": \"What is attention mechanism?\"}}\n","    ]\n","  }}'\n","\n","# Get all sources:\n","curl -X GET \"{public_url}/api/v1/sources\" \\\\\n","  -H \"Authorization: Bearer demo-api-key-123\"\n","\n","# Index new content:\n","curl -X POST \"{public_url}/api/v1/index\" \\\\\n","  -H \"Authorization: Bearer demo-api-key-123\" \\\\\n","  -H \"Content-Type: application/json\" \\\\\n","  -d '{{\"url\": [\"https://example.com/ai-article\"]}}'\n","\n","# Run evaluation:\n","curl -X POST \"{public_url}/api/v1/evaluate\" \\\\\n","  -H \"Authorization: Bearer eval-key-456\" \\\\\n","  -H \"Content-Type: application/json\" \\\\\n","  -d '{{\"test_cases\": []}}'\n","    \"\"\")\n","    print(\"=\" * 60)\n","    print(\"🔥 Your conversational RAG system with sources API is ready!\")\n","    print(\"✨ New Features: Complete source management and tracking\")\n","    print(\"🎓 Perfect for your final data science project!\")\n","    print(\"=\" * 60)\n","\n","    # Start the server\n","    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n","\n","# ========================================\n","# STEP 14: LAUNCH THE SYSTEM\n","# ========================================\n","\n","# Initialize models first\n","print(\"🔄 Initializing models and components...\")\n","initialize_models()\n","\n","# Create demo test cases for your project\n","print(\"\\n📝 Creating demo test cases...\")\n","create_demo_test_cases()\n","\n","\n","# Launch the server\n","print(\"\\n🚀 Starting the conversational RAG system...\")\n","print(\"⏳ This will open ngrok tunnel and start the FastAPI server...\")\n","\n","# Start the server\n","start_server()"]},{"cell_type":"markdown","source":["## AWS Deployment RAG pipeline"],"metadata":{"id":"DyjhowT311_X"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yhwDqatlCrVu"},"outputs":[],"source":["# Setup imports and basic configuration\n","import os\n","import json\n","import time\n","import shutil\n","from datetime import datetime, timedelta\n","from typing import List, Optional, Dict, Any, Union\n","from urllib.parse import urlparse, urljoin\n","import requests\n","from bs4 import BeautifulSoup\n","import uvicorn\n","from fastapi import FastAPI, HTTPException, Depends, status, Body\n","from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n","from fastapi.middleware.cors import CORSMiddleware\n","from pydantic import BaseModel, Field, validator\n","import logging\n","from collections import OrderedDict\n","import hashlib\n","import re\n","import uuid\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","import asyncio\n","from threading import Thread\n","\n","# Import the RAG components\n","from rank_bm25 import BM25Okapi\n","from sentence_transformers import CrossEncoder\n","from langchain_community.embeddings import HuggingFaceEmbeddings\n","from langchain_community.vectorstores import FAISS\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from langchain.prompts import PromptTemplate\n","from langchain.docstore.document import Document\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","print(\"✅ All imports successful!\")\n","\n","# ========================================\n","# STEP 2: CONFIGURATION AND API KEYS\n","# ========================================\n","\n","# Configuration for AWS deployment\n","class Config:\n","    # Base path for AWS EC2\n","    BASE_PATH = \"/home/ubuntu/rag-knowledge-base/rag_demo\"\n","\n","    # Static index (existing)\n","    STATIC_FAISS_PATH = \"/home/ubuntu/rag-knowledge-base/rag_demo/faiss_index\"\n","\n","    # Dynamic index (new)\n","    DYNAMIC_BASE_PATH = \"/home/ubuntu/rag-knowledge-base/rag_demo/dynamic_index\"\n","    DYNAMIC_FAISS_PATH = \"/home/ubuntu/rag-knowledge-base/rag_demo/dynamic_index/faiss_index\"\n","    METADATA_PATH = \"/home/ubuntu/rag-knowledge-base/rag_demo/dynamic_index/metadata\"\n","    BACKUP_PATH = \"/home/ubuntu/rag-knowledge-base/rag_demo/dynamic_index/backups\"\n","\n","    # Conversation settings\n","    CONVERSATIONS_PATH = \"/home/ubuntu/rag-knowledge-base/rag_demo/conversations\"\n","    MAX_CONVERSATION_LENGTH = 10  # Maximum number of message pairs\n","    CONVERSATION_TIMEOUT = 1800   # 30 minutes in seconds\n","    MAX_CONTEXT_LENGTH = 3        # Last N message pairs to include in context\n","\n","    # Indexing settings\n","    USE_RERANKER = True\n","    RERANKER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n","    BM25_TOP_K = 10\n","    FAISS_TOP_K = 10\n","    FINAL_TOP_K = 3\n","\n","    # Web scraping settings\n","    REQUEST_TIMEOUT = 30\n","    MAX_RETRIES = 3\n","    RETRY_DELAY = 2\n","    CHUNK_SIZE = 1000\n","    CHUNK_OVERLAP = 100\n","    RE_INDEX_DAYS = 7\n","    MAX_VERSIONS = 3\n","\n","    # Evaluation settings\n","    EVALUATION_DATASET_PATH = \"/home/ubuntu/rag-knowledge-base/rag_demo/evaluation\"\n","\n","    # Authentication - YOU CAN CHANGE THESE KEYS\n","    VALID_API_KEYS = {\n","        \"demo-api-key-123\": {\"user\": \"demo_user\", \"permissions\": [\"read\", \"query\", \"index\", \"chat\"]},\n","        \"eval-key-456\": {\"user\": \"evaluator\", \"permissions\": [\"read\", \"query\", \"chat\", \"eval\"]},\n","    }\n","\n","config = Config()\n","\n","# Get API keys from environment variables\n","GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n","if not GOOGLE_API_KEY:\n","    logger.error(\"GOOGLE_API_KEY environment variable not set!\")\n","    raise ValueError(\"Please set GOOGLE_API_KEY environment variable\")\n","\n","os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n","print(\"✅ Configuration loaded!\")\n","\n","# ========================================\n","# STEP 3: PYDANTIC MODELS (Same as before)\n","# ========================================\n","\n","# Chat-specific models\n","class ChatMessage(BaseModel):\n","    role: str = Field(..., pattern=\"^(user|assistant)$\")\n","    content: str = Field(..., min_length=1, max_length=2000)\n","    timestamp: Optional[datetime] = Field(default_factory=datetime.now)\n","\n","class ChatRequest(BaseModel):\n","    messages: List[ChatMessage] = Field(..., min_items=1, max_items=20)\n","    session_id: Optional[str] = Field(default=None)\n","    use_dynamic_index: Optional[bool] = Field(default=True)\n","    use_reranker: Optional[bool] = Field(default=True)\n","    top_k: Optional[int] = Field(default=3, ge=1, le=5)\n","\n","class ChatResponse(BaseModel):\n","    session_id: str\n","    response: Dict[str, Any]\n","    conversation_length: int\n","    timestamp: datetime = Field(default_factory=datetime.now)\n","\n","# Original models (updated)\n","class IndexRequest(BaseModel):\n","    url: List[str] = Field(..., min_items=1, max_items=5, description=\"URLs to index\")\n","\n","    @validator('url')\n","    def validate_urls(cls, v):\n","        for url in v:\n","            parsed = urlparse(url)\n","            if not parsed.scheme or not parsed.netloc:\n","                raise ValueError(f\"Invalid URL format: {url}\")\n","        return v\n","\n","class IndexResponse(BaseModel):\n","    status: str\n","    indexed_url: List[str]\n","    failed_url: Optional[List[Dict[str, str]]] = None\n","    metadata: Dict[str, Any] = Field(default_factory=dict)\n","    timestamp: datetime = Field(default_factory=datetime.now)\n","\n","class HealthResponse(BaseModel):\n","    status: str\n","    timestamp: datetime = Field(default_factory=datetime.now)\n","    components: dict\n","    conversations_active: int\n","\n","# Sources models\n","class SourceInfo(BaseModel):\n","    source_url: str\n","    title: Optional[str] = None\n","    indexed_at: Optional[datetime] = None\n","    document_count: int = 0\n","    source_type: str = Field(..., description=\"static or dynamic\")\n","    last_updated: Optional[datetime] = None\n","\n","class SourcesResponse(BaseModel):\n","    total_sources: int\n","    static_sources: List[SourceInfo]\n","    dynamic_sources: List[SourceInfo]\n","    timestamp: datetime = Field(default_factory=datetime.now)\n","\n","# Evaluation models\n","class EvaluationRequest(BaseModel):\n","    test_cases: List[Dict[str, Any]]\n","    session_id: Optional[str] = None\n","\n","class EvaluationResponse(BaseModel):\n","    overall_score: float\n","    detailed_results: List[Dict[str, Any]]\n","    metrics: Dict[str, float]\n","    timestamp: datetime = Field(default_factory=datetime.now)\n","\n","print(\"✅ Pydantic models defined!\")\n","\n","# ========================================\n","# STEP 4: GLOBAL VARIABLES AND UTILITIES\n","# ========================================\n","\n","# Global variables\n","embedding_model = None\n","static_vectorstore = None\n","dynamic_vectorstore = None\n","llm = None\n","bm25_static = None\n","bm25_dynamic = None\n","bm25_docs_static = None\n","bm25_docs_dynamic = None\n","bm25_sources_static = None\n","bm25_sources_dynamic = None\n","reranker = None\n","text_to_docs_static = None\n","text_to_docs_dynamic = None\n","chat_prompt = None\n","text_splitter = None\n","\n","# In-memory conversation storage (will use file-based backup)\n","active_conversations = {}\n","\n","# Updated source URL mapping for static content (AWS paths)\n","source_url_map = {\n","    \"/content/drive/MyDrive/RAG_demo/data/genai-platform.txt\": \"https://huyenchip.com/2024/07/25/genai-platform.html\",\n","    \"/content/drive/MyDrive/RAG_demo/data/hallucination.txt\": \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n","    \"/content/drive/MyDrive/RAG_demo/data/quora_engineering.txt\": \"https://quoraengineering.quora.com/Building-Embedding-Search-at-Quora\"\n","}\n","\n","# Utility functions\n","def ensure_directories():\n","    \"\"\"Ensure all required directories exist\"\"\"\n","    directories = [\n","        config.BASE_PATH,\n","        config.DYNAMIC_BASE_PATH,\n","        config.DYNAMIC_FAISS_PATH,\n","        config.METADATA_PATH,\n","        config.BACKUP_PATH,\n","        config.CONVERSATIONS_PATH,\n","        config.EVALUATION_DATASET_PATH\n","    ]\n","    for directory in directories:\n","        os.makedirs(directory, exist_ok=True)\n","    logger.info(\"✅ Directory structure created\")\n","\n","def get_url_hash(url: str) -> str:\n","    \"\"\"Generate a hash for URL to use as unique identifier\"\"\"\n","    return hashlib.md5(url.encode()).hexdigest()\n","\n","def load_metadata(file_path: str) -> Dict:\n","    \"\"\"Load metadata from JSON file\"\"\"\n","    if os.path.exists(file_path):\n","        try:\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                return json.load(f)\n","        except Exception as e:\n","            logger.error(f\"Error loading metadata from {file_path}: {str(e)}\")\n","    return {}\n","\n","def save_metadata(data: Dict, file_path: str):\n","    \"\"\"Save metadata to JSON file\"\"\"\n","    try:\n","        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n","        with open(file_path, 'w', encoding='utf-8') as f:\n","            json.dump(data, f, indent=2, default=str)\n","    except Exception as e:\n","        logger.error(f\"Error saving metadata to {file_path}: {str(e)}\")\n","        raise\n","\n","def format_sources(sources):\n","    \"\"\"Convert file paths to URLs\"\"\"\n","    formatted = []\n","    for source in sources:\n","        if source.startswith('http'):\n","            formatted.append(source)\n","        else:\n","            formatted.append(source_url_map.get(source, source))\n","    return formatted\n","\n","def generate_session_id() -> str:\n","    \"\"\"Generate a unique session ID\"\"\"\n","    return str(uuid.uuid4())\n","\n","def clean_old_conversations():\n","    \"\"\"Remove old conversations from memory and save important ones\"\"\"\n","    current_time = datetime.now()\n","    expired_sessions = []\n","\n","    for session_id, conv_data in active_conversations.items():\n","        last_active = conv_data.get('last_active', current_time)\n","        if isinstance(last_active, str):\n","            last_active = datetime.fromisoformat(last_active)\n","\n","        if (current_time - last_active).seconds > config.CONVERSATION_TIMEOUT:\n","            # Save conversation before deletion\n","            try:\n","                conv_file = os.path.join(config.CONVERSATIONS_PATH, f\"{session_id}.json\")\n","                save_metadata(conv_data, conv_file)\n","            except Exception as e:\n","                logger.error(f\"Error saving conversation {session_id}: {e}\")\n","\n","            expired_sessions.append(session_id)\n","\n","    for session_id in expired_sessions:\n","        del active_conversations[session_id]\n","        logger.info(f\"Cleaned expired conversation: {session_id}\")\n","\n","print(\"✅ Utility functions defined!\")\n","\n","# ========================================\n","# STEP 5: CONVERSATION MANAGEMENT\n","# ========================================\n","\n","class ConversationManager:\n","    def __init__(self):\n","        self.conversations = active_conversations\n","\n","    def get_or_create_session(self, session_id: Optional[str] = None) -> str:\n","        \"\"\"Get existing session or create new one\"\"\"\n","        if session_id and session_id in self.conversations:\n","            # Update last active time\n","            self.conversations[session_id]['last_active'] = datetime.now()\n","            return session_id\n","\n","        # Create new session\n","        new_session_id = generate_session_id()\n","        self.conversations[new_session_id] = {\n","            'messages': [],\n","            'created_at': datetime.now(),\n","            'last_active': datetime.now(),\n","            'metadata': {}\n","        }\n","        logger.info(f\"Created new conversation session: {new_session_id}\")\n","        return new_session_id\n","\n","    def add_message(self, session_id: str, message: ChatMessage):\n","        \"\"\"Add message to conversation\"\"\"\n","        if session_id not in self.conversations:\n","            raise ValueError(f\"Session {session_id} not found\")\n","\n","        self.conversations[session_id]['messages'].append({\n","            'role': message.role,\n","            'content': message.content,\n","            'timestamp': message.timestamp.isoformat() if message.timestamp else datetime.now().isoformat()\n","        })\n","        self.conversations[session_id]['last_active'] = datetime.now()\n","\n","        # Trim conversation if too long\n","        messages = self.conversations[session_id]['messages']\n","        if len(messages) > config.MAX_CONVERSATION_LENGTH * 2:  # *2 for user+assistant pairs\n","            self.conversations[session_id]['messages'] = messages[-config.MAX_CONVERSATION_LENGTH * 2:]\n","\n","    def get_conversation_context(self, session_id: str, max_pairs: int = None) -> str:\n","        \"\"\"Get conversation context for LLM\"\"\"\n","        if session_id not in self.conversations:\n","            return \"\"\n","\n","        messages = self.conversations[session_id]['messages']\n","        if not messages:\n","            return \"\"\n","\n","        # Get last N pairs (user + assistant messages)\n","        max_pairs = max_pairs or config.MAX_CONTEXT_LENGTH\n","        recent_messages = messages[-(max_pairs * 2):]\n","\n","        context_parts = []\n","        for msg in recent_messages:\n","            role = \"Human\" if msg['role'] == 'user' else \"Assistant\"\n","            context_parts.append(f\"{role}: {msg['content']}\")\n","\n","        return \"\\n\".join(context_parts)\n","\n","    def get_conversation_length(self, session_id: str) -> int:\n","        \"\"\"Get number of message exchanges\"\"\"\n","        if session_id not in self.conversations:\n","            return 0\n","        return len(self.conversations[session_id]['messages'])\n","\n","    def save_conversation(self, session_id: str):\n","        \"\"\"Save conversation to disk\"\"\"\n","        if session_id not in self.conversations:\n","            return\n","\n","        conv_file = os.path.join(config.CONVERSATIONS_PATH, f\"{session_id}.json\")\n","        save_metadata(self.conversations[session_id], conv_file)\n","\n","conversation_manager = ConversationManager()\n","print(\"✅ Conversation manager initialized!\")\n","\n","# ========================================\n","# STEP 6: WEB SCRAPING AND INDEXING\n","# ========================================\n","\n","class WebScraper:\n","    def __init__(self):\n","        self.session = requests.Session()\n","        self.session.headers.update({\n","            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n","        })\n","\n","    def extract_content(self, url: str) -> Dict[str, Any]:\n","        \"\"\"Extract content from URL with retry logic\"\"\"\n","        for attempt in range(config.MAX_RETRIES):\n","            try:\n","                response = self.session.get(url, timeout=config.REQUEST_TIMEOUT)\n","                response.raise_for_status()\n","\n","                soup = BeautifulSoup(response.content, 'html.parser')\n","\n","                # Remove unwanted elements\n","                for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'advertisement']):\n","                    element.decompose()\n","\n","                # Extract title\n","                title = soup.find('title')\n","                title_text = title.get_text().strip() if title else url\n","\n","                # Extract main content\n","                content_selectors = [\n","                    'article', 'main', '[role=\"main\"]',\n","                    '.content', '.post', '.article',\n","                    'div.container', 'div.wrapper'\n","                ]\n","\n","                content_text = \"\"\n","                for selector in content_selectors:\n","                    content = soup.select_one(selector)\n","                    if content:\n","                        content_text = content.get_text()\n","                        break\n","\n","                if not content_text:\n","                    # Fallback to body\n","                    body = soup.find('body')\n","                    content_text = body.get_text() if body else \"\"\n","\n","                # Clean text\n","                content_text = re.sub(r'\\s+', ' ', content_text).strip()\n","\n","                if not content_text:\n","                    raise ValueError(\"No content extracted\")\n","\n","                return {\n","                    'title': title_text,\n","                    'content': content_text,\n","                    'url': url,\n","                    'success': True,\n","                    'error': None\n","                }\n","\n","            except Exception as e:\n","                logger.warning(f\"Attempt {attempt + 1} failed for {url}: {str(e)}\")\n","                if attempt < config.MAX_RETRIES - 1:\n","                    time.sleep(config.RETRY_DELAY * (2 ** attempt))\n","                else:\n","                    return {\n","                        'title': None,\n","                        'content': None,\n","                        'url': url,\n","                        'success': False,\n","                        'error': str(e)\n","                    }\n","\n","def build_bm25_from_vectorstore(vectorstore):\n","    \"\"\"Build BM25 index from FAISS vectorstore\"\"\"\n","    try:\n","        docs = []\n","        sources = []\n","        for doc_id, doc in vectorstore.docstore._dict.items():\n","            text = getattr(doc, \"page_content\", None) or doc.page_content\n","            docs.append(text)\n","            sources.append(doc.metadata.get(\"source\", \"Unknown\"))\n","\n","        tokenized = [d.split() for d in docs]\n","        bm25 = BM25Okapi(tokenized)\n","\n","        # Build text to docs mapping\n","        text_to_docs = {}\n","        for doc in vectorstore.docstore._dict.values():\n","            text = doc.page_content\n","            text_to_docs.setdefault(text, []).append(doc)\n","\n","        logger.info(f\"BM25 built over {len(docs)} chunks\")\n","        return bm25, docs, sources, text_to_docs\n","    except Exception as e:\n","        logger.error(f\"Error building BM25: {str(e)}\")\n","        raise\n","\n","print(\"✅ Web scraping and indexing functions defined!\")\n","\n","# ========================================\n","# STEP 7: HYBRID SEARCH SYSTEM\n","# ========================================\n","\n","def hybrid_search(query: str, use_dynamic: bool = True, bm_k: int = 10, faiss_k: int = 10, top_k: int = 3, use_reranker: bool = True, conversation_context: str = \"\"):\n","    \"\"\"Perform hybrid search on static and/or dynamic indices with conversation context\"\"\"\n","    try:\n","        # Enhanced query with context\n","        enhanced_query = query\n","        if conversation_context:\n","            # Add context to help with pronouns and references\n","            enhanced_query = f\"Context: {conversation_context}\\n\\nCurrent question: {query}\"\n","\n","        all_candidates = []\n","\n","        # Search static index\n","        if static_vectorstore:\n","            candidates_static = _search_single_index(\n","                enhanced_query, static_vectorstore, bm25_static, bm25_docs_static,\n","                bm25_sources_static, text_to_docs_static, bm_k, faiss_k\n","            )\n","            all_candidates.extend(candidates_static)\n","\n","        # Search dynamic index\n","        if use_dynamic and dynamic_vectorstore:\n","            candidates_dynamic = _search_single_index(\n","                enhanced_query, dynamic_vectorstore, bm25_dynamic, bm25_docs_dynamic,\n","                bm25_sources_dynamic, text_to_docs_dynamic, bm_k, faiss_k\n","            )\n","            all_candidates.extend(candidates_dynamic)\n","\n","        if not all_candidates:\n","            return []\n","\n","        # Merge and deduplicate candidates\n","        merged = OrderedDict()\n","        for candidate in all_candidates:\n","            key = candidate[\"text\"].strip()[:100]  # Use first 100 chars as key\n","            if key not in merged or (candidate.get(\"bm25_score\") or 0) > (merged[key].get(\"bm25_score\") or 0):\n","                merged[key] = candidate\n","\n","        candidates = list(merged.values())\n","\n","        # Optional reranking - use original query for reranking\n","        if use_reranker and reranker and candidates:\n","            pairs = [(query, c[\"text\"]) for c in candidates]  # Use original query for reranking\n","            scores = reranker.predict(pairs)\n","            for c, s in zip(candidates, scores):\n","                c[\"rerank_score\"] = float(s)\n","            candidates = sorted(candidates, key=lambda x: x[\"rerank_score\"], reverse=True)\n","        else:\n","            candidates = sorted(candidates, key=lambda x: (0 if x[\"bm25_score\"] is not None else 1, -(x[\"bm25_score\"] or 0)))\n","\n","        # Return top_k Document objects\n","        final_docs = []\n","        for c in candidates[:top_k]:\n","            text = c[\"text\"]\n","            # Try to find in both mappings\n","            docs_list = text_to_docs_static.get(text, []) if text_to_docs_static else []\n","            if not docs_list and text_to_docs_dynamic:\n","                docs_list = text_to_docs_dynamic.get(text, [])\n","\n","            if docs_list:\n","                final_docs.append(docs_list[0])\n","            else:\n","                final_docs.append(Document(page_content=text, metadata={\"source\": c.get(\"source\", \"Unknown\")}))\n","\n","        return final_docs\n","\n","    except Exception as e:\n","        logger.error(f\"Error in hybrid search: {str(e)}\")\n","        raise\n","\n","def _search_single_index(query: str, vectorstore, bm25, bm25_docs, bm25_sources, text_to_docs, bm_k: int, faiss_k: int):\n","    \"\"\"Search a single index (static or dynamic)\"\"\"\n","    candidates = []\n","\n","    # BM25 candidates\n","    if bm25 and bm25_docs:\n","        tokenized_q = query.split()\n","        bm25_scores = bm25.get_scores(tokenized_q)\n","        bm25_indices = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:bm_k]\n","        for idx in bm25_indices:\n","            txt = bm25_docs[idx]\n","            src = bm25_sources[idx]\n","            candidates.append({\"text\": txt, \"source\": src, \"bm25_score\": bm25_scores[idx]})\n","\n","    # FAISS candidates\n","    if vectorstore:\n","        faiss_retriever = vectorstore.as_retriever(search_kwargs={\"k\": faiss_k})\n","        faiss_docs = faiss_retriever.get_relevant_documents(query)\n","        for d in faiss_docs:\n","            candidates.append({\"text\": d.page_content, \"source\": d.metadata.get(\"source\", \"Unknown\"), \"bm25_score\": None})\n","\n","    return candidates\n","\n","print(\"✅ Hybrid search system defined!\")\n","\n","# ========================================\n","# STEP 8: EVALUATION SYSTEM\n","# ========================================\n","\n","class RAGEvaluator:\n","    def __init__(self):\n","        self.test_cases = []\n","        self.results = []\n","\n","    def create_test_dataset(self):\n","        \"\"\"Create evaluation test cases\"\"\"\n","        test_cases = [\n","            # Context retention tests\n","            {\n","                \"conversation\": [\n","                    {\"role\": \"user\", \"content\": \"What is attention mechanism in transformers?\"},\n","                    {\"role\": \"user\", \"content\": \"How does it help with long sequences?\"}\n","                ],\n","                \"expected_context\": \"attention mechanism\",\n","                \"test_type\": \"context_retention\"\n","            },\n","\n","            # Citation accuracy tests\n","            {\n","                \"conversation\": [\n","                    {\"role\": \"user\", \"content\": \"Tell me about RAG systems\"}\n","                ],\n","                \"expected_citations\": True,\n","                \"test_type\": \"citation_accuracy\"\n","            },\n","\n","            # Topic switching tests\n","            {\n","                \"conversation\": [\n","                    {\"role\": \"user\", \"content\": \"Explain neural networks\"},\n","                    {\"role\": \"user\", \"content\": \"Now tell me about cooking recipes\"}\n","                ],\n","                \"expected_behavior\": \"topic_switch\",\n","                \"test_type\": \"topic_switching\"\n","            },\n","\n","            # Multi-source tests\n","            {\n","                \"conversation\": [\n","                    {\"role\": \"user\", \"content\": \"What are the latest trends in AI?\"}\n","                ],\n","                \"expected_sources\": \"mixed\",\n","                \"test_type\": \"multi_source\"\n","            }\n","        ]\n","\n","        # Save test cases\n","        test_file = os.path.join(config.EVALUATION_DATASET_PATH, \"test_cases.json\")\n","        save_metadata(test_cases, test_file)\n","        return test_cases\n","\n","    def evaluate_response_relevance(self, question: str, answer: str, context_docs: List[Document]) -> float:\n","        \"\"\"Evaluate how relevant the answer is to the question\"\"\"\n","        try:\n","            # Simple keyword overlap score\n","            question_words = set(question.lower().split())\n","            answer_words = set(answer.lower().split())\n","\n","            overlap = len(question_words.intersection(answer_words))\n","            relevance_score = overlap / len(question_words) if question_words else 0.0\n","\n","            # Bonus for using context\n","            context_text = \" \".join([doc.page_content for doc in context_docs])\n","            context_words = set(context_text.lower().split())\n","            context_usage = len(answer_words.intersection(context_words)) / len(context_words) if context_words else 0.0\n","\n","            final_score = min(1.0, (relevance_score + context_usage) / 2)\n","            return final_score\n","\n","        except Exception as e:\n","            logger.error(f\"Error evaluating relevance: {str(e)}\")\n","            return 0.0\n","\n","    def evaluate_citation_accuracy(self, answer: str, sources: List[str]) -> float:\n","        \"\"\"Evaluate citation accuracy\"\"\"\n","        if not sources:\n","            return 0.0\n","\n","        # Check if answer contains factual claims (simple heuristic)\n","        factual_indicators = [\"according to\", \"research shows\", \"studies indicate\", \"data reveals\", \"analysis found\"]\n","        has_factual_claims = any(indicator in answer.lower() for indicator in factual_indicators)\n","\n","        if has_factual_claims and sources:\n","            return 1.0\n","        elif not has_factual_claims:\n","            return 0.8  # No factual claims, so no citations needed\n","        else:\n","            return 0.0  # Factual claims but no citations\n","\n","    def evaluate_context_retention(self, conversation_history: List[Dict], current_answer: str) -> float:\n","        \"\"\"Evaluate how well context from previous messages is retained\"\"\"\n","        if len(conversation_history) <= 1:\n","            return 1.0  # No previous context to retain\n","\n","        # Look for references to previous topics\n","        previous_content = \" \".join([msg[\"content\"] for msg in conversation_history[:-1]])\n","        previous_words = set(previous_content.lower().split())\n","        answer_words = set(current_answer.lower().split())\n","\n","        # Check for pronouns and references\n","        references = [\"this\", \"that\", \"it\", \"they\", \"these\", \"those\"]\n","        has_references = any(ref in current_answer.lower() for ref in references)\n","\n","        # Calculate context retention score\n","        word_overlap = len(previous_words.intersection(answer_words)) / len(previous_words) if previous_words else 0.0\n","        reference_bonus = 0.3 if has_references else 0.0\n","\n","        context_score = min(1.0, word_overlap + reference_bonus)\n","        return context_score\n","\n","    async def run_evaluation(self, test_cases: List[Dict]) -> Dict[str, Any]:\n","        \"\"\"Run comprehensive evaluation\"\"\"\n","        results = []\n","        total_scores = {\"relevance\": [], \"citation\": [], \"context\": []}\n","\n","        for i, test_case in enumerate(test_cases):\n","            try:\n","                logger.info(f\"Running test case {i+1}/{len(test_cases)}\")\n","\n","                # Simulate conversation\n","                session_id = generate_session_id()\n","                conversation_history = []\n","\n","                for message in test_case[\"conversation\"]:\n","                    # Add user message\n","                    user_msg = ChatMessage(role=\"user\", content=message[\"content\"])\n","                    conversation_manager.add_message(session_id, user_msg)\n","                    conversation_history.append({\"role\": \"user\", \"content\": message[\"content\"]})\n","\n","                    # Get bot response\n","                    context = conversation_manager.get_conversation_context(session_id)\n","                    docs = hybrid_search(\n","                        message[\"content\"],\n","                        use_dynamic=True,\n","                        conversation_context=context\n","                    )\n","\n","                    if docs:\n","                        doc_context = \"\\n\\n\".join([doc.page_content for doc in docs])\n","                        formatted_prompt = chat_prompt.format(\n","                            conversation_context=context,\n","                            context=doc_context,\n","                            question=message[\"content\"]\n","                        )\n","                        response = llm.invoke(formatted_prompt)\n","                        answer = response.content.strip()\n","                        sources = list(set([doc.metadata.get(\"source\", \"Unknown\") for doc in docs]))\n","                        sources = format_sources(sources)\n","                    else:\n","                        answer = \"I couldn't find specific information about that in my knowledge base.\"\n","                        sources = []\n","\n","                    # Add assistant message\n","                    assistant_message = ChatMessage(role=\"assistant\", content=answer)\n","                    conversation_manager.add_message(session_id, assistant_message)\n","                    conversation_history.append({\"role\": \"assistant\", \"content\": answer})\n","\n","                # Evaluate the last response\n","                last_question = test_case[\"conversation\"][-1][\"content\"]\n","                last_answer = conversation_history[-1][\"content\"]\n","\n","                # Calculate scores\n","                relevance_score = self.evaluate_response_relevance(last_question, last_answer, docs if docs else [])\n","                citation_score = self.evaluate_citation_accuracy(last_answer, sources if sources else [])\n","                context_score = self.evaluate_context_retention(conversation_history[:-1], last_answer)\n","\n","                result = {\n","                    \"test_case_id\": i,\n","                    \"test_type\": test_case.get(\"test_type\", \"general\"),\n","                    \"question\": last_question,\n","                    \"answer\": last_answer,\n","                    \"sources\": sources if sources else [],\n","                    \"scores\": {\n","                        \"relevance\": relevance_score,\n","                        \"citation\": citation_score,\n","                        \"context_retention\": context_score\n","                    },\n","                    \"overall_score\": (relevance_score + citation_score + context_score) / 3\n","                }\n","\n","                results.append(result)\n","                total_scores[\"relevance\"].append(relevance_score)\n","                total_scores[\"citation\"].append(citation_score)\n","                total_scores[\"context\"].append(context_score)\n","\n","            except Exception as e:\n","                logger.error(f\"Error in test case {i}: {str(e)}\")\n","                continue\n","\n","        # Calculate overall metrics\n","        metrics = {\n","            \"average_relevance\": np.mean(total_scores[\"relevance\"]) if total_scores[\"relevance\"] else 0.0,\n","            \"average_citation\": np.mean(total_scores[\"citation\"]) if total_scores[\"citation\"] else 0.0,\n","            \"average_context\": np.mean(total_scores[\"context\"]) if total_scores[\"context\"] else 0.0,\n","            \"overall_average\": np.mean([np.mean(scores) for scores in total_scores.values()]) if any(total_scores.values()) else 0.0,\n","            \"test_cases_completed\": len(results),\n","            \"test_cases_failed\": len(test_cases) - len(results)\n","        }\n","\n","        return {\n","            \"overall_score\": metrics[\"overall_average\"],\n","            \"detailed_results\": results,\n","            \"metrics\": metrics\n","        }\n","\n","evaluator = RAGEvaluator()\n","print(\"✅ Evaluation system initialized!\")\n","\n","# ========================================\n","# MODIFIED: MODEL INITIALIZATION\n","# ========================================\n","\n","def initialize_models():\n","    \"\"\"Initialize all models and components for AWS deployment\"\"\"\n","    global embedding_model, static_vectorstore, dynamic_vectorstore, llm\n","    global bm25_static, bm25_dynamic, bm25_docs_static, bm25_docs_dynamic\n","    global bm25_sources_static, bm25_sources_dynamic, reranker\n","    global text_to_docs_static, text_to_docs_dynamic, chat_prompt, text_splitter\n","\n","    try:\n","        print(\"🔄 Loading models for AWS deployment...\")\n","        ensure_directories()\n","        clean_old_conversations()\n","\n","        # Load embedding model\n","        embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n","        print(\"✅ Embedding model loaded\")\n","\n","        # Initialize text splitter\n","        text_splitter = RecursiveCharacterTextSplitter(\n","            chunk_size=config.CHUNK_SIZE,\n","            chunk_overlap=config.CHUNK_OVERLAP,\n","            length_function=len,\n","            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n","        )\n","        print(\"✅ Text splitter initialized\")\n","\n","        # Try to load static FAISS vectorstore (may not exist initially)\n","        try:\n","            if os.path.exists(config.STATIC_FAISS_PATH):\n","                static_vectorstore = FAISS.load_local(\n","                    config.STATIC_FAISS_PATH,\n","                    embedding_model,\n","                    allow_dangerous_deserialization=True\n","                )\n","                bm25_static, bm25_docs_static, bm25_sources_static, text_to_docs_static = build_bm25_from_vectorstore(static_vectorstore)\n","                print(\"✅ Static FAISS vectorstore loaded\")\n","            else:\n","                print(\"ℹ️ No static FAISS index found - will work with dynamic index only\")\n","        except Exception as e:\n","            print(f\"⚠️ Could not load static FAISS: {str(e)}\")\n","\n","        # Try to load dynamic FAISS vectorstore (may not exist initially)\n","        try:\n","            if os.path.exists(config.DYNAMIC_FAISS_PATH):\n","                dynamic_vectorstore = FAISS.load_local(\n","                    config.DYNAMIC_FAISS_PATH,\n","                    embedding_model,\n","                    allow_dangerous_deserialization=True\n","                )\n","                bm25_dynamic, bm25_docs_dynamic, bm25_sources_dynamic, text_to_docs_dynamic = build_bm25_from_vectorstore(dynamic_vectorstore)\n","                print(\"✅ Dynamic FAISS vectorstore loaded\")\n","            else:\n","                print(\"ℹ️ No dynamic FAISS index found - will create on first index\")\n","        except Exception as e:\n","            print(f\"ℹ️ Dynamic FAISS not found (will create on first index): {str(e)}\")\n","\n","        # Initialize LLM with environment variable\n","        llm = ChatGoogleGenerativeAI(\n","            model=\"gemini-1.5-flash\",\n","            temperature=0,\n","            google_api_key=os.environ[\"GOOGLE_API_KEY\"]\n","        )\n","        print(\"✅ LLM initialized\")\n","\n","        # Create chat prompt template\n","        chat_prompt_template = \"\"\"\n","You are a helpful AI assistant with access to multiple knowledge sources. You can maintain context across conversations and provide accurate citations.\n","\n","Previous conversation context:\n","{conversation_context}\n","\n","Current context from knowledge base:\n","{context}\n","\n","Current question:\n","{question}\n","\n","Instructions:\n","1. Use the conversation context to understand references like \"it\", \"this\", \"that\", etc.\n","2. Provide accurate answers based on the knowledge base context\n","3. If you reference specific information, it should be from the provided context\n","4. If the answer is not in the knowledge base, say: \"I couldn't find specific information about that in my knowledge base.\"\n","5. Be conversational and natural in your responses\n","6. Handle follow-up questions by connecting them to previous context when appropriate\n","\n","Answer:\n","        \"\"\"\n","        chat_prompt = PromptTemplate(\n","            input_variables=[\"conversation_context\", \"context\", \"question\"],\n","            template=chat_prompt_template\n","        )\n","        print(\"✅ Chat prompt template created\")\n","\n","        # Load reranker\n","        if config.USE_RERANKER:\n","            reranker = CrossEncoder(config.RERANKER_MODEL)\n","            print(\"✅ Reranker loaded\")\n","\n","        print(\"🎉 All models initialized successfully!\")\n","\n","    except Exception as e:\n","        print(f\"❌ Error initializing models: {str(e)}\")\n","        raise\n","\n","# ========================================\n","# MODIFIED: SERVER STARTUP FOR AWS\n","# ========================================\n","\n","def start_aws_server():\n","    \"\"\"Start the FastAPI server for AWS deployment\"\"\"\n","    print(\"=\" * 60)\n","    print(\"🚀 CONVERSATIONAL RAG SYSTEM - AWS DEPLOYMENT\")\n","    print(\"=\" * 60)\n","    print(\"🔑 API Keys:\")\n","    print(\"   • demo-api-key-123 (full access)\")\n","    print(\"   • eval-key-456 (evaluation access)\")\n","    print(\"\")\n","    print(\"🎯 ENDPOINTS will be available at:\")\n","    print(\"  • POST /api/v1/chat - Chat with the system\")\n","    print(\"  • POST /api/v1/index - Index new URLs\")\n","    print(\"  • GET  /api/v1/sources - List all sources\")\n","    print(\"  • GET  /api/v1/sources/{hash} - Get source details\")\n","    print(\"  • POST /api/v1/evaluate - Run automated evaluation\")\n","    print(\"  • GET  /api/v1/conversations - List conversations\")\n","    print(\"  • GET  /health - Health check\")\n","    print(\"\")\n","    print(\"🌐 Server starting on 0.0.0.0:8000...\")\n","    print(\"📚 API docs will be at: http://your-ec2-ip:8000/docs\")\n","    print(\"=\" * 60)\n","\n","    # Start the server without ngrok\n","    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n","\n","# ========================================\n","# FASTAPI APPLICATION (Same as before)\n","# ========================================\n","\n","app = FastAPI(\n","    title=\"Conversational RAG API with Dynamic Indexing and Sources\",\n","    description=\"Hybrid Retrieval-Augmented Generation API with Conversation Support, BM25 + FAISS, Dynamic Web Indexing, and Source Management\",\n","    version=\"3.1.0\"\n",")\n","\n","app.add_middleware(\n","    CORSMiddleware,\n","    allow_origins=[\"*\"],\n","    allow_credentials=True,\n","    allow_methods=[\"*\"],\n","    allow_headers=[\"*\"],\n",")\n","\n","security = HTTPBearer()\n","\n","# Authentication (same as before)\n","async def verify_api_key(credentials: HTTPAuthorizationCredentials = Depends(security)):\n","    \"\"\"Verify API key from Authorization header\"\"\"\n","    api_key = credentials.credentials\n","    if api_key not in config.VALID_API_KEYS:\n","        logger.warning(f\"Invalid API key attempted: {api_key[:10]}...\")\n","        raise HTTPException(\n","            status_code=status.HTTP_401_UNAUTHORIZED,\n","            detail=\"Invalid API key\",\n","            headers={\"WWW-Authenticate\": \"Bearer\"},\n","        )\n","    return config.VALID_API_KEYS[api_key]\n","\n","# ========================================\n","# ========================================\n","# STEP 11: API ENDPOINTS\n","# ========================================\n","\n","@app.get(\"/health\", response_model=HealthResponse)\n","async def health_check():\n","    \"\"\"Health check endpoint\"\"\"\n","    clean_old_conversations()  # Cleanup old conversations on health check\n","\n","    components = {\n","        \"static_vectorstore\": static_vectorstore is not None,\n","        \"dynamic_vectorstore\": dynamic_vectorstore is not None,\n","        \"llm\": llm is not None,\n","        \"bm25_static\": bm25_static is not None,\n","        \"bm25_dynamic\": bm25_dynamic is not None,\n","        \"reranker\": reranker is not None if config.USE_RERANKER else \"disabled\",\n","        \"conversation_manager\": True\n","    }\n","    status = \"healthy\" if llm is not None else \"unhealthy\"\n","    return HealthResponse(\n","        status=status,\n","        components=components,\n","        conversations_active=len(active_conversations)\n","    )\n","\n","@app.post(\"/api/v1/chat\", response_model=ChatResponse)\n","async def chat_with_rag(\n","    request: ChatRequest,\n","    user_info: dict = Depends(verify_api_key)\n","):\n","    \"\"\"Main conversational RAG endpoint\"\"\"\n","    try:\n","        # Check permissions\n","        if \"chat\" not in user_info.get(\"permissions\", []):\n","            raise HTTPException(\n","                status_code=status.HTTP_403_FORBIDDEN,\n","                detail=\"Insufficient permissions for chat\"\n","            )\n","\n","        logger.info(f\"Chat request from user {user_info['user']}\")\n","\n","        # Get or create session\n","        session_id = conversation_manager.get_or_create_session(request.session_id)\n","\n","        # Add conversation history to session (except the last message which is the current question)\n","        for message in request.messages[:-1]:\n","            conversation_manager.add_message(session_id, message)\n","\n","        # Get current question\n","        current_message = request.messages[-1]\n","        if current_message.role != \"user\":\n","            raise HTTPException(\n","                status_code=status.HTTP_400_BAD_REQUEST,\n","                detail=\"Last message must be from user\"\n","            )\n","\n","        # Add current question to conversation\n","        conversation_manager.add_message(session_id, current_message)\n","\n","        # Get conversation context for retrieval and generation\n","        conversation_context = conversation_manager.get_conversation_context(session_id)\n","\n","        # Perform hybrid search with conversation context\n","        docs = hybrid_search(\n","            current_message.content,\n","            use_dynamic=request.use_dynamic_index,\n","            bm_k=config.BM25_TOP_K,\n","            faiss_k=config.FAISS_TOP_K,\n","            top_k=request.top_k,\n","            use_reranker=request.use_reranker and config.USE_RERANKER,\n","            conversation_context=conversation_context\n","        )\n","\n","        if not docs:\n","            answer = \"I couldn't find specific information about that in my knowledge base.\"\n","            sources = []\n","        else:\n","            # Create context and query LLM\n","            doc_context = \"\\n\\n\".join([doc.page_content for doc in docs])\n","            formatted_prompt = chat_prompt.format(\n","                conversation_context=conversation_context,\n","                context=doc_context,\n","                question=current_message.content\n","            )\n","\n","            response = llm.invoke(formatted_prompt)\n","            answer = response.content.strip()\n","\n","            # Get unique sources\n","            sources = list(OrderedDict.fromkeys(doc.metadata.get(\"source\", \"Unknown\") for doc in docs))\n","            sources = format_sources(sources)\n","\n","        # Add assistant response to conversation\n","        assistant_message = ChatMessage(role=\"assistant\", content=answer)\n","        conversation_manager.add_message(session_id, assistant_message)\n","\n","        # Save conversation periodically\n","        if conversation_manager.get_conversation_length(session_id) % 4 == 0:  # Every 4 messages\n","            conversation_manager.save_conversation(session_id)\n","\n","        # UPDATED RESPONSE STRUCTURE WITH ENHANCED SOURCES\n","        response_data = {\n","            \"answer\": {\n","                \"content\": answer,\n","                \"role\": \"assistant\"\n","            },\n","            \"sources\": sources,  # Make sources more prominent\n","            \"citations\": sources,  # Keep backward compatibility\n","            \"retrieved_documents\": [\n","                {\n","                    \"content\": doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content,\n","                    \"source\": doc.metadata.get(\"source\", \"Unknown\"),\n","                    \"title\": doc.metadata.get(\"title\", \"\"),\n","                    \"chunk_id\": doc.metadata.get(\"chunk_id\", 0)\n","                } for doc in docs\n","            ] if docs else [],\n","            \"metadata\": {\n","                \"num_docs_retrieved\": len(docs),\n","                \"num_sources\": len(sources),\n","                \"reranker_used\": request.use_reranker and config.USE_RERANKER,\n","                \"dynamic_index_used\": request.use_dynamic_index,\n","                \"conversation_length\": conversation_manager.get_conversation_length(session_id),\n","                \"user\": user_info[\"user\"]\n","            }\n","        }\n","\n","        return ChatResponse(\n","            session_id=session_id,\n","            response=response_data,\n","            conversation_length=conversation_manager.get_conversation_length(session_id)\n","        )\n","\n","    except HTTPException:\n","        raise\n","    except Exception as e:\n","        logger.error(f\"Error processing chat: {str(e)}\")\n","        raise HTTPException(\n","            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n","            detail=f\"Internal server error: {str(e)}\"\n","        )\n","\n","# NEW: Sources management endpoints\n","@app.get(\"/api/v1/sources\", response_model=SourcesResponse)\n","async def get_sources(user_info: dict = Depends(verify_api_key)):\n","    \"\"\"Get all available sources in the system\"\"\"\n","    try:\n","        # Check permissions\n","        if \"read\" not in user_info.get(\"permissions\", []):\n","            raise HTTPException(\n","                status_code=status.HTTP_403_FORBIDDEN,\n","                detail=\"Insufficient permissions to view sources\"\n","            )\n","\n","        static_sources = []\n","        dynamic_sources = []\n","\n","        # Get static sources\n","        if static_vectorstore:\n","            static_source_map = {}\n","            for doc_id, doc in static_vectorstore.docstore._dict.items():\n","                source = doc.metadata.get(\"source\", \"Unknown\")\n","                title = doc.metadata.get(\"title\", \"\")\n","\n","                if source not in static_source_map:\n","                    static_source_map[source] = {\n","                        \"count\": 0,\n","                        \"title\": title\n","                    }\n","                static_source_map[source][\"count\"] += 1\n","\n","            for source, info in static_source_map.items():\n","                # Convert file paths to URLs using source_url_map\n","                display_source = source_url_map.get(source, source)\n","                static_sources.append(SourceInfo(\n","                    source_url=display_source,\n","                    title=info[\"title\"],\n","                    document_count=info[\"count\"],\n","                    source_type=\"static\",\n","                    indexed_at=None,  # Static sources don't have index timestamp\n","                    last_updated=None\n","                ))\n","\n","        # Get dynamic sources\n","        if dynamic_vectorstore:\n","            dynamic_source_map = {}\n","            for doc_id, doc in dynamic_vectorstore.docstore._dict.items():\n","                source = doc.metadata.get(\"source\", \"Unknown\")\n","                title = doc.metadata.get(\"title\", \"\")\n","                indexed_at = doc.metadata.get(\"indexed_at\")\n","\n","                if source not in dynamic_source_map:\n","                    dynamic_source_map[source] = {\n","                        \"count\": 0,\n","                        \"title\": title,\n","                        \"indexed_at\": indexed_at\n","                    }\n","                dynamic_source_map[source][\"count\"] += 1\n","\n","                # Keep the most recent indexed_at\n","                if indexed_at and (not dynamic_source_map[source][\"indexed_at\"] or indexed_at > dynamic_source_map[source][\"indexed_at\"]):\n","                    dynamic_source_map[source][\"indexed_at\"] = indexed_at\n","\n","            for source, info in dynamic_source_map.items():\n","                indexed_datetime = None\n","                if info[\"indexed_at\"]:\n","                    try:\n","                        indexed_datetime = datetime.fromisoformat(info[\"indexed_at\"]) if isinstance(info[\"indexed_at\"], str) else info[\"indexed_at\"]\n","                    except:\n","                        indexed_datetime = None\n","\n","                dynamic_sources.append(SourceInfo(\n","                    source_url=source,\n","                    title=info[\"title\"],\n","                    document_count=info[\"count\"],\n","                    source_type=\"dynamic\",\n","                    indexed_at=indexed_datetime,\n","                    last_updated=indexed_datetime\n","                ))\n","\n","        # Sort sources by document count (descending)\n","        static_sources.sort(key=lambda x: x.document_count, reverse=True)\n","        dynamic_sources.sort(key=lambda x: x.document_count, reverse=True)\n","\n","        return SourcesResponse(\n","            total_sources=len(static_sources) + len(dynamic_sources),\n","            static_sources=static_sources,\n","            dynamic_sources=dynamic_sources\n","        )\n","\n","    except Exception as e:\n","        logger.error(f\"Error retrieving sources: {str(e)}\")\n","        raise HTTPException(\n","            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n","            detail=f\"Failed to retrieve sources: {str(e)}\"\n","        )\n","\n","@app.get(\"/api/v1/sources/{source_hash}\")\n","async def get_source_details(\n","    source_hash: str,\n","    user_info: dict = Depends(verify_api_key)\n","):\n","    \"\"\"Get detailed information about a specific source\"\"\"\n","    # Check permissions\n","    if \"read\" not in user_info.get(\"permissions\", []):\n","        raise HTTPException(\n","            status_code=status.HTTP_403_FORBIDDEN,\n","            detail=\"Insufficient permissions to view source details\"\n","        )\n","\n","    # Find source by hash or URL\n","    source_found = False\n","    source_details = {\n","        \"source_url\": None,\n","        \"title\": None,\n","        \"chunks\": [],\n","        \"source_type\": None,\n","        \"indexed_at\": None\n","    }\n","\n","    # Search in both vectorstores\n","    for vectorstore, source_type in [(static_vectorstore, \"static\"), (dynamic_vectorstore, \"dynamic\")]:\n","        if vectorstore:\n","            for doc_id, doc in vectorstore.docstore._dict.items():\n","                source = doc.metadata.get(\"source\", \"\")\n","                url_hash = get_url_hash(source)\n","\n","                if url_hash == source_hash or source.endswith(source_hash):\n","                    source_found = True\n","                    source_details.update({\n","                        \"source_url\": source_url_map.get(source, source),\n","                        \"title\": doc.metadata.get(\"title\", \"\"),\n","                        \"source_type\": source_type,\n","                        \"indexed_at\": doc.metadata.get(\"indexed_at\")\n","                    })\n","\n","                    source_details[\"chunks\"].append({\n","                        \"chunk_id\": doc.metadata.get(\"chunk_id\", 0),\n","                        \"content_preview\": doc.page_content[:300] + \"...\" if len(doc.page_content) > 300 else doc.page_content,\n","                        \"content_length\": len(doc.page_content)\n","                    })\n","\n","    if not source_found:\n","        raise HTTPException(\n","            status_code=status.HTTP_404_NOT_FOUND,\n","            detail=\"Source not found\"\n","        )\n","\n","    return source_details\n","\n","@app.post(\"/api/v1/index\", response_model=IndexResponse)\n","async def index_urls(\n","    request: IndexRequest,\n","    user_info: dict = Depends(verify_api_key)\n","):\n","    \"\"\"Index URLs into the dynamic vector database\"\"\"\n","    global dynamic_vectorstore, bm25_dynamic, bm25_docs_dynamic, bm25_sources_dynamic, text_to_docs_dynamic\n","\n","    # Check permissions\n","    if \"index\" not in user_info.get(\"permissions\", []):\n","        raise HTTPException(\n","            status_code=status.HTTP_403_FORBIDDEN,\n","            detail=\"Insufficient permissions for indexing\"\n","        )\n","\n","    logger.info(f\"Indexing request from {user_info['user']}: {len(request.url)} URLs\")\n","\n","    scraper = WebScraper()\n","    indexed_urls = []\n","    failed_urls = []\n","\n","    try:\n","        new_documents = []\n","\n","        for url in request.url:\n","            try:\n","                logger.info(f\"Processing URL: {url}\")\n","\n","                # Extract content\n","                result = scraper.extract_content(url)\n","\n","                if not result['success']:\n","                    failed_urls.append({\n","                        \"url\": url,\n","                        \"error\": result['error'],\n","                        \"error_type\": \"EXTRACTION_FAILED\"\n","                    })\n","                    continue\n","\n","                # Split content into chunks\n","                chunks = text_splitter.split_text(result['content'])\n","\n","                # Create documents\n","                for i, chunk in enumerate(chunks):\n","                    doc = Document(\n","                        page_content=chunk,\n","                        metadata={\n","                            \"source\": url,\n","                            \"title\": result['title'],\n","                            \"chunk_id\": i,\n","                            \"total_chunks\": len(chunks),\n","                            \"indexed_at\": datetime.now().isoformat(),\n","                            \"url_hash\": get_url_hash(url)\n","                        }\n","                    )\n","                    new_documents.append(doc)\n","\n","                indexed_urls.append(url)\n","                logger.info(f\"✅ Successfully processed {url} - {len(chunks)} chunks\")\n","\n","            except Exception as e:\n","                logger.error(f\"Error processing {url}: {str(e)}\")\n","                failed_urls.append({\n","                    \"url\": url,\n","                    \"error\": str(e),\n","                    \"error_type\": \"PROCESSING_ERROR\"\n","                })\n","\n","        # Update vector database if we have new documents\n","        if new_documents:\n","            try:\n","                if dynamic_vectorstore is None:\n","                    # Create new FAISS index\n","                    dynamic_vectorstore = FAISS.from_documents(new_documents, embedding_model)\n","                    logger.info(\"✅ Created new dynamic FAISS index\")\n","                else:\n","                    # Add to existing index\n","                    dynamic_vectorstore.add_documents(new_documents)\n","                    logger.info(f\"✅ Added {len(new_documents)} documents to existing index\")\n","\n","                # Save updated index\n","                dynamic_vectorstore.save_local(config.DYNAMIC_FAISS_PATH)\n","\n","                # Rebuild BM25 and mappings\n","                bm25_dynamic, bm25_docs_dynamic, bm25_sources_dynamic, text_to_docs_dynamic = build_bm25_from_vectorstore(dynamic_vectorstore)\n","\n","                logger.info(f\"✅ Dynamic index updated\")\n","\n","            except Exception as e:\n","                logger.error(f\"Error updating vector database: {str(e)}\")\n","                raise HTTPException(\n","                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n","                    detail=f\"Failed to update vector database: {str(e)}\"\n","                )\n","\n","        # Prepare response\n","        response_status = \"success\" if indexed_urls else \"failed\"\n","        if indexed_urls and failed_urls:\n","            response_status = \"partial_success\"\n","\n","        metadata = {\n","            \"total_requested\": len(request.url),\n","            \"successfully_indexed\": len(indexed_urls),\n","            \"failed\": len(failed_urls),\n","            \"new_documents_added\": len(new_documents),\n","            \"user\": user_info[\"user\"]\n","        }\n","\n","        return IndexResponse(\n","            status=response_status,\n","            indexed_url=indexed_urls,\n","            failed_url=failed_urls if failed_urls else None,\n","            metadata=metadata\n","        )\n","\n","    except Exception as e:\n","        logger.error(f\"Critical error in indexing: {str(e)}\")\n","        raise HTTPException(\n","            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n","            detail=f\"Indexing failed: {str(e)}\"\n","        )\n","\n","@app.post(\"/api/v1/evaluate\", response_model=EvaluationResponse)\n","async def evaluate_system(\n","    request: EvaluationRequest,\n","    user_info: dict = Depends(verify_api_key)\n","):\n","    \"\"\"Run automated evaluation of the RAG system\"\"\"\n","    # Check permissions\n","    if \"eval\" not in user_info.get(\"permissions\", []):\n","        raise HTTPException(\n","            status_code=status.HTTP_403_FORBIDDEN,\n","            detail=\"Insufficient permissions for evaluation\"\n","        )\n","\n","    logger.info(f\"Evaluation request from user {user_info['user']}\")\n","\n","    try:\n","        # Use provided test cases or create default ones\n","        test_cases = request.test_cases if request.test_cases else evaluator.create_test_dataset()\n","\n","        # Run evaluation\n","        evaluation_results = await evaluator.run_evaluation(test_cases)\n","\n","        return EvaluationResponse(\n","            overall_score=evaluation_results[\"overall_score\"],\n","            detailed_results=evaluation_results[\"detailed_results\"],\n","            metrics=evaluation_results[\"metrics\"]\n","        )\n","\n","    except Exception as e:\n","        logger.error(f\"Error running evaluation: {str(e)}\")\n","        raise HTTPException(\n","            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n","            detail=f\"Evaluation failed: {str(e)}\"\n","        )\n","\n","@app.get(\"/api/v1/conversations/{session_id}\")\n","async def get_conversation(\n","    session_id: str,\n","    user_info: dict = Depends(verify_api_key)\n","):\n","    \"\"\"Get conversation history\"\"\"\n","    if session_id not in active_conversations:\n","        raise HTTPException(\n","            status_code=status.HTTP_404_NOT_FOUND,\n","            detail=\"Conversation not found\"\n","        )\n","\n","    conversation_data = active_conversations[session_id]\n","    return {\n","        \"session_id\": session_id,\n","        \"messages\": conversation_data[\"messages\"],\n","        \"created_at\": conversation_data[\"created_at\"],\n","        \"last_active\": conversation_data[\"last_active\"],\n","        \"message_count\": len(conversation_data[\"messages\"])\n","    }\n","\n","@app.delete(\"/api/v1/conversations/{session_id}\")\n","async def delete_conversation(\n","    session_id: str,\n","    user_info: dict = Depends(verify_api_key)\n","):\n","    \"\"\"Delete a conversation\"\"\"\n","    if session_id not in active_conversations:\n","        raise HTTPException(\n","            status_code=status.HTTP_404_NOT_FOUND,\n","            detail=\"Conversation not found\"\n","        )\n","\n","    del active_conversations[session_id]\n","    return {\"status\": \"deleted\", \"session_id\": session_id}\n","\n","@app.get(\"/api/v1/conversations\")\n","async def list_conversations(user_info: dict = Depends(verify_api_key)):\n","    \"\"\"List active conversations\"\"\"\n","    clean_old_conversations()\n","\n","    conversations_summary = []\n","    for session_id, conv_data in active_conversations.items():\n","        conversations_summary.append({\n","            \"session_id\": session_id,\n","            \"message_count\": len(conv_data[\"messages\"]),\n","            \"created_at\": conv_data[\"created_at\"],\n","            \"last_active\": conv_data[\"last_active\"],\n","            \"last_message_preview\": conv_data[\"messages\"][-1][\"content\"][:100] if conv_data[\"messages\"] else \"\"\n","        })\n","\n","    return {\n","        \"active_conversations\": len(conversations_summary),\n","        \"conversations\": conversations_summary\n","    }\n","\n","print(\"✅ API endpoints defined!\")\n","# ========================================\n","# MAIN EXECUTION\n","# ========================================\n","\n","if __name__ == \"__main__\":\n","    print(\"🔄 Initializing conversational RAG system for AWS...\")\n","\n","    # Initialize models first\n","    initialize_models()\n","\n","    # Create demo test cases\n","    print(\"\\n📝 Creating demo test cases...\")\n","    # create_demo_test_cases()  # Include this function from your original code\n","\n","    # Print startup info\n","    print(\"\\n🎓 SYSTEM READY FOR DEPLOYMENT!\")\n","    print(\"✅ All models loaded\")\n","    print(\"✅ API endpoints configured\")\n","    print(\"✅ Ready for recruiter demos\")\n","\n","    # Start the AWS server\n","    start_aws_server()"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"13b-tUP3fJxUbxPeWWJszruSJNkO_IkPL","authorship_tag":"ABX9TyPc6YArMF7T1fIEAw1/FAMt"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}